{
  "hash": "030ff458b26064c829530ef2d9122690",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Advanced Machine Learning with Python (Session 2 - Part 2)\nauthor: Fernando Cervantes (fernando.cervantes@jax.org)\nformat: \n  revealjs:\n    code-fold: false\n    progress: true\n    controls: true\n    output-file: \"Adv_ML_Python_presentation_2_2\"\n    fontsize: 20pt\n\nexecute:\n  error: false\n  echo: true\n  cache: true\n  freeze: true\n  keep-ipynb: true\n\njupyter: adv-ml\n---\n\n\n# Tissue classification with the MoNuSAC dataset\n\n## Tissue classification with the MoNuSAC dataset\n\n* R. Verma, et al. \"MoNuSAC2020: A Multi-organ Nuclei Segmentation and Classification Challenge.\" IEEE Transactions on Medical Imaging (2021)\n\n![](../imgs/Tissue_classification.png)\n\n[Open notebook in Colab](https://colab.research.google.com/drive/1TXFtySMQ6_r2r4vO8tt-ldP9FiE0Un7q){.btn .btn-outline-primary .btn role=\"button\" target=”_blank”} [View solutions](https://colab.research.google.com/drive/1dmegbJxh0QF9kzn18fA9nQ_-Nq6KwiYC){.btn .btn-outline-primary .btn role=\"button\" target=”_blank”}\n\n\n## Data preparation\n\n- [ ] Download the MoNuSAC dataset from https://monusac-2020.grand-challenge.org/Data/\n\n\n\n\n\n::: {.callout-note}\nMore information about the type of tissue of each image can be found [here](https://drive.google.com/file/d/1kdOl3s6uQBRv0nToSIf1dPuceZunzL4N/view).\n:::\n\n---\n\n## Tissue classification with the MoNuSAC dataset {.scrollable}\n\n- [ ] Explore the dataset\n\n::: {#c39ea862 .cell execution_count=4}\n``` {.python .cell-code}\nfrom skimage.io import imread\nimport matplotlib.pyplot as plt\n\nimg = imread(train_images_fns[0])\nplt.imshow(img)\nplt.title(tissue_classes[train_labels[0]])\nplt.show()\n\nimg = imread(test_images_fns[0])\nplt.imshow(img)\nplt.title(tissue_classes[test_labels[0]])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](DL_image_analysis_2_2_files/figure-revealjs/cell-4-output-1.png){width=481 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](DL_image_analysis_2_2_files/figure-revealjs/cell-4-output-2.png){width=498 height=431}\n:::\n:::\n\n\n## Tissue classification with the MoNuSAC dataset\n\n- [ ] Define the pre-processing pipeline using the transforms from the pre-trained InceptionV3 model\n\n::: {#4b82a480 .cell execution_count=5}\n``` {.python .cell-code}\nimport torchvision\nfrom torchvision.transforms.v2 import Compose, ToTensor\n\ninception_weights = torchvision.models.inception.Inception_V3_Weights.IMAGENET1K_V1\n\npipeline = Compose([\n  ToTensor(),\n  inception_weights.transforms()\n])\n\npipeline\n```\n:::\n\n\n---\n\n## Tissue classification with the MoNuSAC dataset\n\n- [ ] Create a dataset class to load the images from disk\n\n::: {#d62e451f .cell execution_count=6}\n``` {.python .cell-code}\nfrom torch.utils.data import Dataset\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, image_filenames, image_labels, transform=None):\n        self.image_filenames = image_filenames\n        self.image_labels = image_labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_labels)\n\n    def __getitem__(self, idx):\n        image = imread(self.image_filenames[idx])\n        if self.transform is not None:\n            image = self.transform(image)\n\n        label = self.image_labels[idx]\n\n        return image, label\n```\n:::\n\n\n---\n\n## Tissue classification with the MoNuSAC dataset\n\n- [ ] Create the training and test sets using the `CustomImageDataset` class.\n\n::: {#1022ca22 .cell execution_count=7}\n``` {.python .cell-code}\nfrom torch.utils.data import random_split\n\ntrain_ds = CustomImageDataset(train_images_fns, train_labels, pipeline)\ntest_ds = CustomImageDataset(test_images_fns, test_labels, pipeline)\n\ntrain_ds, val_ds = random_split(train_ds, [0.8, 0.2])\n\nprint(f\"Training images={len(train_ds)}\")\nprint(f\"Validation images={len(val_ds)}\")\nprint(f\"Test images={len(test_ds)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining images=168\nValidation images=41\nTest images=101\n```\n:::\n:::\n\n\n- [ ] Use a `DataLoader` to serve image batches from the datasets.\n\n::: {#47ed4416 .cell execution_count=8}\n``` {.python .cell-code}\nfrom torch.utils.data import DataLoader\n\ntrain_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_dl = DataLoader(val_ds, batch_size=128)\ntest_dl = DataLoader(test_ds, batch_size=128)\n```\n:::\n\n\n# Transfer learning from ImageNet to MoNuSAC\n\n---\n\n## Tissue classification with the MoNuSAC dataset\n\n- [ ] Use the pre-trained InceptionV3 model from `torchvision`\n\n::: {#330f1f5c .cell execution_count=9}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\n\ndl_model = torchvision.models.inception_v3(\n    inception_weights,\n    progress=True\n)\n\ndl_model.fc = nn.Identity()\n\ndl_model.eval()\n```\n:::\n\n\n---\n\n## Tissue classification with the MoNuSAC dataset\n\n- [ ] Replace the current classifier with a linear layer with the output number of classes of the MoNuSAS dataset.\n\n::: {#b05e8fbf .cell execution_count=10}\n``` {.python .cell-code}\ndl_model.fc = nn.Sequential(\n    nn.Linear(in_features=2048, out_features=32, bias=True),\n    nn.ReLU(),\n    nn.Linear(in_features=32, out_features=4, bias=True)\n)\n```\n:::\n\n\n- [ ] Freeze all layers but the MLP that serves as classifier.\n\n::: {#ea66fae2 .cell execution_count=11}\n``` {.python .cell-code}\nfor param in dl_model.parameters():\n    param.requires_grad = False\n\nfor param in dl_model.fc.parameters():\n    param.requires_grad = True\n```\n:::\n\n\n---\n\n## Tissue classification with the MoNuSAC dataset\n\n- [ ] Define the optimization method and loss function.\n\n::: {#46fbefad .cell execution_count=12}\n``` {.python .cell-code}\nimport torch.optim as optim\n\nif torch.cuda.is_available():\n    dl_model.cuda()\n\noptimizer = optim.Adam(dl_model.fc.parameters(), lr=0.001, weight_decay=0.0001)\n\nloss_fun = nn.CrossEntropyLoss()\n```\n:::\n\n\n---\n\n## Tissue classification with the MoNuSAC dataset {.scrollable}\n\n- [ ] Implement the training and validation steps\n\n::: {#99906507 .cell overflow-y='true' execution_count=13}\n``` {.python .cell-code}\nfrom torchmetrics.classification import Accuracy\n\ntrain_acc_metric = Accuracy(task=\"multiclass\", num_classes=4)\nval_acc_metric = Accuracy(task=\"multiclass\", num_classes=4)\n\nif torch.cuda.is_available():\n    train_acc_metric.cuda()\n    val_acc_metric.cuda()\n\nnum_epochs = 100\nfor e in range(num_epochs):\n    avg_train_loss = 0\n    total_train_samples = 0\n\n    dl_model.train()\n    for x, y in train_dl:\n        optimizer.zero_grad()\n\n        if torch.cuda.is_available():\n            x = x.cuda()\n            y = y.cuda()\n\n        y_hat = dl_model(x).logits\n\n        loss = loss_fun(y_hat, y)\n\n        loss.backward()\n\n        optimizer.step()\n\n        avg_train_loss += loss.item() * len(y)\n        total_train_samples += len(y)\n\n        train_acc_metric(y_hat.softmax(dim=1), y)\n\n    avg_train_loss /= total_train_samples\n    train_acc = train_acc_metric.compute()\n\n    avg_val_loss = 0\n    total_val_samples = 0\n\n    dl_model.eval()\n    with torch.no_grad():\n        for x, y in val_dl:\n            if torch.cuda.is_available():\n                x = x.cuda()\n                y = y.cuda()\n\n            y_hat = dl_model(x)\n\n            loss = loss_fun(y_hat, y)\n\n            avg_val_loss += loss.item() * len(y)\n            total_val_samples += len(y)\n\n            val_acc_metric(y_hat.softmax(dim=1), y)\n\n    avg_val_loss /= total_val_samples\n    val_acc = val_acc_metric.compute()\n\n    print(f\"[{(e + 1) / num_epochs: 2.2%}] Train loss={avg_train_loss: 2.4} (Acc={train_acc: 2.2%}), Validation loss={avg_val_loss: 2.4} (Acc={val_acc: 2.2%})\")\n\n    train_acc_metric.reset()\n    val_acc_metric.reset()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 1.00%] Train loss= 1.435 (Acc= 29.76%), Validation loss= 1.458 (Acc= 21.95%)\n[ 2.00%] Train loss= 1.357 (Acc= 28.57%), Validation loss= 1.408 (Acc= 24.39%)\n[ 3.00%] Train loss= 1.31 (Acc= 47.62%), Validation loss= 1.39 (Acc= 26.83%)\n[ 4.00%] Train loss= 1.291 (Acc= 33.33%), Validation loss= 1.455 (Acc= 24.39%)\n[ 5.00%] Train loss= 1.239 (Acc= 41.07%), Validation loss= 1.364 (Acc= 31.71%)\n[ 6.00%] Train loss= 1.223 (Acc= 61.31%), Validation loss= 1.381 (Acc= 34.15%)\n[ 7.00%] Train loss= 1.19 (Acc= 57.74%), Validation loss= 1.406 (Acc= 39.02%)\n[ 8.00%] Train loss= 1.166 (Acc= 53.57%), Validation loss= 1.428 (Acc= 36.59%)\n[ 9.00%] Train loss= 1.088 (Acc= 64.88%), Validation loss= 1.37 (Acc= 29.27%)\n[ 10.00%] Train loss= 1.069 (Acc= 67.86%), Validation loss= 1.352 (Acc= 39.02%)\n[ 11.00%] Train loss= 1.011 (Acc= 65.48%), Validation loss= 1.358 (Acc= 43.90%)\n[ 12.00%] Train loss= 0.9853 (Acc= 66.07%), Validation loss= 1.382 (Acc= 36.59%)\n[ 13.00%] Train loss= 0.946 (Acc= 73.81%), Validation loss= 1.325 (Acc= 41.46%)\n[ 14.00%] Train loss= 0.9135 (Acc= 75.60%), Validation loss= 1.329 (Acc= 41.46%)\n[ 15.00%] Train loss= 0.8952 (Acc= 75.00%), Validation loss= 1.306 (Acc= 41.46%)\n[ 16.00%] Train loss= 0.8605 (Acc= 76.79%), Validation loss= 1.299 (Acc= 43.90%)\n[ 17.00%] Train loss= 0.8389 (Acc= 76.79%), Validation loss= 1.275 (Acc= 41.46%)\n[ 18.00%] Train loss= 0.8207 (Acc= 76.19%), Validation loss= 1.357 (Acc= 39.02%)\n[ 19.00%] Train loss= 0.7796 (Acc= 73.81%), Validation loss= 1.31 (Acc= 43.90%)\n[ 20.00%] Train loss= 0.6983 (Acc= 86.31%), Validation loss= 1.325 (Acc= 41.46%)\n[ 21.00%] Train loss= 0.7003 (Acc= 81.55%), Validation loss= 1.351 (Acc= 41.46%)\n[ 22.00%] Train loss= 0.6698 (Acc= 85.12%), Validation loss= 1.3 (Acc= 43.90%)\n[ 23.00%] Train loss= 0.7269 (Acc= 74.40%), Validation loss= 1.33 (Acc= 43.90%)\n[ 24.00%] Train loss= 0.696 (Acc= 81.55%), Validation loss= 1.325 (Acc= 36.59%)\n[ 25.00%] Train loss= 0.6905 (Acc= 80.36%), Validation loss= 1.51 (Acc= 39.02%)\n[ 26.00%] Train loss= 0.5891 (Acc= 79.76%), Validation loss= 1.303 (Acc= 43.90%)\n[ 27.00%] Train loss= 0.635 (Acc= 76.79%), Validation loss= 1.276 (Acc= 36.59%)\n[ 28.00%] Train loss= 0.5582 (Acc= 86.90%), Validation loss= 1.47 (Acc= 39.02%)\n[ 29.00%] Train loss= 0.6385 (Acc= 75.00%), Validation loss= 1.395 (Acc= 39.02%)\n[ 30.00%] Train loss= 0.5849 (Acc= 82.74%), Validation loss= 1.29 (Acc= 41.46%)\n[ 31.00%] Train loss= 0.6524 (Acc= 79.76%), Validation loss= 1.51 (Acc= 36.59%)\n[ 32.00%] Train loss= 0.6003 (Acc= 80.95%), Validation loss= 1.336 (Acc= 41.46%)\n[ 33.00%] Train loss= 0.4973 (Acc= 87.50%), Validation loss= 1.292 (Acc= 46.34%)\n[ 34.00%] Train loss= 0.5211 (Acc= 83.93%), Validation loss= 1.427 (Acc= 46.34%)\n[ 35.00%] Train loss= 0.4815 (Acc= 86.31%), Validation loss= 1.304 (Acc= 46.34%)\n[ 36.00%] Train loss= 0.522 (Acc= 86.90%), Validation loss= 1.346 (Acc= 46.34%)\n[ 37.00%] Train loss= 0.5074 (Acc= 80.95%), Validation loss= 1.431 (Acc= 43.90%)\n[ 38.00%] Train loss= 0.4129 (Acc= 91.67%), Validation loss= 1.297 (Acc= 41.46%)\n[ 39.00%] Train loss= 0.517 (Acc= 85.12%), Validation loss= 1.315 (Acc= 41.46%)\n[ 40.00%] Train loss= 0.4432 (Acc= 85.71%), Validation loss= 1.414 (Acc= 41.46%)\n[ 41.00%] Train loss= 0.4182 (Acc= 88.10%), Validation loss= 1.33 (Acc= 43.90%)\n[ 42.00%] Train loss= 0.4394 (Acc= 88.10%), Validation loss= 1.345 (Acc= 41.46%)\n[ 43.00%] Train loss= 0.4791 (Acc= 83.93%), Validation loss= 1.416 (Acc= 43.90%)\n[ 44.00%] Train loss= 0.4631 (Acc= 85.71%), Validation loss= 1.45 (Acc= 39.02%)\n[ 45.00%] Train loss= 0.4765 (Acc= 80.95%), Validation loss= 1.37 (Acc= 46.34%)\n[ 46.00%] Train loss= 0.4197 (Acc= 87.50%), Validation loss= 1.345 (Acc= 41.46%)\n[ 47.00%] Train loss= 0.4168 (Acc= 85.71%), Validation loss= 1.396 (Acc= 41.46%)\n[ 48.00%] Train loss= 0.4388 (Acc= 88.10%), Validation loss= 1.397 (Acc= 43.90%)\n[ 49.00%] Train loss= 0.4328 (Acc= 88.10%), Validation loss= 1.459 (Acc= 41.46%)\n[ 50.00%] Train loss= 0.316 (Acc= 94.64%), Validation loss= 1.349 (Acc= 46.34%)\n[ 51.00%] Train loss= 0.3433 (Acc= 88.69%), Validation loss= 1.355 (Acc= 48.78%)\n[ 52.00%] Train loss= 0.3498 (Acc= 93.45%), Validation loss= 1.412 (Acc= 48.78%)\n[ 53.00%] Train loss= 0.4013 (Acc= 87.50%), Validation loss= 1.493 (Acc= 46.34%)\n[ 54.00%] Train loss= 0.371 (Acc= 87.50%), Validation loss= 1.328 (Acc= 48.78%)\n[ 55.00%] Train loss= 0.3857 (Acc= 89.29%), Validation loss= 1.527 (Acc= 51.22%)\n[ 56.00%] Train loss= 0.4304 (Acc= 83.93%), Validation loss= 1.565 (Acc= 48.78%)\n[ 57.00%] Train loss= 0.4279 (Acc= 84.52%), Validation loss= 1.412 (Acc= 41.46%)\n[ 58.00%] Train loss= 0.3969 (Acc= 85.71%), Validation loss= 1.428 (Acc= 41.46%)\n[ 59.00%] Train loss= 0.4317 (Acc= 85.12%), Validation loss= 1.45 (Acc= 46.34%)\n[ 60.00%] Train loss= 0.3444 (Acc= 89.29%), Validation loss= 1.392 (Acc= 48.78%)\n[ 61.00%] Train loss= 0.3795 (Acc= 89.88%), Validation loss= 1.534 (Acc= 43.90%)\n[ 62.00%] Train loss= 0.3485 (Acc= 88.10%), Validation loss= 1.568 (Acc= 34.15%)\n[ 63.00%] Train loss= 0.3696 (Acc= 89.88%), Validation loss= 1.35 (Acc= 51.22%)\n[ 64.00%] Train loss= 0.3626 (Acc= 88.69%), Validation loss= 1.393 (Acc= 41.46%)\n[ 65.00%] Train loss= 0.3637 (Acc= 89.29%), Validation loss= 1.425 (Acc= 39.02%)\n[ 66.00%] Train loss= 0.3611 (Acc= 88.10%), Validation loss= 1.518 (Acc= 46.34%)\n[ 67.00%] Train loss= 0.3005 (Acc= 91.07%), Validation loss= 1.632 (Acc= 34.15%)\n[ 68.00%] Train loss= 0.3463 (Acc= 89.29%), Validation loss= 1.438 (Acc= 51.22%)\n[ 69.00%] Train loss= 0.3334 (Acc= 91.07%), Validation loss= 1.385 (Acc= 48.78%)\n[ 70.00%] Train loss= 0.3236 (Acc= 89.88%), Validation loss= 1.557 (Acc= 46.34%)\n[ 71.00%] Train loss= 0.3246 (Acc= 89.29%), Validation loss= 1.571 (Acc= 34.15%)\n[ 72.00%] Train loss= 0.2648 (Acc= 91.07%), Validation loss= 1.613 (Acc= 39.02%)\n[ 73.00%] Train loss= 0.3297 (Acc= 94.05%), Validation loss= 1.544 (Acc= 43.90%)\n[ 74.00%] Train loss= 0.3139 (Acc= 91.67%), Validation loss= 1.499 (Acc= 41.46%)\n[ 75.00%] Train loss= 0.3491 (Acc= 88.10%), Validation loss= 1.674 (Acc= 41.46%)\n[ 76.00%] Train loss= 0.3893 (Acc= 85.71%), Validation loss= 1.603 (Acc= 43.90%)\n[ 77.00%] Train loss= 0.3154 (Acc= 88.69%), Validation loss= 1.612 (Acc= 48.78%)\n[ 78.00%] Train loss= 0.4907 (Acc= 83.93%), Validation loss= 1.438 (Acc= 39.02%)\n[ 79.00%] Train loss= 0.2842 (Acc= 92.86%), Validation loss= 2.057 (Acc= 29.27%)\n[ 80.00%] Train loss= 0.4921 (Acc= 81.55%), Validation loss= 1.594 (Acc= 41.46%)\n[ 81.00%] Train loss= 0.3356 (Acc= 89.29%), Validation loss= 1.426 (Acc= 41.46%)\n[ 82.00%] Train loss= 0.2576 (Acc= 93.45%), Validation loss= 1.452 (Acc= 48.78%)\n[ 83.00%] Train loss= 0.3234 (Acc= 89.29%), Validation loss= 1.633 (Acc= 39.02%)\n[ 84.00%] Train loss= 0.2623 (Acc= 90.48%), Validation loss= 1.415 (Acc= 51.22%)\n[ 85.00%] Train loss= 0.3058 (Acc= 89.29%), Validation loss= 1.477 (Acc= 48.78%)\n[ 86.00%] Train loss= 0.2876 (Acc= 89.88%), Validation loss= 1.536 (Acc= 41.46%)\n[ 87.00%] Train loss= 0.3292 (Acc= 89.29%), Validation loss= 1.528 (Acc= 41.46%)\n[ 88.00%] Train loss= 0.2311 (Acc= 92.86%), Validation loss= 1.574 (Acc= 43.90%)\n[ 89.00%] Train loss= 0.2575 (Acc= 93.45%), Validation loss= 1.494 (Acc= 48.78%)\n[ 90.00%] Train loss= 0.3032 (Acc= 90.48%), Validation loss= 1.603 (Acc= 39.02%)\n[ 91.00%] Train loss= 0.3307 (Acc= 89.88%), Validation loss= 1.552 (Acc= 41.46%)\n[ 92.00%] Train loss= 0.3068 (Acc= 90.48%), Validation loss= 1.69 (Acc= 43.90%)\n[ 93.00%] Train loss= 0.3022 (Acc= 88.10%), Validation loss= 1.665 (Acc= 46.34%)\n[ 94.00%] Train loss= 0.3037 (Acc= 91.07%), Validation loss= 1.711 (Acc= 39.02%)\n[ 95.00%] Train loss= 0.2834 (Acc= 92.86%), Validation loss= 1.71 (Acc= 39.02%)\n[ 96.00%] Train loss= 0.3441 (Acc= 89.88%), Validation loss= 1.539 (Acc= 41.46%)\n[ 97.00%] Train loss= 0.2494 (Acc= 92.26%), Validation loss= 1.643 (Acc= 41.46%)\n[ 98.00%] Train loss= 0.3221 (Acc= 88.10%), Validation loss= 1.902 (Acc= 36.59%)\n[ 99.00%] Train loss= 0.2736 (Acc= 91.67%), Validation loss= 1.635 (Acc= 36.59%)\n[ 100.00%] Train loss= 0.2898 (Acc= 88.10%), Validation loss= 1.58 (Acc= 39.02%)\n```\n:::\n:::\n\n\n---\n\n## Tissue classification with the MoNuSAC dataset\n\n- [ ] Compute the performance of the model on the test set\n\n::: {#5fb2b42d .cell execution_count=14}\n``` {.python .cell-code}\navg_test_loss = 0\ntotal_test_samples = 0\n\ntest_acc_metric = Accuracy(task=\"multiclass\", num_classes=4)\n\nif torch.cuda.is_available():\n    test_acc_metric.cuda()\n\ndl_model.eval()\nwith torch.no_grad():\n    for x, y in test_dl:\n        if torch.cuda.is_available():\n            x = x.cuda()\n            y = y.cuda()\n\n        y_hat = dl_model(x)\n\n        loss = loss_fun(y_hat, y)\n\n        avg_test_loss += loss.item() * len(y)\n        total_test_samples += len(y)\n\n        test_acc_metric(y_hat.softmax(dim=1), y)\n\navg_test_loss /= total_test_samples\ntest_acc = test_acc_metric.compute()\n\nprint(f\"Test loss={avg_test_loss: 2.4} (Acc={test_acc: 2.2%})\")\n\ntest_acc_metric.reset()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest loss= 1.737 (Acc= 37.62%)\n```\n:::\n:::\n\n\n---\n\n## Tissue classification with the MoNuSAC dataset\n\n- [ ] Save the current state of the model\n\n::: {#512076eb .cell execution_count=15}\n``` {.python .cell-code}\ncheckpoint = dl_model.state_dict()\ntorch.save(checkpoint, \"monusac_checkpoint.pt\")\n```\n:::\n\n\n# Explore the MoNuSAC dataset in the embedded feature space\n\n---\n\n## Explore the MoNuSAC dataset in the embedded feature space\n\n- [ ] Extract the *embedded* features from the train set.\n\n::: {#1534e79d .cell execution_count=16}\n``` {.python .cell-code}\nimport numpy as np\n\nif torch.cuda.is_available():\n    dl_model.cuda()\n    dl_model.fc.cuda()\n\ntrain_features = []\ntrain_labels = []\n\ndl_model.fc[-1] = nn.Identity()\n\ndl_model.eval()\nwith torch.no_grad():\n    for x, y in train_dl:\n        if torch.cuda.is_available():\n            x = x.cuda()\n\n        fx = dl_model(x).cpu().detach().numpy()\n\n        train_features.append(fx)\n        train_labels.append(y.numpy())\n\ntrain_features = np.concatenate(train_features, 0)\ntrain_labels = np.concatenate(train_labels, 0)\n```\n:::\n\n\n---\n\n## Explore the MoNuSAC dataset in the embedded feature space\n\n- [ ] Project the features into a two-dimensional UMap.\n\n::: {#163576ec .cell execution_count=17}\n``` {.python .cell-code}\nimport umap\n\nreducer = umap.UMAP()\n\nembedding = reducer.fit_transform(train_features)\n\nembedding.shape\n```\n:::\n\n\n---\n\n## Explore the MoNuSAC dataset in the embedded feature space\n\n- [ ] Show the embedded space.\n\n::: {#6bc5ac3a .cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\"}\nimport matplotlib.pyplot as plt\n\nemb_plot = plt.scatter(embedding[:, 0], embedding[:, 1], c=train_labels, marker=\"o\")\n\nplt.legend(handles=emb_plot.legend_elements()[0], labels=tissue_classes)\nplt.gca().set_aspect('equal', 'datalim')\nplt.title('UMAP projection of InceptionV3 features of the MoNuSAC dataset', fontsize=24)\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\nText(0.5, 1.0, 'UMAP projection of InceptionV3 features of the MoNuSAC dataset')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](DL_image_analysis_2_2_files/figure-revealjs/cell-18-output-2.png){width=1067 height=444}\n:::\n:::\n\n\n---\n\n## Explore the MoNuSAC dataset in the embedded feature space\n\n- [ ] Extract the *embedded* features from the validation and test sets.\n\n::: {#ddd4eb1e .cell overflow-y='true' execution_count=19}\n``` {.python .cell-code}\nval_features = []\nval_labels = []\n\ntest_features = []\ntest_labels = []\n\nwith torch.no_grad():\n    for x, y in val_dl:\n        if torch.cuda.is_available():\n            x = x.cuda()\n\n        fx = dl_model(x).cpu().detach().numpy()\n\n        val_features.append(fx)\n        val_labels.append(y.numpy())\n\nval_features = np.concatenate(val_features, 0)\nval_labels = np.concatenate(val_labels, 0)\n\nwith torch.no_grad():\n    for x, y in test_dl:\n        if torch.cuda.is_available():\n            x = x.cuda()\n\n        fx = dl_model(x).cpu().detach().numpy()\n\n        test_features.append(fx)\n        test_labels.append(y.numpy())\n\ntest_features = np.concatenate(test_features, 0)\ntest_labels = np.concatenate(test_labels, 0)\n```\n:::\n\n\n---\n\n## Explore the MoNuSAC dataset in the embedded feature space {.scrollable}\n\n- [ ] Project the features of the test and validation sets into a two-dimensional UMap.\n\n::: {#2d7ade92 .cell execution_count=20}\n``` {.python .cell-code}\nembedding_val = reducer.transform(val_features)\nembedding_test = reducer.transform(test_features)\n```\n:::\n\n\n- [ ] Show the embedded space.\n\n::: {#8f258d7c .cell execution_count=21}\n``` {.python .cell-code code-fold=\"true\"}\nfig, (ax_0, ax_1) = plt.subplots(1, 2)\n\nemb_train_plot = ax_0.scatter(embedding[:, 0], embedding[:, 1], c=train_labels, marker=\"o\", alpha=0.5)\nlegend_train = ax_0.legend(handles=emb_train_plot.legend_elements()[0], labels=tissue_classes, loc=\"lower left\", title=\"Train dataset\")\nax_0.add_artist(legend_train)\n\nemb_val_plot = ax_0.scatter(embedding_val[:, 0], embedding_val[:, 1], c=val_labels,  marker=\"v\", alpha=0.5)\nlegend_val = ax_0.legend(handles=emb_val_plot.legend_elements()[0], labels=tissue_classes, loc=\"lower right\", title=\"Validation dataset\")\nax_0.add_artist(legend_val)\n\nax_0.set_aspect('equal', 'datalim')\n\nemb_train_plot = ax_1.scatter(embedding[:, 0], embedding[:, 1], c=train_labels, marker=\"o\", alpha=0.5)\nlegend_train = ax_1.legend(handles=emb_train_plot.legend_elements()[0], labels=tissue_classes, loc=\"lower left\", title=\"Train dataset\")\nax_1.add_artist(legend_train)\n\nemb_test_plot = ax_1.scatter(embedding_test[:, 0], embedding_test[:, 1], c=test_labels, marker=\"s\", alpha=0.5)\nlegend_test = ax_1.legend(handles=emb_test_plot.legend_elements()[0], labels=tissue_classes, loc=\"lower right\", title=\"Test dataset\")\nax_1.add_artist(legend_test)\n\nax_1.set_aspect('equal', 'datalim')\nplt.title('UMAP projection of InceptionV3 features of the MoNuSAC dataset', fontsize=24)\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\nText(0.5, 1.0, 'UMAP projection of InceptionV3 features of the MoNuSAC dataset')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](DL_image_analysis_2_2_files/figure-revealjs/cell-21-output-2.png){width=1156 height=444}\n:::\n:::\n\n\n",
    "supporting": [
      "DL_image_analysis_2_2_files"
    ],
    "filters": [],
    "includes": {}
  }
}