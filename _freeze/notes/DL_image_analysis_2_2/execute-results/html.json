{
  "hash": "3d08076734e887c4b904cdccd9785a3f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Advanced Machine Learning with Python (Session 2 - Part 2)\nauthor: Fernando Cervantes (fernando.cervantes@jax.org)\nformat:\n  revealjs:\n    code-fold: false\n    progress: true\n    controls: true\n    output-file: \"Adv_ML_Python_presentation_2_2\"\n    fontsize: 20pt\n    include-in-header:\n      text: |\n        <link href=\"https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.3/css/bootstrap.min.css\" rel =\"stylesheet\" integrity=\"sha512-jnSuA4Ss2PkkikSOLtYs8BlYIeeIK1h99ty4YfvRPAlzr377vr3CXDb7sb7eEEBYjDtcYj+AjBH3FLv5uSJuXg==\" crossorigin=\"anonymous\">\n\nexecute:\n  error: true\n  echo: true\n  cache: true\n  freeze: true\n  keep-ipynb: true\n\njupyter: python3\n---\n\n\n## Materials\n\n[Open notebook in Colab](https://colab.research.google.com/gist/fercer/8e5262d5df22d5b55468642855e0d1b8/advanced_machine_learning_with_python_session_2_2.ipynb){.btn .btn-outline-primary .btn role=\"button\" target=‚Äù_blank‚Äù}\n\n\n# Working with Transformers\n\n---\n\n## Review the architecture of a Vision Transformer (ViT) {.scrollable}\n\n![Dosovitskiy, Alexey et al. ‚ÄúAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.‚Äù ArXiv abs/2010.11929 (2020)](../imgs/ViT.png)\n\n* https://docs.pytorch.org/vision/stable/models/vision_transformer.html\n\n- [ ] Review the Attention mechanism in transformer models\n\n![](../imgs/Attn_layer.png)\n\n![Vaswani, Ashish et al. ‚ÄúAttention is All you Need.‚Äù Neural Information Processing Systems (2017).](../imgs/Attn_operation.png)\n\n---\n\n## Exercise: Use a pre-trained ViT model to classify images {.scrollable}\n\n- [ ] Import the pre-trained weights of the Inception V3 model from models.inception_v3\n\n::: {#4e9a0a70 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nfrom torchvision import models\n\ntransformer_weights = models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1\n\ntransformer_weights.meta\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n{'categories': ['tench',\n  'goldfish',\n  'great white shark',\n  'tiger shark',\n  'hammerhead',\n  'electric ray',\n  'stingray',\n  'cock',\n  'hen',\n  'ostrich',\n  'brambling',\n  'goldfinch',\n  'house finch',\n  'junco',\n  'indigo bunting',\n  'robin',\n  'bulbul',\n  'jay',\n  'magpie',\n  'chickadee',\n  'water ouzel',\n  'kite',\n  'bald eagle',\n  'vulture',\n  'great grey owl',\n  'European fire salamander',\n  'common newt',\n  'eft',\n  'spotted salamander',\n  'axolotl',\n  'bullfrog',\n  'tree frog',\n  'tailed frog',\n  'loggerhead',\n  'leatherback turtle',\n  'mud turtle',\n  'terrapin',\n  'box turtle',\n  'banded gecko',\n  'common iguana',\n  'American chameleon',\n  'whiptail',\n  'agama',\n  'frilled lizard',\n  'alligator lizard',\n  'Gila monster',\n  'green lizard',\n  'African chameleon',\n  'Komodo dragon',\n  'African crocodile',\n  'American alligator',\n  'triceratops',\n  'thunder snake',\n  'ringneck snake',\n  'hognose snake',\n  'green snake',\n  'king snake',\n  'garter snake',\n  'water snake',\n  'vine snake',\n  'night snake',\n  'boa constrictor',\n  'rock python',\n  'Indian cobra',\n  'green mamba',\n  'sea snake',\n  'horned viper',\n  'diamondback',\n  'sidewinder',\n  'trilobite',\n  'harvestman',\n  'scorpion',\n  'black and gold garden spider',\n  'barn spider',\n  'garden spider',\n  'black widow',\n  'tarantula',\n  'wolf spider',\n  'tick',\n  'centipede',\n  'black grouse',\n  'ptarmigan',\n  'ruffed grouse',\n  'prairie chicken',\n  'peacock',\n  'quail',\n  'partridge',\n  'African grey',\n  'macaw',\n  'sulphur-crested cockatoo',\n  'lorikeet',\n  'coucal',\n  'bee eater',\n  'hornbill',\n  'hummingbird',\n  'jacamar',\n  'toucan',\n  'drake',\n  'red-breasted merganser',\n  'goose',\n  'black swan',\n  'tusker',\n  'echidna',\n  'platypus',\n  'wallaby',\n  'koala',\n  'wombat',\n  'jellyfish',\n  'sea anemone',\n  'brain coral',\n  'flatworm',\n  'nematode',\n  'conch',\n  'snail',\n  'slug',\n  'sea slug',\n  'chiton',\n  'chambered nautilus',\n  'Dungeness crab',\n  'rock crab',\n  'fiddler crab',\n  'king crab',\n  'American lobster',\n  'spiny lobster',\n  'crayfish',\n  'hermit crab',\n  'isopod',\n  'white stork',\n  'black stork',\n  'spoonbill',\n  'flamingo',\n  'little blue heron',\n  'American egret',\n  'bittern',\n  'crane bird',\n  'limpkin',\n  'European gallinule',\n  'American coot',\n  'bustard',\n  'ruddy turnstone',\n  'red-backed sandpiper',\n  'redshank',\n  'dowitcher',\n  'oystercatcher',\n  'pelican',\n  'king penguin',\n  'albatross',\n  'grey whale',\n  'killer whale',\n  'dugong',\n  'sea lion',\n  'Chihuahua',\n  'Japanese spaniel',\n  'Maltese dog',\n  'Pekinese',\n  'Shih-Tzu',\n  'Blenheim spaniel',\n  'papillon',\n  'toy terrier',\n  'Rhodesian ridgeback',\n  'Afghan hound',\n  'basset',\n  'beagle',\n  'bloodhound',\n  'bluetick',\n  'black-and-tan coonhound',\n  'Walker hound',\n  'English foxhound',\n  'redbone',\n  'borzoi',\n  'Irish wolfhound',\n  'Italian greyhound',\n  'whippet',\n  'Ibizan hound',\n  'Norwegian elkhound',\n  'otterhound',\n  'Saluki',\n  'Scottish deerhound',\n  'Weimaraner',\n  'Staffordshire bullterrier',\n  'American Staffordshire terrier',\n  'Bedlington terrier',\n  'Border terrier',\n  'Kerry blue terrier',\n  'Irish terrier',\n  'Norfolk terrier',\n  'Norwich terrier',\n  'Yorkshire terrier',\n  'wire-haired fox terrier',\n  'Lakeland terrier',\n  'Sealyham terrier',\n  'Airedale',\n  'cairn',\n  'Australian terrier',\n  'Dandie Dinmont',\n  'Boston bull',\n  'miniature schnauzer',\n  'giant schnauzer',\n  'standard schnauzer',\n  'Scotch terrier',\n  'Tibetan terrier',\n  'silky terrier',\n  'soft-coated wheaten terrier',\n  'West Highland white terrier',\n  'Lhasa',\n  'flat-coated retriever',\n  'curly-coated retriever',\n  'golden retriever',\n  'Labrador retriever',\n  'Chesapeake Bay retriever',\n  'German short-haired pointer',\n  'vizsla',\n  'English setter',\n  'Irish setter',\n  'Gordon setter',\n  'Brittany spaniel',\n  'clumber',\n  'English springer',\n  'Welsh springer spaniel',\n  'cocker spaniel',\n  'Sussex spaniel',\n  'Irish water spaniel',\n  'kuvasz',\n  'schipperke',\n  'groenendael',\n  'malinois',\n  'briard',\n  'kelpie',\n  'komondor',\n  'Old English sheepdog',\n  'Shetland sheepdog',\n  'collie',\n  'Border collie',\n  'Bouvier des Flandres',\n  'Rottweiler',\n  'German shepherd',\n  'Doberman',\n  'miniature pinscher',\n  'Greater Swiss Mountain dog',\n  'Bernese mountain dog',\n  'Appenzeller',\n  'EntleBucher',\n  'boxer',\n  'bull mastiff',\n  'Tibetan mastiff',\n  'French bulldog',\n  'Great Dane',\n  'Saint Bernard',\n  'Eskimo dog',\n  'malamute',\n  'Siberian husky',\n  'dalmatian',\n  'affenpinscher',\n  'basenji',\n  'pug',\n  'Leonberg',\n  'Newfoundland',\n  'Great Pyrenees',\n  'Samoyed',\n  'Pomeranian',\n  'chow',\n  'keeshond',\n  'Brabancon griffon',\n  'Pembroke',\n  'Cardigan',\n  'toy poodle',\n  'miniature poodle',\n  'standard poodle',\n  'Mexican hairless',\n  'timber wolf',\n  'white wolf',\n  'red wolf',\n  'coyote',\n  'dingo',\n  'dhole',\n  'African hunting dog',\n  'hyena',\n  'red fox',\n  'kit fox',\n  'Arctic fox',\n  'grey fox',\n  'tabby',\n  'tiger cat',\n  'Persian cat',\n  'Siamese cat',\n  'Egyptian cat',\n  'cougar',\n  'lynx',\n  'leopard',\n  'snow leopard',\n  'jaguar',\n  'lion',\n  'tiger',\n  'cheetah',\n  'brown bear',\n  'American black bear',\n  'ice bear',\n  'sloth bear',\n  'mongoose',\n  'meerkat',\n  'tiger beetle',\n  'ladybug',\n  'ground beetle',\n  'long-horned beetle',\n  'leaf beetle',\n  'dung beetle',\n  'rhinoceros beetle',\n  'weevil',\n  'fly',\n  'bee',\n  'ant',\n  'grasshopper',\n  'cricket',\n  'walking stick',\n  'cockroach',\n  'mantis',\n  'cicada',\n  'leafhopper',\n  'lacewing',\n  'dragonfly',\n  'damselfly',\n  'admiral',\n  'ringlet',\n  'monarch',\n  'cabbage butterfly',\n  'sulphur butterfly',\n  'lycaenid',\n  'starfish',\n  'sea urchin',\n  'sea cucumber',\n  'wood rabbit',\n  'hare',\n  'Angora',\n  'hamster',\n  'porcupine',\n  'fox squirrel',\n  'marmot',\n  'beaver',\n  'guinea pig',\n  'sorrel',\n  'zebra',\n  'hog',\n  'wild boar',\n  'warthog',\n  'hippopotamus',\n  'ox',\n  'water buffalo',\n  'bison',\n  'ram',\n  'bighorn',\n  'ibex',\n  'hartebeest',\n  'impala',\n  'gazelle',\n  'Arabian camel',\n  'llama',\n  'weasel',\n  'mink',\n  'polecat',\n  'black-footed ferret',\n  'otter',\n  'skunk',\n  'badger',\n  'armadillo',\n  'three-toed sloth',\n  'orangutan',\n  'gorilla',\n  'chimpanzee',\n  'gibbon',\n  'siamang',\n  'guenon',\n  'patas',\n  'baboon',\n  'macaque',\n  'langur',\n  'colobus',\n  'proboscis monkey',\n  'marmoset',\n  'capuchin',\n  'howler monkey',\n  'titi',\n  'spider monkey',\n  'squirrel monkey',\n  'Madagascar cat',\n  'indri',\n  'Indian elephant',\n  'African elephant',\n  'lesser panda',\n  'giant panda',\n  'barracouta',\n  'eel',\n  'coho',\n  'rock beauty',\n  'anemone fish',\n  'sturgeon',\n  'gar',\n  'lionfish',\n  'puffer',\n  'abacus',\n  'abaya',\n  'academic gown',\n  'accordion',\n  'acoustic guitar',\n  'aircraft carrier',\n  'airliner',\n  'airship',\n  'altar',\n  'ambulance',\n  'amphibian',\n  'analog clock',\n  'apiary',\n  'apron',\n  'ashcan',\n  'assault rifle',\n  'backpack',\n  'bakery',\n  'balance beam',\n  'balloon',\n  'ballpoint',\n  'Band Aid',\n  'banjo',\n  'bannister',\n  'barbell',\n  'barber chair',\n  'barbershop',\n  'barn',\n  'barometer',\n  'barrel',\n  'barrow',\n  'baseball',\n  'basketball',\n  'bassinet',\n  'bassoon',\n  'bathing cap',\n  'bath towel',\n  'bathtub',\n  'beach wagon',\n  'beacon',\n  'beaker',\n  'bearskin',\n  'beer bottle',\n  'beer glass',\n  'bell cote',\n  'bib',\n  'bicycle-built-for-two',\n  'bikini',\n  'binder',\n  'binoculars',\n  'birdhouse',\n  'boathouse',\n  'bobsled',\n  'bolo tie',\n  'bonnet',\n  'bookcase',\n  'bookshop',\n  'bottlecap',\n  'bow',\n  'bow tie',\n  'brass',\n  'brassiere',\n  'breakwater',\n  'breastplate',\n  'broom',\n  'bucket',\n  'buckle',\n  'bulletproof vest',\n  'bullet train',\n  'butcher shop',\n  'cab',\n  'caldron',\n  'candle',\n  'cannon',\n  'canoe',\n  'can opener',\n  'cardigan',\n  'car mirror',\n  'carousel',\n  \"carpenter's kit\",\n  'carton',\n  'car wheel',\n  'cash machine',\n  'cassette',\n  'cassette player',\n  'castle',\n  'catamaran',\n  'CD player',\n  'cello',\n  'cellular telephone',\n  'chain',\n  'chainlink fence',\n  'chain mail',\n  'chain saw',\n  'chest',\n  'chiffonier',\n  'chime',\n  'china cabinet',\n  'Christmas stocking',\n  'church',\n  'cinema',\n  'cleaver',\n  'cliff dwelling',\n  'cloak',\n  'clog',\n  'cocktail shaker',\n  'coffee mug',\n  'coffeepot',\n  'coil',\n  'combination lock',\n  'computer keyboard',\n  'confectionery',\n  'container ship',\n  'convertible',\n  'corkscrew',\n  'cornet',\n  'cowboy boot',\n  'cowboy hat',\n  'cradle',\n  'crane',\n  'crash helmet',\n  'crate',\n  'crib',\n  'Crock Pot',\n  'croquet ball',\n  'crutch',\n  'cuirass',\n  'dam',\n  'desk',\n  'desktop computer',\n  'dial telephone',\n  'diaper',\n  'digital clock',\n  'digital watch',\n  'dining table',\n  'dishrag',\n  'dishwasher',\n  'disk brake',\n  'dock',\n  'dogsled',\n  'dome',\n  'doormat',\n  'drilling platform',\n  'drum',\n  'drumstick',\n  'dumbbell',\n  'Dutch oven',\n  'electric fan',\n  'electric guitar',\n  'electric locomotive',\n  'entertainment center',\n  'envelope',\n  'espresso maker',\n  'face powder',\n  'feather boa',\n  'file',\n  'fireboat',\n  'fire engine',\n  'fire screen',\n  'flagpole',\n  'flute',\n  'folding chair',\n  'football helmet',\n  'forklift',\n  'fountain',\n  'fountain pen',\n  'four-poster',\n  'freight car',\n  'French horn',\n  'frying pan',\n  'fur coat',\n  'garbage truck',\n  'gasmask',\n  'gas pump',\n  'goblet',\n  'go-kart',\n  'golf ball',\n  'golfcart',\n  'gondola',\n  'gong',\n  'gown',\n  'grand piano',\n  'greenhouse',\n  'grille',\n  'grocery store',\n  'guillotine',\n  'hair slide',\n  'hair spray',\n  'half track',\n  'hammer',\n  'hamper',\n  'hand blower',\n  'hand-held computer',\n  'handkerchief',\n  'hard disc',\n  'harmonica',\n  'harp',\n  'harvester',\n  'hatchet',\n  'holster',\n  'home theater',\n  'honeycomb',\n  'hook',\n  'hoopskirt',\n  'horizontal bar',\n  'horse cart',\n  'hourglass',\n  'iPod',\n  'iron',\n  \"jack-o'-lantern\",\n  'jean',\n  'jeep',\n  'jersey',\n  'jigsaw puzzle',\n  'jinrikisha',\n  'joystick',\n  'kimono',\n  'knee pad',\n  'knot',\n  'lab coat',\n  'ladle',\n  'lampshade',\n  'laptop',\n  'lawn mower',\n  'lens cap',\n  'letter opener',\n  'library',\n  'lifeboat',\n  'lighter',\n  'limousine',\n  'liner',\n  'lipstick',\n  'Loafer',\n  'lotion',\n  'loudspeaker',\n  'loupe',\n  'lumbermill',\n  'magnetic compass',\n  'mailbag',\n  'mailbox',\n  'maillot',\n  'maillot tank suit',\n  'manhole cover',\n  'maraca',\n  'marimba',\n  'mask',\n  'matchstick',\n  'maypole',\n  'maze',\n  'measuring cup',\n  'medicine chest',\n  'megalith',\n  'microphone',\n  'microwave',\n  'military uniform',\n  'milk can',\n  'minibus',\n  'miniskirt',\n  'minivan',\n  'missile',\n  'mitten',\n  'mixing bowl',\n  'mobile home',\n  'Model T',\n  'modem',\n  'monastery',\n  'monitor',\n  'moped',\n  'mortar',\n  'mortarboard',\n  'mosque',\n  'mosquito net',\n  'motor scooter',\n  'mountain bike',\n  'mountain tent',\n  'mouse',\n  'mousetrap',\n  'moving van',\n  'muzzle',\n  'nail',\n  'neck brace',\n  'necklace',\n  'nipple',\n  'notebook',\n  'obelisk',\n  'oboe',\n  'ocarina',\n  'odometer',\n  'oil filter',\n  'organ',\n  'oscilloscope',\n  'overskirt',\n  'oxcart',\n  'oxygen mask',\n  'packet',\n  'paddle',\n  'paddlewheel',\n  'padlock',\n  'paintbrush',\n  'pajama',\n  'palace',\n  'panpipe',\n  'paper towel',\n  'parachute',\n  'parallel bars',\n  'park bench',\n  'parking meter',\n  'passenger car',\n  'patio',\n  'pay-phone',\n  'pedestal',\n  'pencil box',\n  'pencil sharpener',\n  'perfume',\n  'Petri dish',\n  'photocopier',\n  'pick',\n  'pickelhaube',\n  'picket fence',\n  'pickup',\n  'pier',\n  'piggy bank',\n  'pill bottle',\n  'pillow',\n  'ping-pong ball',\n  'pinwheel',\n  'pirate',\n  'pitcher',\n  'plane',\n  'planetarium',\n  'plastic bag',\n  'plate rack',\n  'plow',\n  'plunger',\n  'Polaroid camera',\n  'pole',\n  'police van',\n  'poncho',\n  'pool table',\n  'pop bottle',\n  'pot',\n  \"potter's wheel\",\n  'power drill',\n  'prayer rug',\n  'printer',\n  'prison',\n  'projectile',\n  'projector',\n  'puck',\n  'punching bag',\n  'purse',\n  'quill',\n  'quilt',\n  'racer',\n  'racket',\n  'radiator',\n  'radio',\n  'radio telescope',\n  'rain barrel',\n  'recreational vehicle',\n  'reel',\n  'reflex camera',\n  'refrigerator',\n  'remote control',\n  'restaurant',\n  'revolver',\n  'rifle',\n  'rocking chair',\n  'rotisserie',\n  'rubber eraser',\n  'rugby ball',\n  'rule',\n  'running shoe',\n  'safe',\n  'safety pin',\n  'saltshaker',\n  'sandal',\n  'sarong',\n  'sax',\n  'scabbard',\n  'scale',\n  'school bus',\n  'schooner',\n  'scoreboard',\n  'screen',\n  'screw',\n  'screwdriver',\n  'seat belt',\n  'sewing machine',\n  'shield',\n  'shoe shop',\n  'shoji',\n  'shopping basket',\n  'shopping cart',\n  'shovel',\n  'shower cap',\n  'shower curtain',\n  'ski',\n  'ski mask',\n  'sleeping bag',\n  'slide rule',\n  'sliding door',\n  'slot',\n  'snorkel',\n  'snowmobile',\n  'snowplow',\n  'soap dispenser',\n  'soccer ball',\n  'sock',\n  'solar dish',\n  'sombrero',\n  'soup bowl',\n  'space bar',\n  'space heater',\n  'space shuttle',\n  'spatula',\n  'speedboat',\n  'spider web',\n  'spindle',\n  'sports car',\n  'spotlight',\n  'stage',\n  'steam locomotive',\n  'steel arch bridge',\n  'steel drum',\n  'stethoscope',\n  'stole',\n  'stone wall',\n  'stopwatch',\n  'stove',\n  'strainer',\n  'streetcar',\n  'stretcher',\n  'studio couch',\n  'stupa',\n  'submarine',\n  'suit',\n  'sundial',\n  'sunglass',\n  'sunglasses',\n  'sunscreen',\n  'suspension bridge',\n  'swab',\n  'sweatshirt',\n  'swimming trunks',\n  'swing',\n  'switch',\n  'syringe',\n  'table lamp',\n  'tank',\n  'tape player',\n  'teapot',\n  'teddy',\n  'television',\n  'tennis ball',\n  'thatch',\n  'theater curtain',\n  'thimble',\n  'thresher',\n  'throne',\n  'tile roof',\n  'toaster',\n  'tobacco shop',\n  'toilet seat',\n  'torch',\n  'totem pole',\n  'tow truck',\n  'toyshop',\n  'tractor',\n  'trailer truck',\n  'tray',\n  'trench coat',\n  'tricycle',\n  'trimaran',\n  'tripod',\n  'triumphal arch',\n  'trolleybus',\n  'trombone',\n  'tub',\n  'turnstile',\n  'typewriter keyboard',\n  'umbrella',\n  'unicycle',\n  'upright',\n  'vacuum',\n  'vase',\n  'vault',\n  'velvet',\n  'vending machine',\n  'vestment',\n  'viaduct',\n  'violin',\n  'volleyball',\n  'waffle iron',\n  'wall clock',\n  'wallet',\n  'wardrobe',\n  'warplane',\n  'washbasin',\n  'washer',\n  'water bottle',\n  'water jug',\n  'water tower',\n  'whiskey jug',\n  'whistle',\n  'wig',\n  'window screen',\n  'window shade',\n  'Windsor tie',\n  'wine bottle',\n  'wing',\n  'wok',\n  'wooden spoon',\n  'wool',\n  'worm fence',\n  'wreck',\n  'yawl',\n  'yurt',\n  'web site',\n  'comic book',\n  'crossword puzzle',\n  'street sign',\n  'traffic light',\n  'book jacket',\n  'menu',\n  'plate',\n  'guacamole',\n  'consomme',\n  'hot pot',\n  'trifle',\n  'ice cream',\n  'ice lolly',\n  'French loaf',\n  'bagel',\n  'pretzel',\n  'cheeseburger',\n  'hotdog',\n  'mashed potato',\n  'head cabbage',\n  'broccoli',\n  'cauliflower',\n  'zucchini',\n  'spaghetti squash',\n  'acorn squash',\n  'butternut squash',\n  'cucumber',\n  'artichoke',\n  'bell pepper',\n  'cardoon',\n  'mushroom',\n  'Granny Smith',\n  'strawberry',\n  'orange',\n  'lemon',\n  'fig',\n  'pineapple',\n  'banana',\n  'jackfruit',\n  'custard apple',\n  'pomegranate',\n  'hay',\n  'carbonara',\n  'chocolate sauce',\n  'dough',\n  'meat loaf',\n  'pizza',\n  'potpie',\n  'burrito',\n  'red wine',\n  'espresso',\n  'cup',\n  'eggnog',\n  'alp',\n  'bubble',\n  'cliff',\n  'coral reef',\n  'geyser',\n  'lakeside',\n  'promontory',\n  'sandbar',\n  'seashore',\n  'valley',\n  'volcano',\n  'ballplayer',\n  'groom',\n  'scuba diver',\n  'rapeseed',\n  'daisy',\n  \"yellow lady's slipper\",\n  'corn',\n  'acorn',\n  'hip',\n  'buckeye',\n  'coral fungus',\n  'agaric',\n  'gyromitra',\n  'stinkhorn',\n  'earthstar',\n  'hen-of-the-woods',\n  'bolete',\n  'ear',\n  'toilet tissue'],\n 'recipe': 'https://github.com/facebookresearch/SWAG',\n 'license': 'https://github.com/facebookresearch/SWAG/blob/main/LICENSE',\n 'num_params': 86859496,\n 'min_size': (384, 384),\n '_metrics': {'ImageNet-1K': {'acc@1': 85.304, 'acc@5': 97.65}},\n '_ops': 55.484,\n '_file_size': 331.398,\n '_docs': '\\n                These weights are learnt via transfer learning by end-to-end fine-tuning the original\\n                `SWAG <https://arxiv.org/abs/2201.08371>`_ weights on ImageNet-1K data.\\n            '}\n```\n:::\n:::\n\n\n- [ ] Store the categories in a variable to use them later\n\n::: {#8dbe6e2a .cell execution_count=2}\n``` {.python .cell-code}\ncategories = transformer_weights.meta[\"categories\"]\n```\n:::\n\n\n::: {.callout-tip}\nMore info about Vision Transformer implementation in `torchvision` [here](https://docs.pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html)\n:::\n\n---\n\n- [ ] Load the Inception V3 model using the pre-trained weights `inception_weights`\n\n::: {#af45d02d .cell execution_count=3}\n``` {.python .cell-code}\ndl_model = models.vit_b_16(transformer_weights, progress=True)\n\ndl_model.eval()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nVisionTransformer(\n  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.0, inplace=False)\n    (layers): Sequential(\n      (encoder_layer_0): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_1): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_2): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_3): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_4): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_5): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_6): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_7): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_8): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_9): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_10): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_11): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  )\n  (heads): Sequential(\n    (head): Linear(in_features=768, out_features=1000, bias=True)\n  )\n)\n```\n:::\n:::\n\n\n---\n\n- [ ] Load a sample image to predict its category\n\n::: {#0cb08424 .cell execution_count=4}\n``` {.python .cell-code}\nimport skimage\nimport matplotlib.pyplot as plt\n\nsample_im = skimage.data.rocket()\nsample_im.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n(427, 640, 3)\n```\n:::\n:::\n\n\n- [ ] Visualize the sample image\n\n::: {#238fad12 .cell execution_count=5}\n``` {.python .cell-code}\nplt.imshow(sample_im)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](DL_image_analysis_2_2_files/figure-revealjs/cell-6-output-1.png){width=608 height=416}\n:::\n:::\n\n\n---\n\n- [ ] Inspect what transforms are required by the pre-trained ViT model to work properly\n\n::: {#e85125c7 .cell execution_count=6}\n``` {.python .cell-code}\ntransformer_weights.transforms\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nfunctools.partial(<class 'torchvision.transforms._presets.ImageClassification'>, crop_size=384, resize_size=384, interpolation=<InterpolationMode.BICUBIC: 'bicubic'>)\n```\n:::\n:::\n\n\n::: {.callout-important}\n`functools.partial` is a function to define functions with static arguments. So üëÜ returns a function when it is called!\n:::\n\n::: {.callout-note}\nThe transforms used by the Inception V3 are\n\n  1. resize the image to 384x384 pixels, and\n\n  2. normalize the values of the RGB channels.\n:::\n\n---\n\n- [ ] Define a preprocessing pipeline using the inception_weights.transforms() method. Add also a transformation from `numpy` arrays into torch tensors.\n\n::: {#3ffdcd62 .cell execution_count=7}\n``` {.python .cell-code}\nfrom torchvision.transforms.v2 import Compose, ToTensor\n\npipeline = Compose([\n  ToTensor(),\n  transformer_weights.transforms()\n])\n\npipeline\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nCompose(\n      ToTensor()\n      ImageClassification(\n      crop_size=[384]\n      resize_size=[384]\n      mean=[0.485, 0.456, 0.406]\n      std=[0.229, 0.224, 0.225]\n      interpolation=InterpolationMode.BICUBIC\n  )\n)\n```\n:::\n:::\n\n\n---\n\n- [ ] Pre-process the sample image using our pipeline\n\n::: {#7c095f0a .cell execution_count=8}\n``` {.python .cell-code}\nsample_x = pipeline(sample_im)\ntype(sample_x), sample_x.shape, sample_x.min(), sample_x.max()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n(torch.Tensor, torch.Size([3, 384, 384]), tensor(-1.9139), tensor(2.8376))\n```\n:::\n:::\n\n\n---\n\n- [ ] Use the pre-trained model to predict the class of our sample image\n\n::: {.callout-caution}\nApply the model on sample_x[None, ...], so it is treated as a one-sample batch\n:::\n\n::: {#64d1cc97 .cell execution_count=9}\n``` {.python .cell-code}\nsample_y = dl_model(sample_x[None, ...])\n\nsample_y.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\ntorch.Size([1, 1000])\n```\n:::\n:::\n\n\n---\n\n- [ ] Show the categories with the highest *log-*probabilities.\n\n::: {#01f56eba .cell execution_count=10}\n``` {.python .cell-code}\nsample_y.argsort(dim=1)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\ntensor([[641, 646, 398, 926, 340, 429, 990, 645, 924, 469, 346, 885, 351, 116,\n         199,  62, 134, 661, 263, 678, 972, 594, 825, 976,  39, 397, 295, 880,\n         793, 396, 853, 973, 817, 347, 433, 227, 975,  84,  32, 160, 841, 993,\n         521, 462, 463, 544, 724, 518, 899,  25,  29, 832, 738, 982, 858, 290,\n         939, 238, 500, 334, 659, 306, 329, 869, 488,  48,  93, 639, 834, 624,\n         941, 843, 529, 966, 999, 343,  68, 801, 121, 809, 567, 373, 870,   0,\n         197, 323, 522, 652, 725, 479, 297,  50, 573, 390, 381, 350, 823,  45,\n         115, 344, 925, 515, 224, 865, 268, 235, 314, 706, 439, 173, 772, 453,\n         214, 177, 883, 301, 419, 406, 554,  82, 150,  30,  85,   7, 655, 308,\n         226, 849, 712, 369, 868, 828, 612, 460, 552, 109, 935, 959, 266,  88,\n         244, 225, 400, 703, 259, 997, 964, 808, 302, 236, 709, 223,   5, 157,\n          86, 649, 371, 179, 576, 928, 987, 960, 187,  36,  49, 417, 949, 291,\n         762,  42, 328, 723, 305, 281, 673, 275,  98, 205, 349, 601, 854,  27,\n           4, 449, 905, 392, 278, 904, 988, 366, 294, 245, 276, 292, 472, 216,\n         933, 144, 989, 971, 963, 670,  72, 643, 714, 955, 401,  76, 108, 937,\n         456, 512, 154, 304,  99, 289, 336, 775, 118, 133, 931, 836, 342, 962,\n         385, 983,  41, 608, 260, 961, 474, 246, 330, 752, 690, 310, 611, 717,\n         944, 459, 156, 345, 665, 602, 147,  75,  16, 208, 148, 539, 892, 207,\n         280, 286,  92, 996, 648, 654, 981, 124, 787, 715, 663, 241,  23, 543,\n         923, 909, 130, 277, 230, 288, 153, 948, 119, 879, 495, 943, 986, 211,\n         233, 578, 790, 198, 331,  83, 175,  15, 575, 676, 234, 875, 741, 496,\n         172, 627, 679, 353,  12, 421, 797, 658, 947, 167, 932, 531, 721, 969,\n         307, 824, 237,  28, 897, 102, 504, 778, 719,  61, 919, 756, 248, 911,\n         368,  17, 906, 584, 991, 729, 861, 430, 671,  14, 243, 887, 555, 874,\n         322, 376, 100, 293, 958,  37, 616, 922, 501, 901,  38, 774, 170, 229,\n         274, 339, 151,  67, 620, 326, 122, 965, 272,  51, 138, 193, 860, 936,\n         434, 386, 873, 418, 432, 794, 666, 127, 445, 623, 713, 750, 212, 375,\n         113, 549, 852, 638, 161, 614, 642,  20, 951, 270, 577, 635, 327, 364,\n         940, 766, 640, 231, 796, 475, 917,  47, 128, 332, 783,  69, 186, 204,\n         367, 748, 312,  31,  26, 215, 174,  71, 443, 180, 913, 866, 910, 468,\n          60, 159, 104, 210, 416, 607, 300, 194, 945, 736, 415,  97, 884,  73,\n         968, 391, 710, 771, 533, 192, 572,  53, 815, 651, 303, 735, 393,  64,\n         566, 387, 746,  34, 165, 731,  66,  18, 399, 355, 155,  33, 129,  10,\n         930, 956, 209, 110, 389, 934,  95, 499, 760, 363, 610, 598, 256, 335,\n         213, 502, 196, 271, 444, 705,  55, 548, 927, 374, 980, 618, 316, 219,\n         698, 979,  57, 361, 785,  21, 603, 337, 220, 921, 546, 876, 506, 158,\n         169, 647, 842, 242, 262, 162,  91, 838, 770, 454, 452,  80, 324, 938,\n         888, 136, 542, 672, 112, 617, 534, 768, 283, 864, 143, 953, 348, 633,\n         183, 526, 101, 886, 178, 605, 560, 221, 253,  90, 734, 896,  94, 889,\n         903, 946, 630, 749, 106, 568,  43, 918, 464,  54, 878, 287, 476, 510,\n         751,  77, 315, 789, 593, 486, 321, 379, 667, 282, 569, 513, 985,  79,\n         788, 636, 716, 826, 942, 370, 269, 821, 163, 240, 107, 320, 200, 509,\n         492, 952, 780, 586, 455, 114, 228, 103, 265, 831, 325, 377, 564, 791,\n         684,  58,  70, 388, 247, 420, 722, 457, 970, 126,  59,  24, 541, 252,\n         779, 239, 915,  44, 805, 111, 296, 422, 466, 490, 362, 311, 436, 309,\n         264, 957, 195, 358, 168,   9, 137, 588, 890, 333, 835, 257, 258, 284,\n         669, 249,  74, 201, 483, 950, 992, 580, 123,  46, 395, 357, 820, 916,\n         813,   3,  78, 769,  22, 806, 814, 802, 255, 285, 508, 382, 261, 697,\n         535, 203, 467,  52, 338, 810, 319,  81,  96, 352, 458, 767, 545, 739,\n         764, 446, 582, 596, 587, 726, 893, 800,  35, 441, 273, 675, 570, 176,\n         125, 480, 747, 435, 538, 581,   6,  13, 105, 354, 171, 859, 728, 407,\n         149,   1, 613, 622, 317, 978,  40, 424, 528, 699, 360, 411, 146, 135,\n         152, 365, 839, 359, 621, 696, 798, 732, 254, 743,  87, 730, 190, 579,\n         609, 591, 117, 777, 120, 977, 685, 489, 461, 615, 758, 318, 737, 140,\n         556, 425, 589, 998, 447, 442,  65, 691, 600, 428, 995, 855, 299, 164,\n         218, 487, 776, 759, 384, 394, 511, 142, 188, 631, 356,  89, 465, 491,\n         881, 202, 984, 380, 520, 590, 830, 827, 550, 505, 250, 232, 383, 372,\n         131, 891,   2, 378, 524, 403, 819, 423, 857, 850, 745, 574, 493, 954,\n         516, 547, 485, 478, 829, 902, 139, 626, 681, 840, 689, 592, 438,  19,\n         413, 537,  11, 298, 145, 251, 217, 822, 781, 816, 660, 597, 222, 530,\n         862,  63, 664, 967,   8, 604, 166, 532, 184, 206, 686, 481, 182, 440,\n         410, 628, 562, 551, 700, 599, 451, 848, 687, 804, 606, 753, 693, 267,\n         313,  56, 414, 677, 701, 448, 482, 402, 692, 527, 141, 727, 629, 662,\n         863, 929, 837, 803, 795, 784, 132, 656, 341, 702, 898, 683, 912, 695,\n         470, 914, 625, 426, 974, 559, 740, 497, 742, 525, 191, 846, 711, 786,\n         792, 409, 507, 757, 498, 181, 619, 718, 523, 704, 856, 427, 765, 484,\n         707, 553, 847, 185, 694, 920, 907, 650, 680, 595, 450, 477, 563, 473,\n         408, 189, 585, 514, 844, 994, 763, 634, 412, 708, 894, 536, 845, 851,\n         773, 761, 471, 279, 871, 688, 431, 644, 674, 682, 867, 782, 811, 799,\n         720, 558, 494, 877, 668, 833, 583, 637, 503, 557, 882, 632, 561, 895,\n         653, 519, 908, 404, 571, 405, 565, 818, 733, 872, 517, 807, 437, 755,\n         900, 540, 754, 744, 657, 812]])\n```\n:::\n:::\n\n\n::: {.callout-note}\nThe model's output are the log-probabilities of `sample_x` belonging to each of the 1000 classes.\n:::\n\n---\n\n- [ ] Use the list of categories to translate the predicted class index into its category.\n\n::: {#91a87dc7 .cell execution_count=11}\n``` {.python .cell-code}\nsorted_predicted_classes = sample_y.argsort(dim=1, descending=True)[0, :10]\nsorted_probs = torch.softmax(sample_y, dim=1)[0, sorted_predicted_classes]\n\nfor idx, prob in zip(sorted_predicted_classes, sorted_probs):\n    print(categories[idx], \"%3.2f %%\" % (prob * 100))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nspace shuttle 79.91 %\nmissile 12.59 %\nprojectile 6.04 %\nradio 0.11 %\ndrilling platform 0.08 %\nwater tower 0.05 %\nradio telescope 0.05 %\nbeacon 0.04 %\nsolar dish 0.02 %\ncrane 0.02 %\n```\n:::\n:::\n\n\n## Inspect the self-attention operations of the ViT\n\n::: {#5df8f13d .cell execution_count=12}\n``` {.python .cell-code}\ndl_model.encoder.layers[-1]\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\nEncoderBlock(\n  (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  (self_attention): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (dropout): Dropout(p=0.0, inplace=False)\n  (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  (mlp): MLPBlock(\n    (0): Linear(in_features=768, out_features=3072, bias=True)\n    (1): GELU(approximate='none')\n    (2): Dropout(p=0.0, inplace=False)\n    (3): Linear(in_features=3072, out_features=768, bias=True)\n    (4): Dropout(p=0.0, inplace=False)\n  )\n)\n```\n:::\n:::\n\n\n# Integrate a mechanism to review the attention maps of the model\n\n## Redefine some of transformer operations to enable storing the attention weights {.scrollable}\n\n::: {#5d7f8131 .cell execution_count=13}\n``` {.python .cell-code}\nfrom functools import partial\nfrom typing import Callable\nfrom timm.models.vision_transformer import Attention\n\nclass EncoderBlockAttnMap(models.vision_transformer.EncoderBlock):\n    \"\"\"Transformer encoder block.\"\"\"\n\n    def __init__(self,\n                 num_heads: int,\n                 hidden_dim: int,\n                 mlp_dim: int,\n                 dropout: float,\n                 attention_dropout: float,\n                 norm_layer: Callable[..., torch.nn.Module] = partial(torch.nn.LayerNorm, eps=1e-6)):\n        # The definition is the same, only the forward function changes <------------------------------------\n        super(EncoderBlockAttnMap, self).__init__(num_heads, hidden_dim, mlp_dim, dropout, attention_dropout, norm_layer)\n        self.self_attention = Attention(hidden_dim, num_heads, attn_drop=attention_dropout, proj_drop=0.0, norm_layer=norm_layer, qkv_bias=True)\n\n    def forward(self, input: torch.Tensor):\n        # with torch.autograd.graph.save_on_cpu(pin_memory=True):\n        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n        x = self.ln_1(input)\n\n        # Modify this line, so we get the attention map from the self attention modules <--------------------\n        x = self.self_attention(x)\n\n        x = self.dropout(x)\n        x = x + input\n\n        y = self.ln_2(x)\n        y = self.mlp(y)\n\n        # Return the attention map along with the encoder output <-------------------------------------------\n        return x + y\n```\n:::\n\n\n::: {#0cfb652e .cell execution_count=14}\n``` {.python .cell-code}\nfrom collections import OrderedDict\n\nclass EncoderAttnMap(models.vision_transformer.Encoder):\n    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n\n    def __init__(\n        self,\n        seq_length: int,\n        num_layers: int,\n        num_heads: int,\n        hidden_dim: int,\n        mlp_dim: int,\n        dropout: float,\n        attention_dropout: float,\n        norm_layer: Callable[..., torch.nn.Module] = partial(torch.nn.LayerNorm, eps=1e-6),\n    ):\n        super().__init__(seq_length, num_layers, num_heads, hidden_dim, mlp_dim, dropout, attention_dropout, norm_layer)\n\n        layers: OrderedDict[str, nn.Module] = OrderedDict()\n        for i in range(num_layers):\n            # Use the modified encoder block <---------------------------------------------------------------\n            layers[f\"encoder_layer_{i}\"] = EncoderBlockAttnMap(\n                num_heads,\n                hidden_dim,\n                mlp_dim,\n                dropout,\n                attention_dropout,\n                norm_layer,\n            )\n\n        self.layers = torch.nn.Sequential(layers)\n```\n:::\n\n\n::: {#e7638947 .cell execution_count=15}\n``` {.python .cell-code}\n# Redefine the classifier head to have access to the attention maps\nclass ViTAttnEnabled(models.vision_transformer.VisionTransformer):\n    \"\"\"Implementation of the classifier head from the ViT-B-16 architecture.\n    \"\"\"\n    def __init__(self, image_size, patch_size=14, num_layers=32, num_heads=16, hidden_dim=1280, mlp_dim=5120, **kwargs):      \n        super(ViTAttnEnabled, self).__init__(\n            image_size,\n            patch_size=patch_size,\n            num_layers=num_layers,\n            num_heads=num_heads,\n            hidden_dim=hidden_dim,\n            mlp_dim=mlp_dim,\n            **kwargs)\n\n        # Change the encoder to the modified ekwargsoder that returns the attention maps <-----------\n        self.encoder = EncoderAttnMap(\n            self.seq_length,\n            num_layers=num_layers,\n            num_heads=num_heads,\n            hidden_dim=hidden_dim,\n            mlp_dim=mlp_dim,\n            dropout=0,\n            attention_dropout=0,\n            norm_layer=partial(torch.nn.LayerNorm, eps=1e-6)\n        )\n\n        self.attentions = []\n        self.attentions_gradients = []\n\n    def get_attention(self, module, input, output):\n        self.attentions.append(output.detach())\n\n    def get_attention_gradients(self, module, grad_input, grad_output):\n        self.attentions_gradients.append(grad_input[0].detach())\n\n    def register_attn_grad_hooks(self):\n        for name, module in self.named_modules():\n            if \"self_attention.norm\" in name:\n                module.register_forward_hook(self.get_attention)\n                module.register_full_backward_hook(self.get_attention_gradients)\n\n    def clear_attentions(self):\n        self.attentions.clear()\n        self.attentions_gradients.clear()\n```\n:::\n\n\n---\n\n## Use a modified ViT model that enables tracking its attention weights\n\n- [] Initialize a ViT with the `Vit-B-16` architecture\n\n::: {#ca59c829 .cell execution_count=16}\n``` {.python .cell-code}\nvit_model_self_attn = ViTAttnEnabled(\n        image_size=384,\n        patch_size=16,\n        num_heads=12,\n        num_layers=12,\n        hidden_dim=768,\n        mlp_dim=3072)\n```\n:::\n\n\n---\n\n- [ ] Map the names of the modules in the orginal `torchvision` Attention layer, to the names of the `timm` Attention layer\n\n::: {#cad0d14b .cell execution_count=17}\n``` {.python .cell-code}\nname_map = {\n    \"in_proj_weight\": \"qkv.weight\",\n    \"in_proj_bias\": \"qkv.bias\",\n    \"out_proj.weight\": \"proj.weight\",\n    \"out_proj.bias\": \"proj.bias\"\n}\n\ntransformer_weights_dict = transformer_weights.get_state_dict(progress=True)\n\nvit_weights = {}\nfor k, v in transformer_weights_dict.items():\n    k_old = list(filter(lambda n: n in k, name_map.keys()))\n    if len(k_old):\n        k_old = k_old[0]\n        old_name = k.split(k_old)[0]\n        new_name = old_name + name_map[k_old]\n    else:\n        new_name = k\n\n    vit_weights[new_name] = v\n\nvit_model_self_attn.load_state_dict(vit_weights)\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n<All keys matched successfully>\n```\n:::\n:::\n\n\n---\n\n- [ ] Enable the tracking of the attention weights\n\n::: {#9e6e1ecf .cell execution_count=18}\n``` {.python .cell-code}\nvit_model_self_attn.register_attn_grad_hooks()\n```\n:::\n\n\n---\n\n## Apply the ViT class prediction on an image and compute the corresponding attention map \n\n- [ ] Run the `forward` operation of the ViT model and the respective `backward` opreation to compute and store the attention weights\n\n::: {#bce9041b .cell execution_count=19}\n``` {.python .cell-code}\nsample_im = skimage.io.imread(\"https://r0k.us/graphics/kodak/kodak/kodim20.png\")\n\nvit_model_self_attn.clear_attentions()\n\nsample_x = pipeline(sample_im)\n\nwith torch.autograd.graph.save_on_cpu(pin_memory=True):\n    sample_y = vit_model_self_attn(sample_x[None, ...])\n\nattn_class = torch.argmax(sample_y, dim=1).item()\nattn_class = torch.LongTensor([attn_class])\nattn_class = torch.nn.functional.one_hot(attn_class, num_classes=sample_y.shape[1])\n\nattn_class = torch.sum(attn_class * sample_y)\n\nvit_model_self_attn.zero_grad()\nattn_class.backward()\n\nattn_out = [attn_tensor.clone() for attn_tensor in vit_model_self_attn.attentions]\ngrad_attn_out = [attn_tensor.clone() for attn_tensor in vit_model_self_attn.attentions_gradients]\n```\n:::\n\n\n---\n\n## Roll out the attention maps\n\n- [ ] The attention map can be computed as the accumulation of the attention weigths of each *Encoder* layer of the Transformer\n\n::: {#223aaf61 .cell execution_count=20}\n``` {.python .cell-code}\nattn_rollout = torch.eye(attn_out[0].size(1))[None, ...]\n\nfor attn_map, attn_grad in zip(attn_out, grad_attn_out):\n    if attn_grad is not None:\n        attn_map = attn_map * attn_grad\n        attn_map[attn_map < 0] = 0\n\n    attn_map, _ = torch.topk(attn_map, 10, dim=-1)\n    attn_map = attn_map.mean(dim=-1)\n\n    # Normalize attention map\n    attn_map = attn_map + torch.eye(attn_map.size(1), device=attn_map.device)[None, ...]\n    attn_map = attn_map / attn_map.sum(dim=-1, keepdim=True)\n\n    attn_rollout = torch.matmul(attn_map, attn_rollout)\n\nattn_rollout.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\ntorch.Size([1, 577, 577])\n```\n:::\n:::\n\n\n---\n\n- [ ] Keep only the attention weights of the class token with respect to the spatial tokens\n\n::: {#68b264e8 .cell execution_count=21}\n``` {.python .cell-code}\nattn_rollout = attn_rollout[:, :1, 1:]\n```\n:::\n\n\n- [ ] Reshape the attention weights into the original patch coordinates\n\n::: {#24a946f9 .cell execution_count=22}\n``` {.python .cell-code}\nattn_rollout = attn_rollout.reshape(1, -1, 24 ** 2)\nattn_rollout.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\ntorch.Size([1, 1, 576])\n```\n:::\n:::\n\n\n::: {#e762e2e4 .cell execution_count=23}\n``` {.python .cell-code}\nattn_rollout = torch.nn.functional.fold(attn_rollout.transpose(1, 2),\n                      (24, 24),\n                      kernel_size=(24, 24),\n                      stride=(24, 24))\n```\n:::\n\n\n::: {#00a4c983 .cell execution_count=24}\n``` {.python .cell-code}\nattn_rollout = attn_rollout.squeeze()\n\nattn_rollout = attn_rollout / torch.max(attn_rollout)\n\nattn_rollout.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\ntorch.Size([24, 24])\n```\n:::\n:::\n\n\n---\n\n## Visualize the attention map computed from the attention weights\n\n- [] Show an overlay of the attention map over the original image\n\n::: {#2da4a589 .cell execution_count=25}\n``` {.python .cell-code}\nplt.imshow(sample_im)\nplt.imshow(attn_rollout, cmap=\"magma\", extent=(0, sample_im.shape[1], sample_im.shape[0], 0), alpha=0.75)\n```\n\n::: {.cell-output .cell-output-display}\n![](DL_image_analysis_2_2_files/figure-revealjs/cell-26-output-1.png){width=608 height=416}\n:::\n:::\n\n\n",
    "supporting": [
      "DL_image_analysis_2_2_files"
    ],
    "filters": [],
    "includes": {}
  }
}