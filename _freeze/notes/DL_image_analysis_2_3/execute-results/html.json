{
  "hash": "d0b1d149cd45f5f862996c7f22a8f9c4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Advanced Machine Learning with Python (Session 2 - Part 3)\nauthor: Fernando Cervantes (fernando.cervantes@jax.org)\nformat:\n  revealjs:\n    code-fold: false\n    progress: true\n    controls: true\n    output-file: \"Adv_ML_Python_presentation_2_3\"\n    fontsize: 20pt\n    include-in-header:\n      text: |\n        <link href=\"https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.3/css/bootstrap.min.css\" rel =\"stylesheet\" integrity=\"sha512-jnSuA4Ss2PkkikSOLtYs8BlYIeeIK1h99ty4YfvRPAlzr377vr3CXDb7sb7eEEBYjDtcYj+AjBH3FLv5uSJuXg==\" crossorigin=\"anonymous\">\n\nexecute:\n  error: true\n  echo: true\n  cache: true\n  freeze: true\n  keep-ipynb: true\n\njupyter: python3\n---\n\n\n## Materials\n\n[Open notebook in Colab](https://colab.research.google.com/gist/fercer/cdee0a8283df59b9e0f8bc04747b23f0/advanced_machine_learning_with_python_session_2_3.ipynb){.btn .btn-outline-primary .btn role=\"button\" target=”_blank”}\n\n# Setup\n\n```\n!pip install s3fs imagecodecs umap-learn torchmetrics\n```\n\n::: {#9bcb6b7e .cell execution_count=1}\n``` {.python .cell-code}\n!git clone https://github.com/jump-cellpainting/JUMP-Target\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCloning into 'JUMP-Target'...\nremote: Enumerating objects: 243, done.\nremote: Counting objects:   1% (1/78)\rremote: Counting objects:   2% (2/78)\rremote: Counting objects:   3% (3/78)\rremote: Counting objects:   5% (4/78)\rremote: Counting objects:   6% (5/78)\rremote: Counting objects:   7% (6/78)\rremote: Counting objects:   8% (7/78)\rremote: Counting objects:  10% (8/78)\rremote: Counting objects:  11% (9/78)\rremote: Counting objects:  12% (10/78)\rremote: Counting objects:  14% (11/78)\rremote: Counting objects:  15% (12/78)\rremote: Counting objects:  16% (13/78)\rremote: Counting objects:  17% (14/78)\rremote: Counting objects:  19% (15/78)\rremote: Counting objects:  20% (16/78)\rremote: Counting objects:  21% (17/78)\rremote: Counting objects:  23% (18/78)\rremote: Counting objects:  24% (19/78)\rremote: Counting objects:  25% (20/78)\rremote: Counting objects:  26% (21/78)\rremote: Counting objects:  28% (22/78)\rremote: Counting objects:  29% (23/78)\rremote: Counting objects:  30% (24/78)\rremote: Counting objects:  32% (25/78)\rremote: Counting objects:  33% (26/78)\rremote: Counting objects:  34% (27/78)\rremote: Counting objects:  35% (28/78)\rremote: Counting objects:  37% (29/78)\rremote: Counting objects:  38% (30/78)\rremote: Counting objects:  39% (31/78)\rremote: Counting objects:  41% (32/78)\rremote: Counting objects:  42% (33/78)\rremote: Counting objects:  43% (34/78)\rremote: Counting objects:  44% (35/78)\rremote: Counting objects:  46% (36/78)\rremote: Counting objects:  47% (37/78)\rremote: Counting objects:  48% (38/78)\rremote: Counting objects:  50% (39/78)\rremote: Counting objects:  51% (40/78)\rremote: Counting objects:  52% (41/78)\rremote: Counting objects:  53% (42/78)\rremote: Counting objects:  55% (43/78)\rremote: Counting objects:  56% (44/78)\rremote: Counting objects:  57% (45/78)\rremote: Counting objects:  58% (46/78)\rremote: Counting objects:  60% (47/78)\rremote: Counting objects:  61% (48/78)\rremote: Counting objects:  62% (49/78)\rremote: Counting objects:  64% (50/78)\rremote: Counting objects:  65% (51/78)\rremote: Counting objects:  66% (52/78)\rremote: Counting objects:  67% (53/78)\rremote: Counting objects:  69% (54/78)\rremote: Counting objects:  70% (55/78)\rremote: Counting objects:  71% (56/78)\rremote: Counting objects:  73% (57/78)\rremote: Counting objects:  74% (58/78)\rremote: Counting objects:  75% (59/78)\rremote: Counting objects:  76% (60/78)\rremote: Counting objects:  78% (61/78)\rremote: Counting objects:  79% (62/78)\rremote: Counting objects:  80% (63/78)\rremote: Counting objects:  82% (64/78)\rremote: Counting objects:  83% (65/78)\rremote: Counting objects:  84% (66/78)\rremote: Counting objects:  85% (67/78)\rremote: Counting objects:  87% (68/78)\rremote: Counting objects:  88% (69/78)\rremote: Counting objects:  89% (70/78)\rremote: Counting objects:  91% (71/78)\rremote: Counting objects:  92% (72/78)\rremote: Counting objects:  93% (73/78)\rremote: Counting objects:  94% (74/78)\rremote: Counting objects:  96% (75/78)\rremote: Counting objects:  97% (76/78)\rremote: Counting objects:  98% (77/78)\rremote: Counting objects: 100% (78/78)\rremote: Counting objects: 100% (78/78), done.\nremote: Compressing objects:   1% (1/69)\rremote: Compressing objects:   2% (2/69)\rremote: Compressing objects:   4% (3/69)\rremote: Compressing objects:   5% (4/69)\rremote: Compressing objects:   7% (5/69)\rremote: Compressing objects:   8% (6/69)\rremote: Compressing objects:  10% (7/69)\rremote: Compressing objects:  11% (8/69)\rremote: Compressing objects:  13% (9/69)\rremote: Compressing objects:  14% (10/69)\rremote: Compressing objects:  15% (11/69)\rremote: Compressing objects:  17% (12/69)\rremote: Compressing objects:  18% (13/69)\rremote: Compressing objects:  20% (14/69)\rremote: Compressing objects:  21% (15/69)\rremote: Compressing objects:  23% (16/69)\rremote: Compressing objects:  24% (17/69)\rremote: Compressing objects:  26% (18/69)\rremote: Compressing objects:  27% (19/69)\rremote: Compressing objects:  28% (20/69)\rremote: Compressing objects:  30% (21/69)\rremote: Compressing objects:  31% (22/69)\rremote: Compressing objects:  33% (23/69)\rremote: Compressing objects:  34% (24/69)\rremote: Compressing objects:  36% (25/69)\rremote: Compressing objects:  37% (26/69)\rremote: Compressing objects:  39% (27/69)\rremote: Compressing objects:  40% (28/69)\rremote: Compressing objects:  42% (29/69)\rremote: Compressing objects:  43% (30/69)\rremote: Compressing objects:  44% (31/69)\rremote: Compressing objects:  46% (32/69)\rremote: Compressing objects:  47% (33/69)\rremote: Compressing objects:  49% (34/69)\rremote: Compressing objects:  50% (35/69)\rremote: Compressing objects:  52% (36/69)\rremote: Compressing objects:  53% (37/69)\rremote: Compressing objects:  55% (38/69)\rremote: Compressing objects:  56% (39/69)\rremote: Compressing objects:  57% (40/69)\rremote: Compressing objects:  59% (41/69)\rremote: Compressing objects:  60% (42/69)\rremote: Compressing objects:  62% (43/69)\rremote: Compressing objects:  63% (44/69)\rremote: Compressing objects:  65% (45/69)\rremote: Compressing objects:  66% (46/69)\rremote: Compressing objects:  68% (47/69)\rremote: Compressing objects:  69% (48/69)\rremote: Compressing objects:  71% (49/69)\rremote: Compressing objects:  72% (50/69)\rremote: Compressing objects:  73% (51/69)\rremote: Compressing objects:  75% (52/69)\rremote: Compressing objects:  76% (53/69)\rremote: Compressing objects:  78% (54/69)\rremote: Compressing objects:  79% (55/69)\rremote: Compressing objects:  81% (56/69)\rremote: Compressing objects:  82% (57/69)\rremote: Compressing objects:  84% (58/69)\rremote: Compressing objects:  85% (59/69)\rremote: Compressing objects:  86% (60/69)\rremote: Compressing objects:  88% (61/69)\rremote: Compressing objects:  89% (62/69)\rremote: Compressing objects:  91% (63/69)\rremote: Compressing objects:  92% (64/69)\rremote: Compressing objects:  94% (65/69)\rremote: Compressing objects:  95% (66/69)\rremote: Compressing objects:  97% (67/69)\rremote: Compressing objects:  98% (68/69)\rremote: Compressing objects: 100% (69/69)\rremote: Compressing objects: 100% (69/69), done.\nReceiving objects:   0% (1/243)\rReceiving objects:   1% (3/243)\rReceiving objects:   2% (5/243)\rReceiving objects:   3% (8/243)\rReceiving objects:   4% (10/243)\rReceiving objects:   5% (13/243)\rReceiving objects:   6% (15/243)\rReceiving objects:   7% (18/243)\rReceiving objects:   8% (20/243)\rReceiving objects:   9% (22/243)\rReceiving objects:  10% (25/243)\rReceiving objects:  11% (27/243)\rReceiving objects:  12% (30/243)\rReceiving objects:  13% (32/243)\rReceiving objects:  14% (35/243)\rReceiving objects:  15% (37/243)\rReceiving objects:  16% (39/243)\rReceiving objects:  17% (42/243)\rReceiving objects:  18% (44/243)\rReceiving objects:  19% (47/243)\rReceiving objects:  20% (49/243)\rReceiving objects:  21% (52/243)\rReceiving objects:  22% (54/243)\rReceiving objects:  23% (56/243)\rReceiving objects:  24% (59/243)\rReceiving objects:  25% (61/243)\rReceiving objects:  26% (64/243)\rReceiving objects:  27% (66/243)\rReceiving objects:  28% (69/243)\rReceiving objects:  29% (71/243)\rReceiving objects:  30% (73/243)\rReceiving objects:  31% (76/243)\rReceiving objects:  32% (78/243)\rReceiving objects:  33% (81/243)\rReceiving objects:  34% (83/243)\rReceiving objects:  35% (86/243)\rReceiving objects:  36% (88/243)\rReceiving objects:  37% (90/243)\rReceiving objects:  38% (93/243)\rReceiving objects:  39% (95/243)\rReceiving objects:  40% (98/243)\rReceiving objects:  41% (100/243)\rReceiving objects:  42% (103/243)\rReceiving objects:  43% (105/243)\rReceiving objects:  44% (107/243)\rReceiving objects:  45% (110/243)\rReceiving objects:  46% (112/243)\rReceiving objects:  47% (115/243)\rReceiving objects:  48% (117/243)\rReceiving objects:  49% (120/243)\rReceiving objects:  50% (122/243)\rReceiving objects:  51% (124/243)\rReceiving objects:  52% (127/243)\rReceiving objects:  53% (129/243)\rReceiving objects:  54% (132/243)\rReceiving objects:  55% (134/243)\rReceiving objects:  56% (137/243)\rReceiving objects:  57% (139/243)\rReceiving objects:  58% (141/243)\rReceiving objects:  59% (144/243)\rReceiving objects:  60% (146/243)\rReceiving objects:  61% (149/243)\rReceiving objects:  62% (151/243)\rReceiving objects:  63% (154/243)\rReceiving objects:  64% (156/243)\rReceiving objects:  65% (158/243)\rReceiving objects:  66% (161/243)\rReceiving objects:  67% (163/243)\rReceiving objects:  68% (166/243)\rReceiving objects:  69% (168/243)\rReceiving objects:  70% (171/243)\rReceiving objects:  71% (173/243)\rReceiving objects:  72% (175/243)\rReceiving objects:  73% (178/243)\rReceiving objects:  74% (180/243)\rReceiving objects:  75% (183/243)\rReceiving objects:  76% (185/243)\rReceiving objects:  77% (188/243)\rReceiving objects:  78% (190/243)\rReceiving objects:  79% (192/243)\rReceiving objects:  80% (195/243)\rReceiving objects:  81% (197/243)\rReceiving objects:  82% (200/243)\rReceiving objects:  83% (202/243)\rReceiving objects:  84% (205/243)\rReceiving objects:  85% (207/243)\rReceiving objects:  86% (209/243)\rReceiving objects:  87% (212/243)\rReceiving objects:  88% (214/243)\rReceiving objects:  89% (217/243)\rReceiving objects:  90% (219/243)\rReceiving objects:  91% (222/243)\rReceiving objects:  92% (224/243)\rReceiving objects:  93% (226/243)\rReceiving objects:  94% (229/243)\rReceiving objects:  95% (231/243)\rremote: Total 243 (delta 40), reused 18 (delta 9), pack-reused 165 (from 1)\nReceiving objects:  96% (234/243)\rReceiving objects:  97% (236/243)\rReceiving objects:  98% (239/243)\rReceiving objects:  99% (241/243)\rReceiving objects: 100% (243/243)\rReceiving objects: 100% (243/243), 289.62 KiB | 8.27 MiB/s, done.\nResolving deltas:   0% (0/133)\rResolving deltas:   1% (2/133)\rResolving deltas:   2% (3/133)\rResolving deltas:   3% (4/133)\rResolving deltas:   4% (6/133)\rResolving deltas:   5% (7/133)\rResolving deltas:   6% (8/133)\rResolving deltas:   7% (10/133)\rResolving deltas:   8% (11/133)\rResolving deltas:   9% (12/133)\rResolving deltas:  10% (14/133)\rResolving deltas:  11% (15/133)\rResolving deltas:  12% (16/133)\rResolving deltas:  13% (18/133)\rResolving deltas:  14% (19/133)\rResolving deltas:  15% (20/133)\rResolving deltas:  16% (22/133)\rResolving deltas:  17% (23/133)\rResolving deltas:  18% (24/133)\rResolving deltas:  19% (26/133)\rResolving deltas:  20% (27/133)\rResolving deltas:  21% (28/133)\rResolving deltas:  22% (30/133)\rResolving deltas:  23% (31/133)\rResolving deltas:  24% (32/133)\rResolving deltas:  25% (34/133)\rResolving deltas:  26% (35/133)\rResolving deltas:  27% (36/133)\rResolving deltas:  28% (38/133)\rResolving deltas:  29% (39/133)\rResolving deltas:  30% (40/133)\rResolving deltas:  31% (42/133)\rResolving deltas:  32% (43/133)\rResolving deltas:  33% (44/133)\rResolving deltas:  34% (46/133)\rResolving deltas:  35% (47/133)\rResolving deltas:  36% (48/133)\rResolving deltas:  37% (50/133)\rResolving deltas:  38% (51/133)\rResolving deltas:  39% (52/133)\rResolving deltas:  40% (54/133)\rResolving deltas:  41% (55/133)\rResolving deltas:  42% (56/133)\rResolving deltas:  43% (58/133)\rResolving deltas:  44% (59/133)\rResolving deltas:  45% (60/133)\rResolving deltas:  46% (62/133)\rResolving deltas:  47% (63/133)\rResolving deltas:  48% (64/133)\rResolving deltas:  49% (66/133)\rResolving deltas:  50% (67/133)\rResolving deltas:  51% (68/133)\rResolving deltas:  52% (70/133)\rResolving deltas:  53% (71/133)\rResolving deltas:  54% (72/133)\rResolving deltas:  55% (74/133)\rResolving deltas:  56% (75/133)\rResolving deltas:  57% (76/133)\rResolving deltas:  58% (78/133)\rResolving deltas:  59% (79/133)\rResolving deltas:  60% (80/133)\rResolving deltas:  61% (82/133)\rResolving deltas:  62% (83/133)\rResolving deltas:  63% (84/133)\rResolving deltas:  64% (86/133)\rResolving deltas:  65% (87/133)\rResolving deltas:  66% (88/133)\rResolving deltas:  67% (90/133)\rResolving deltas:  68% (91/133)\rResolving deltas:  69% (92/133)\rResolving deltas:  70% (94/133)\rResolving deltas:  71% (95/133)\rResolving deltas:  72% (96/133)\rResolving deltas:  73% (98/133)\rResolving deltas:  75% (100/133)\rResolving deltas:  76% (102/133)\rResolving deltas:  77% (103/133)\rResolving deltas:  78% (104/133)\rResolving deltas:  79% (106/133)\rResolving deltas:  80% (107/133)\rResolving deltas:  81% (108/133)\rResolving deltas:  82% (110/133)\rResolving deltas:  83% (111/133)\rResolving deltas:  84% (112/133)\rResolving deltas:  85% (114/133)\rResolving deltas:  86% (115/133)\rResolving deltas:  87% (116/133)\rResolving deltas:  88% (118/133)\rResolving deltas:  89% (119/133)\rResolving deltas:  90% (120/133)\rResolving deltas:  91% (122/133)\rResolving deltas:  92% (123/133)\rResolving deltas:  93% (124/133)\rResolving deltas:  94% (126/133)\rResolving deltas:  95% (127/133)\rResolving deltas:  96% (128/133)\rResolving deltas:  97% (130/133)\rResolving deltas:  98% (131/133)\rResolving deltas:  99% (132/133)\rResolving deltas: 100% (133/133)\rResolving deltas: 100% (133/133), done.\n```\n:::\n:::\n\n\n::: {#0a3cddc7 .cell execution_count=2}\n``` {.python .cell-code}\n!git clone https://github.com/jump-cellpainting/datasets.git\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCloning into 'datasets'...\nremote: Enumerating objects: 880, done.\nremote: Counting objects:   0% (1/451)\rremote: Counting objects:   1% (5/451)\rremote: Counting objects:   2% (10/451)\rremote: Counting objects:   3% (14/451)\rremote: Counting objects:   4% (19/451)\rremote: Counting objects:   5% (23/451)\rremote: Counting objects:   6% (28/451)\rremote: Counting objects:   7% (32/451)\rremote: Counting objects:   8% (37/451)\rremote: Counting objects:   9% (41/451)\rremote: Counting objects:  10% (46/451)\rremote: Counting objects:  11% (50/451)\rremote: Counting objects:  12% (55/451)\rremote: Counting objects:  13% (59/451)\rremote: Counting objects:  14% (64/451)\rremote: Counting objects:  15% (68/451)\rremote: Counting objects:  16% (73/451)\rremote: Counting objects:  17% (77/451)\rremote: Counting objects:  18% (82/451)\rremote: Counting objects:  19% (86/451)\rremote: Counting objects:  20% (91/451)\rremote: Counting objects:  21% (95/451)\rremote: Counting objects:  22% (100/451)\rremote: Counting objects:  23% (104/451)\rremote: Counting objects:  24% (109/451)\rremote: Counting objects:  25% (113/451)\rremote: Counting objects:  26% (118/451)\rremote: Counting objects:  27% (122/451)\rremote: Counting objects:  28% (127/451)\rremote: Counting objects:  29% (131/451)\rremote: Counting objects:  30% (136/451)\rremote: Counting objects:  31% (140/451)\rremote: Counting objects:  32% (145/451)\rremote: Counting objects:  33% (149/451)\rremote: Counting objects:  34% (154/451)\rremote: Counting objects:  35% (158/451)\rremote: Counting objects:  36% (163/451)\rremote: Counting objects:  37% (167/451)\rremote: Counting objects:  38% (172/451)\rremote: Counting objects:  39% (176/451)\rremote: Counting objects:  40% (181/451)\rremote: Counting objects:  41% (185/451)\rremote: Counting objects:  42% (190/451)\rremote: Counting objects:  43% (194/451)\rremote: Counting objects:  44% (199/451)\rremote: Counting objects:  45% (203/451)\rremote: Counting objects:  46% (208/451)\rremote: Counting objects:  47% (212/451)\rremote: Counting objects:  48% (217/451)\rremote: Counting objects:  49% (221/451)\rremote: Counting objects:  50% (226/451)\rremote: Counting objects:  51% (231/451)\rremote: Counting objects:  52% (235/451)\rremote: Counting objects:  53% (240/451)\rremote: Counting objects:  54% (244/451)\rremote: Counting objects:  55% (249/451)\rremote: Counting objects:  56% (253/451)\rremote: Counting objects:  57% (258/451)\rremote: Counting objects:  58% (262/451)\rremote: Counting objects:  59% (267/451)\rremote: Counting objects:  60% (271/451)\rremote: Counting objects:  61% (276/451)\rremote: Counting objects:  62% (280/451)\rremote: Counting objects:  63% (285/451)\rremote: Counting objects:  64% (289/451)\rremote: Counting objects:  65% (294/451)\rremote: Counting objects:  66% (298/451)\rremote: Counting objects:  67% (303/451)\rremote: Counting objects:  68% (307/451)\rremote: Counting objects:  69% (312/451)\rremote: Counting objects:  70% (316/451)\rremote: Counting objects:  71% (321/451)\rremote: Counting objects:  72% (325/451)\rremote: Counting objects:  73% (330/451)\rremote: Counting objects:  74% (334/451)\rremote: Counting objects:  75% (339/451)\rremote: Counting objects:  76% (343/451)\rremote: Counting objects:  77% (348/451)\rremote: Counting objects:  78% (352/451)\rremote: Counting objects:  79% (357/451)\rremote: Counting objects:  80% (361/451)\rremote: Counting objects:  81% (366/451)\rremote: Counting objects:  82% (370/451)\rremote: Counting objects:  83% (375/451)\rremote: Counting objects:  84% (379/451)\rremote: Counting objects:  85% (384/451)\rremote: Counting objects:  86% (388/451)\rremote: Counting objects:  87% (393/451)\rremote: Counting objects:  88% (397/451)\rremote: Counting objects:  89% (402/451)\rremote: Counting objects:  90% (406/451)\rremote: Counting objects:  91% (411/451)\rremote: Counting objects:  92% (415/451)\rremote: Counting objects:  93% (420/451)\rremote: Counting objects:  94% (424/451)\rremote: Counting objects:  95% (429/451)\rremote: Counting objects:  96% (433/451)\rremote: Counting objects:  97% (438/451)\rremote: Counting objects:  98% (442/451)\rremote: Counting objects:  99% (447/451)\rremote: Counting objects: 100% (451/451)\rremote: Counting objects: 100% (451/451), done.\nremote: Compressing objects:   0% (1/166)\rremote: Compressing objects:   1% (2/166)\rremote: Compressing objects:   2% (4/166)\rremote: Compressing objects:   3% (5/166)\rremote: Compressing objects:   4% (7/166)\rremote: Compressing objects:   5% (9/166)\rremote: Compressing objects:   6% (10/166)\rremote: Compressing objects:   7% (12/166)\rremote: Compressing objects:   8% (14/166)\rremote: Compressing objects:   9% (15/166)\rremote: Compressing objects:  10% (17/166)\rremote: Compressing objects:  11% (19/166)\rremote: Compressing objects:  12% (20/166)\rremote: Compressing objects:  13% (22/166)\rremote: Compressing objects:  14% (24/166)\rremote: Compressing objects:  15% (25/166)\rremote: Compressing objects:  16% (27/166)\rremote: Compressing objects:  17% (29/166)\rremote: Compressing objects:  18% (30/166)\rremote: Compressing objects:  19% (32/166)\rremote: Compressing objects:  20% (34/166)\rremote: Compressing objects:  21% (35/166)\rremote: Compressing objects:  22% (37/166)\rremote: Compressing objects:  23% (39/166)\rremote: Compressing objects:  24% (40/166)\rremote: Compressing objects:  25% (42/166)\rremote: Compressing objects:  26% (44/166)\rremote: Compressing objects:  27% (45/166)\rremote: Compressing objects:  28% (47/166)\rremote: Compressing objects:  29% (49/166)\rremote: Compressing objects:  30% (50/166)\rremote: Compressing objects:  31% (52/166)\rremote: Compressing objects:  32% (54/166)\rremote: Compressing objects:  33% (55/166)\rremote: Compressing objects:  34% (57/166)\rremote: Compressing objects:  35% (59/166)\rremote: Compressing objects:  36% (60/166)\rremote: Compressing objects:  37% (62/166)\rremote: Compressing objects:  38% (64/166)\rremote: Compressing objects:  39% (65/166)\rremote: Compressing objects:  40% (67/166)\rremote: Compressing objects:  41% (69/166)\rremote: Compressing objects:  42% (70/166)\rremote: Compressing objects:  43% (72/166)\rremote: Compressing objects:  44% (74/166)\rremote: Compressing objects:  45% (75/166)\rremote: Compressing objects:  46% (77/166)\rremote: Compressing objects:  47% (79/166)\rremote: Compressing objects:  48% (80/166)\rremote: Compressing objects:  49% (82/166)\rremote: Compressing objects:  50% (83/166)\rremote: Compressing objects:  51% (85/166)\rremote: Compressing objects:  52% (87/166)\rremote: Compressing objects:  53% (88/166)\rremote: Compressing objects:  54% (90/166)\rremote: Compressing objects:  55% (92/166)\rremote: Compressing objects:  56% (93/166)\rremote: Compressing objects:  57% (95/166)\rremote: Compressing objects:  58% (97/166)\rremote: Compressing objects:  59% (98/166)\rremote: Compressing objects:  60% (100/166)\rremote: Compressing objects:  61% (102/166)\rremote: Compressing objects:  62% (103/166)\rremote: Compressing objects:  63% (105/166)\rremote: Compressing objects:  64% (107/166)\rremote: Compressing objects:  65% (108/166)\rremote: Compressing objects:  66% (110/166)\rremote: Compressing objects:  67% (112/166)\rremote: Compressing objects:  68% (113/166)\rremote: Compressing objects:  69% (115/166)\rremote: Compressing objects:  70% (117/166)\rremote: Compressing objects:  71% (118/166)\rremote: Compressing objects:  72% (120/166)\rremote: Compressing objects:  73% (122/166)\rremote: Compressing objects:  74% (123/166)\rremote: Compressing objects:  75% (125/166)\rremote: Compressing objects:  76% (127/166)\rremote: Compressing objects:  77% (128/166)\rremote: Compressing objects:  78% (130/166)\rremote: Compressing objects:  79% (132/166)\rremote: Compressing objects:  80% (133/166)\rremote: Compressing objects:  81% (135/166)\rremote: Compressing objects:  82% (137/166)\rremote: Compressing objects:  83% (138/166)\rremote: Compressing objects:  84% (140/166)\rremote: Compressing objects:  85% (142/166)\rremote: Compressing objects:  86% (143/166)\rremote: Compressing objects:  87% (145/166)\rremote: Compressing objects:  88% (147/166)\rremote: Compressing objects:  89% (148/166)\rremote: Compressing objects:  90% (150/166)\rremote: Compressing objects:  91% (152/166)\rremote: Compressing objects:  92% (153/166)\rremote: Compressing objects:  93% (155/166)\rremote: Compressing objects:  94% (157/166)\rremote: Compressing objects:  95% (158/166)\rremote: Compressing objects:  96% (160/166)\rremote: Compressing objects:  97% (162/166)\rremote: Compressing objects:  98% (163/166)\rremote: Compressing objects:  99% (165/166)\rremote: Compressing objects: 100% (166/166)\rremote: Compressing objects: 100% (166/166), done.\nReceiving objects:   0% (1/880)\rReceiving objects:   1% (9/880)\rReceiving objects:   2% (18/880)\rReceiving objects:   3% (27/880)\rReceiving objects:   4% (36/880)\rReceiving objects:   5% (44/880)\rReceiving objects:   6% (53/880)\rReceiving objects:   7% (62/880)\rReceiving objects:   8% (71/880)\rReceiving objects:   9% (80/880)\rReceiving objects:  10% (88/880)\rReceiving objects:  11% (97/880)\rReceiving objects:  12% (106/880)\rReceiving objects:  13% (115/880)\rReceiving objects:  14% (124/880)\rReceiving objects:  15% (132/880)\rReceiving objects:  16% (141/880)\rReceiving objects:  17% (150/880)\rReceiving objects:  18% (159/880)\rReceiving objects:  19% (168/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  20% (176/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  21% (185/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  22% (194/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  23% (203/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  24% (212/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  25% (220/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  26% (229/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  27% (238/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  28% (247/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  29% (256/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  30% (264/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  31% (273/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  32% (282/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  33% (291/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  34% (300/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  35% (308/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  36% (317/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  37% (326/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  38% (335/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  39% (344/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  40% (352/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  41% (361/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  42% (370/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  43% (379/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  44% (388/880), 35.34 MiB | 70.80 MiB/s\rReceiving objects:  44% (388/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  45% (396/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  46% (405/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  47% (414/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  48% (423/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  49% (432/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  50% (440/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  51% (449/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  52% (458/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  53% (467/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  54% (476/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  55% (484/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  56% (493/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  57% (502/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  58% (511/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  59% (520/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  60% (528/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  61% (537/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  62% (546/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  63% (555/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  64% (564/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  65% (572/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  66% (581/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  67% (590/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  68% (599/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  69% (608/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  70% (616/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  71% (625/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  72% (634/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  73% (643/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  74% (652/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  75% (660/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  76% (669/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  77% (678/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  78% (687/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  79% (696/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  80% (704/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  81% (713/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  82% (722/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  83% (731/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  84% (740/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  85% (748/880), 75.67 MiB | 75.73 MiB/s\rremote: Total 880 (delta 342), reused 310 (delta 279), pack-reused 429 (from 1)\nReceiving objects:  86% (757/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  87% (766/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  88% (775/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  89% (784/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  90% (792/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  91% (801/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  92% (810/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  93% (819/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  94% (828/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  95% (836/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  96% (845/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  97% (854/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  98% (863/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects:  99% (872/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects: 100% (880/880), 75.67 MiB | 75.73 MiB/s\rReceiving objects: 100% (880/880), 91.91 MiB | 79.02 MiB/s, done.\nResolving deltas:   0% (0/457)\rResolving deltas:   1% (5/457)\rResolving deltas:   2% (10/457)\rResolving deltas:   3% (14/457)\rResolving deltas:   4% (19/457)\rResolving deltas:   5% (23/457)\rResolving deltas:   6% (28/457)\rResolving deltas:   7% (32/457)\rResolving deltas:   8% (37/457)\rResolving deltas:   9% (42/457)\rResolving deltas:  10% (46/457)\rResolving deltas:  11% (51/457)\rResolving deltas:  12% (55/457)\rResolving deltas:  13% (60/457)\rResolving deltas:  14% (64/457)\rResolving deltas:  15% (69/457)\rResolving deltas:  16% (74/457)\rResolving deltas:  17% (78/457)\rResolving deltas:  18% (83/457)\rResolving deltas:  19% (87/457)\rResolving deltas:  20% (92/457)\rResolving deltas:  21% (96/457)\rResolving deltas:  22% (101/457)\rResolving deltas:  23% (106/457)\rResolving deltas:  24% (110/457)\rResolving deltas:  25% (115/457)\rResolving deltas:  26% (119/457)\rResolving deltas:  27% (124/457)\rResolving deltas:  28% (129/457)\rResolving deltas:  29% (133/457)\rResolving deltas:  30% (138/457)\rResolving deltas:  31% (142/457)\rResolving deltas:  32% (147/457)\rResolving deltas:  33% (151/457)\rResolving deltas:  34% (156/457)\rResolving deltas:  35% (161/457)\rResolving deltas:  36% (165/457)\rResolving deltas:  37% (170/457)\rResolving deltas:  38% (174/457)\rResolving deltas:  39% (179/457)\rResolving deltas:  40% (183/457)\rResolving deltas:  41% (188/457)\rResolving deltas:  42% (192/457)\rResolving deltas:  43% (197/457)\rResolving deltas:  44% (202/457)\rResolving deltas:  45% (206/457)\rResolving deltas:  46% (212/457)\rResolving deltas:  47% (215/457)\rResolving deltas:  48% (220/457)\rResolving deltas:  49% (224/457)\rResolving deltas:  50% (229/457)\rResolving deltas:  51% (234/457)\rResolving deltas:  52% (238/457)\rResolving deltas:  53% (243/457)\rResolving deltas:  54% (247/457)\rResolving deltas:  55% (252/457)\rResolving deltas:  56% (256/457)\rResolving deltas:  57% (261/457)\rResolving deltas:  58% (266/457)\rResolving deltas:  59% (270/457)\rResolving deltas:  60% (275/457)\rResolving deltas:  61% (279/457)\rResolving deltas:  62% (285/457)\rResolving deltas:  63% (288/457)\rResolving deltas:  64% (293/457)\rResolving deltas:  65% (298/457)\rResolving deltas:  66% (302/457)\rResolving deltas:  67% (307/457)\rResolving deltas:  68% (311/457)\rResolving deltas:  69% (316/457)\rResolving deltas:  70% (320/457)\rResolving deltas:  71% (325/457)\rResolving deltas:  72% (330/457)\rResolving deltas:  73% (334/457)\rResolving deltas:  74% (339/457)\rResolving deltas:  75% (343/457)\rResolving deltas:  76% (348/457)\rResolving deltas:  77% (352/457)\rResolving deltas:  78% (357/457)\rResolving deltas:  79% (362/457)\rResolving deltas:  80% (366/457)\rResolving deltas:  81% (371/457)\rResolving deltas:  82% (375/457)\rResolving deltas:  83% (380/457)\rResolving deltas:  84% (384/457)\rResolving deltas:  85% (389/457)\rResolving deltas:  86% (394/457)\rResolving deltas:  87% (398/457)\rResolving deltas:  88% (403/457)\rResolving deltas:  89% (407/457)\rResolving deltas:  90% (412/457)\rResolving deltas:  91% (416/457)\rResolving deltas:  92% (421/457)\rResolving deltas:  93% (426/457)\rResolving deltas:  94% (430/457)\rResolving deltas:  95% (435/457)\rResolving deltas:  96% (439/457)\rResolving deltas:  97% (444/457)\rResolving deltas:  98% (448/457)\rResolving deltas:  99% (453/457)\rResolving deltas: 100% (457/457)\rResolving deltas: 100% (457/457), done.\n```\n:::\n:::\n\n\n## Review the `metadata` of the `cpg0016-jump` dataset {.scrollable}\n\n- [ ] Load the plate-level metadata from the `jump-cellpainting/datasets` repository\n\n::: {#7055e979 .cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\n```\n:::\n\n\n::: {#efc7099b .cell execution_count=4}\n``` {.python .cell-code}\njump_plates_metadata = pd.read_csv(\"datasets/metadata/plate.csv.gz\")\njump_plates_metadata[\"Metadata_PlateType\"].unique()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\narray(['COMPOUND_EMPTY', 'COMPOUND', 'DMSO', 'TARGET2', 'CRISPR', 'ORF',\n       'TARGET1', 'POSCON8'], dtype=object)\n```\n:::\n:::\n\n\n- [ ] Inspect the structure of the database\n\n::: {#916fa75b .cell execution_count=5}\n``` {.python .cell-code}\njump_plates_metadata.groupby([\"Metadata_Source\", \"Metadata_Batch\"]).describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n\n  <div id=\"df-6ccfbbf9-9290-4af2-886f-e009c389b77c\" class=\"colab-df-container\">\n    <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th></th>\n      <th colspan=\"4\" halign=\"left\">Metadata_Plate</th>\n      <th colspan=\"4\" halign=\"left\">Metadata_PlateType</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th></th>\n      <th>count</th>\n      <th>unique</th>\n      <th>top</th>\n      <th>freq</th>\n      <th>count</th>\n      <th>unique</th>\n      <th>top</th>\n      <th>freq</th>\n    </tr>\n    <tr>\n      <th>Metadata_Source</th>\n      <th>Metadata_Batch</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">source_1</th>\n      <th>Batch1_20221004</th>\n      <td>9</td>\n      <td>9</td>\n      <td>UL000109</td>\n      <td>1</td>\n      <td>9</td>\n      <td>2</td>\n      <td>COMPOUND</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>Batch2_20221006</th>\n      <td>7</td>\n      <td>7</td>\n      <td>UL001647</td>\n      <td>1</td>\n      <td>7</td>\n      <td>1</td>\n      <td>COMPOUND</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>Batch3_20221010</th>\n      <td>8</td>\n      <td>8</td>\n      <td>UL000087</td>\n      <td>1</td>\n      <td>8</td>\n      <td>1</td>\n      <td>COMPOUND</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>Batch4_20221012</th>\n      <td>8</td>\n      <td>8</td>\n      <td>UL000081</td>\n      <td>1</td>\n      <td>8</td>\n      <td>1</td>\n      <td>COMPOUND</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>Batch5_20221030</th>\n      <td>11</td>\n      <td>11</td>\n      <td>UL000561</td>\n      <td>1</td>\n      <td>11</td>\n      <td>2</td>\n      <td>COMPOUND</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">source_9</th>\n      <th>20210918-Run11</th>\n      <td>9</td>\n      <td>9</td>\n      <td>GR00004367</td>\n      <td>1</td>\n      <td>9</td>\n      <td>2</td>\n      <td>COMPOUND</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>20210918-Run12</th>\n      <td>8</td>\n      <td>8</td>\n      <td>GR00004377</td>\n      <td>1</td>\n      <td>8</td>\n      <td>1</td>\n      <td>COMPOUND</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>20211013-Run14</th>\n      <td>13</td>\n      <td>13</td>\n      <td>GR00003279</td>\n      <td>1</td>\n      <td>13</td>\n      <td>2</td>\n      <td>COMPOUND</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>20211102-Run15</th>\n      <td>11</td>\n      <td>11</td>\n      <td>GR00004391</td>\n      <td>1</td>\n      <td>11</td>\n      <td>2</td>\n      <td>COMPOUND</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>20211103-Run16</th>\n      <td>17</td>\n      <td>17</td>\n      <td>GR00004405</td>\n      <td>1</td>\n      <td>17</td>\n      <td>2</td>\n      <td>COMPOUND</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n<p>149 rows × 8 columns</p>\n</div>\n    <div class=\"colab-df-buttons\">\n      \n  <div class=\"colab-df-container\">\n    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ccfbbf9-9290-4af2-886f-e009c389b77c')\"\n            title=\"Convert this dataframe to an interactive table.\"\n            style=\"display:none;\">\n      \n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n  </svg>\n    </button>\n    \n  <style>\n    .colab-df-container {\n      display:flex;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    .colab-df-buttons div {\n      margin-bottom: 4px;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n    <script>\n      const buttonEl =\n        document.querySelector('#df-6ccfbbf9-9290-4af2-886f-e009c389b77c button.colab-df-convert');\n      buttonEl.style.display =\n        google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n      async function convertToInteractive(key) {\n        const element = document.querySelector('#df-6ccfbbf9-9290-4af2-886f-e009c389b77c');\n        const dataTable =\n          await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                    [key], {});\n        if (!dataTable) return;\n\n        const docLinkHtml = 'Like what you see? Visit the ' +\n          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n          + ' to learn more about interactive tables.';\n        element.innerHTML = '';\n        dataTable['output_type'] = 'display_data';\n        await google.colab.output.renderOutput(dataTable, element);\n        const docLink = document.createElement('div');\n        docLink.innerHTML = docLinkHtml;\n        element.appendChild(docLink);\n      }\n    </script>\n  </div>\n  \n    </div>\n  </div>\n  \n```\n:::\n:::\n\n\n---\n\n## Subset the dataset to extract samples with *CRISPR*, *ORF*, and *NONE/DMSO* treatments {.scrollable}\n\n- [ ] Read the *CRISPR* plate maps from the `JUMP-Target` repository\n\n::: {#e3fc7db5 .cell execution_count=6}\n``` {.python .cell-code}\ncrispr_wells_metadata = pd.read_csv(\"JUMP-Target/JUMP-Target-1_crispr_platemap.tsv\", sep=\"\\t\")\n```\n:::\n\n\n- [ ] Add one column to identify the type of treatment, and other to assign a numeric label (*CRISPR* = 1)\n\n::: {#bfc7960c .cell execution_count=7}\n``` {.python .cell-code}\ncrispr_wells_metadata[\"Plate_type\"] = \"CRISPR\"\ncrispr_wells_metadata[\"Plate_label\"] = 1\ncrispr_wells_metadata\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n\n  <div id=\"df-59e0bf09-6bf4-4dca-93d3-0e96943e47d7\" class=\"colab-df-container\">\n    <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>well_position</th>\n      <th>broad_sample</th>\n      <th>Plate_type</th>\n      <th>Plate_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A01</td>\n      <td>BRDN0001480888</td>\n      <td>CRISPR</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A02</td>\n      <td>BRDN0001483495</td>\n      <td>CRISPR</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A03</td>\n      <td>BRDN0001147364</td>\n      <td>CRISPR</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A04</td>\n      <td>BRDN0001490272</td>\n      <td>CRISPR</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A05</td>\n      <td>BRDN0001480510</td>\n      <td>CRISPR</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>379</th>\n      <td>P20</td>\n      <td>BRDN0001145303</td>\n      <td>CRISPR</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>380</th>\n      <td>P21</td>\n      <td>BRDN0001484228</td>\n      <td>CRISPR</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>381</th>\n      <td>P22</td>\n      <td>BRDN0001487618</td>\n      <td>CRISPR</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>P23</td>\n      <td>BRDN0001487864</td>\n      <td>CRISPR</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>383</th>\n      <td>P24</td>\n      <td>BRDN0000735603</td>\n      <td>CRISPR</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>384 rows × 4 columns</p>\n</div>\n    <div class=\"colab-df-buttons\">\n      \n  <div class=\"colab-df-container\">\n    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59e0bf09-6bf4-4dca-93d3-0e96943e47d7')\"\n            title=\"Convert this dataframe to an interactive table.\"\n            style=\"display:none;\">\n      \n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n  </svg>\n    </button>\n    \n  <style>\n    .colab-df-container {\n      display:flex;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    .colab-df-buttons div {\n      margin-bottom: 4px;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n    <script>\n      const buttonEl =\n        document.querySelector('#df-59e0bf09-6bf4-4dca-93d3-0e96943e47d7 button.colab-df-convert');\n      buttonEl.style.display =\n        google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n      async function convertToInteractive(key) {\n        const element = document.querySelector('#df-59e0bf09-6bf4-4dca-93d3-0e96943e47d7');\n        const dataTable =\n          await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                    [key], {});\n        if (!dataTable) return;\n\n        const docLinkHtml = 'Like what you see? Visit the ' +\n          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n          + ' to learn more about interactive tables.';\n        element.innerHTML = '';\n        dataTable['output_type'] = 'display_data';\n        await google.colab.output.renderOutput(dataTable, element);\n        const docLink = document.createElement('div');\n        docLink.innerHTML = docLinkHtml;\n        element.appendChild(docLink);\n      }\n    </script>\n  </div>\n  \n    </div>\n  </div>\n  \n```\n:::\n:::\n\n\n---\n\n## Subset the dataset to extract samples with *CRISPR*, *ORF*, and *NONE/DMSO* treatments {.scrollable}\n\n- [ ] Read the *ORF* plate maps from the `JUMP-Target` repository\n\n::: {#d8db3b3b .cell execution_count=8}\n``` {.python .cell-code}\norf_wells_metadata = pd.read_csv(\"JUMP-Target/JUMP-Target-1_orf_platemap.tsv\", sep=\"\\t\")\n```\n:::\n\n\n- [ ] Add one column to identify the type of treatment, and other to assign a numeric label (*ORF* = 2)\n\n::: {#04a301e3 .cell execution_count=9}\n``` {.python .cell-code}\norf_wells_metadata[\"Plate_type\"] = \"ORF\"\norf_wells_metadata[\"Plate_label\"] = 2\norf_wells_metadata\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n\n  <div id=\"df-d57e6e3b-ee1e-4bb0-8478-6df27bf75069\" class=\"colab-df-container\">\n    <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>well_position</th>\n      <th>broad_sample</th>\n      <th>Plate_type</th>\n      <th>Plate_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A01</td>\n      <td>ccsbBroad304_00900</td>\n      <td>ORF</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A02</td>\n      <td>ccsbBroad304_07795</td>\n      <td>ORF</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A03</td>\n      <td>ccsbBroad304_02826</td>\n      <td>ORF</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A04</td>\n      <td>ccsbBroad304_01492</td>\n      <td>ORF</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A05</td>\n      <td>ccsbBroad304_00691</td>\n      <td>ORF</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>379</th>\n      <td>P20</td>\n      <td>ccsbBroad304_00277</td>\n      <td>ORF</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>380</th>\n      <td>P21</td>\n      <td>ccsbBroad304_06464</td>\n      <td>ORF</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>381</th>\n      <td>P22</td>\n      <td>ccsbBroad304_00476</td>\n      <td>ORF</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>P23</td>\n      <td>ccsbBroad304_01649</td>\n      <td>ORF</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>383</th>\n      <td>P24</td>\n      <td>ccsbBroad304_03934</td>\n      <td>ORF</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>384 rows × 4 columns</p>\n</div>\n    <div class=\"colab-df-buttons\">\n      \n  <div class=\"colab-df-container\">\n    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d57e6e3b-ee1e-4bb0-8478-6df27bf75069')\"\n            title=\"Convert this dataframe to an interactive table.\"\n            style=\"display:none;\">\n      \n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n  </svg>\n    </button>\n    \n  <style>\n    .colab-df-container {\n      display:flex;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    .colab-df-buttons div {\n      margin-bottom: 4px;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n    <script>\n      const buttonEl =\n        document.querySelector('#df-d57e6e3b-ee1e-4bb0-8478-6df27bf75069 button.colab-df-convert');\n      buttonEl.style.display =\n        google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n      async function convertToInteractive(key) {\n        const element = document.querySelector('#df-d57e6e3b-ee1e-4bb0-8478-6df27bf75069');\n        const dataTable =\n          await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                    [key], {});\n        if (!dataTable) return;\n\n        const docLinkHtml = 'Like what you see? Visit the ' +\n          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n          + ' to learn more about interactive tables.';\n        element.innerHTML = '';\n        dataTable['output_type'] = 'display_data';\n        await google.colab.output.renderOutput(dataTable, element);\n        const docLink = document.createElement('div');\n        docLink.innerHTML = docLinkHtml;\n        element.appendChild(docLink);\n      }\n    </script>\n  </div>\n  \n    </div>\n  </div>\n  \n```\n:::\n:::\n\n\n---\n\n## Subset the dataset to extract samples with *CRISPR*, *ORF*, and *NONE/DMSO* treatments {.scrollable}\n\n- [ ] Read the *COMPOUND* plate maps from the `JUMP-Target` repository\n\n::: {#4f079458 .cell execution_count=10}\n``` {.python .cell-code}\ncompound_wells_metadata = pd.read_csv(\"JUMP-Target/JUMP-Target-1_compound_platemap.tsv\", sep=\"\\t\")\n```\n:::\n\n\n- [ ] Add one column to identify the type of treatment, and other to assign a numeric label (*COMPOUND* = 3)\n\n::: {#510bd7fd .cell execution_count=11}\n``` {.python .cell-code}\ncompound_wells_metadata[\"Plate_type\"] = \"COMPOUND\"\ncompound_wells_metadata[\"Plate_label\"] = 3\ncompound_wells_metadata\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n\n  <div id=\"df-7e94d6f9-1da6-4cf9-8015-777f9d5398ef\" class=\"colab-df-container\">\n    <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>well_position</th>\n      <th>broad_sample</th>\n      <th>solvent</th>\n      <th>Plate_type</th>\n      <th>Plate_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A01</td>\n      <td>BRD-A86665761-001-01-1</td>\n      <td>DMSO</td>\n      <td>COMPOUND</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A02</td>\n      <td>NaN</td>\n      <td>DMSO</td>\n      <td>COMPOUND</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A03</td>\n      <td>BRD-A22032524-074-09-9</td>\n      <td>DMSO</td>\n      <td>COMPOUND</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A04</td>\n      <td>BRD-A01078468-001-14-8</td>\n      <td>DMSO</td>\n      <td>COMPOUND</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A05</td>\n      <td>BRD-K48278478-001-01-2</td>\n      <td>DMSO</td>\n      <td>COMPOUND</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>379</th>\n      <td>P20</td>\n      <td>BRD-K68982262-001-01-4</td>\n      <td>DMSO</td>\n      <td>COMPOUND</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>380</th>\n      <td>P21</td>\n      <td>BRD-K24616672-003-20-1</td>\n      <td>DMSO</td>\n      <td>COMPOUND</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>381</th>\n      <td>P22</td>\n      <td>BRD-A82396632-008-30-8</td>\n      <td>DMSO</td>\n      <td>COMPOUND</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>P23</td>\n      <td>BRD-K61250553-003-30-6</td>\n      <td>DMSO</td>\n      <td>COMPOUND</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>383</th>\n      <td>P24</td>\n      <td>BRD-K70358946-001-17-3</td>\n      <td>DMSO</td>\n      <td>COMPOUND</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>384 rows × 5 columns</p>\n</div>\n    <div class=\"colab-df-buttons\">\n      \n  <div class=\"colab-df-container\">\n    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e94d6f9-1da6-4cf9-8015-777f9d5398ef')\"\n            title=\"Convert this dataframe to an interactive table.\"\n            style=\"display:none;\">\n      \n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n  </svg>\n    </button>\n    \n  <style>\n    .colab-df-container {\n      display:flex;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    .colab-df-buttons div {\n      margin-bottom: 4px;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n    <script>\n      const buttonEl =\n        document.querySelector('#df-7e94d6f9-1da6-4cf9-8015-777f9d5398ef button.colab-df-convert');\n      buttonEl.style.display =\n        google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n      async function convertToInteractive(key) {\n        const element = document.querySelector('#df-7e94d6f9-1da6-4cf9-8015-777f9d5398ef');\n        const dataTable =\n          await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                    [key], {});\n        if (!dataTable) return;\n\n        const docLinkHtml = 'Like what you see? Visit the ' +\n          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n          + ' to learn more about interactive tables.';\n        element.innerHTML = '';\n        dataTable['output_type'] = 'display_data';\n        await google.colab.output.renderOutput(dataTable, element);\n        const docLink = document.createElement('div');\n        docLink.innerHTML = docLinkHtml;\n        element.appendChild(docLink);\n      }\n    </script>\n  </div>\n  \n    </div>\n  </div>\n  \n```\n:::\n:::\n\n\n---\n\n## Subset the dataset to extract samples with *CRISPR*, *ORF*, and *NONE/DMSO* treatments {.scrollable}\n\n- [ ] Concatenate the three tables into a single table\n\n::: {#54231686 .cell execution_count=12}\n``` {.python .cell-code}\nwells_metadata = pd.concat([compound_wells_metadata, orf_wells_metadata, crispr_wells_metadata])\n```\n:::\n\n\n- [ ] Change the label of the well positions that were not treated with any compound (*NONE/DMSO* = 0)\n\n::: {#3763b687 .cell execution_count=13}\n``` {.python .cell-code}\nwells_metadata.loc[wells_metadata[\"broad_sample\"].isna(), \"Plate_label\"] = 0\nwells_metadata\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n\n  <div id=\"df-2b583fe8-6f4e-4de5-ba7b-20223daca887\" class=\"colab-df-container\">\n    <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>well_position</th>\n      <th>broad_sample</th>\n      <th>solvent</th>\n      <th>Plate_type</th>\n      <th>Plate_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A01</td>\n      <td>BRD-A86665761-001-01-1</td>\n      <td>DMSO</td>\n      <td>COMPOUND</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A02</td>\n      <td>NaN</td>\n      <td>DMSO</td>\n      <td>COMPOUND</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A03</td>\n      <td>BRD-A22032524-074-09-9</td>\n      <td>DMSO</td>\n      <td>COMPOUND</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A04</td>\n      <td>BRD-A01078468-001-14-8</td>\n      <td>DMSO</td>\n      <td>COMPOUND</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A05</td>\n      <td>BRD-K48278478-001-01-2</td>\n      <td>DMSO</td>\n      <td>COMPOUND</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>379</th>\n      <td>P20</td>\n      <td>BRDN0001145303</td>\n      <td>NaN</td>\n      <td>CRISPR</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>380</th>\n      <td>P21</td>\n      <td>BRDN0001484228</td>\n      <td>NaN</td>\n      <td>CRISPR</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>381</th>\n      <td>P22</td>\n      <td>BRDN0001487618</td>\n      <td>NaN</td>\n      <td>CRISPR</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>P23</td>\n      <td>BRDN0001487864</td>\n      <td>NaN</td>\n      <td>CRISPR</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>383</th>\n      <td>P24</td>\n      <td>BRDN0000735603</td>\n      <td>NaN</td>\n      <td>CRISPR</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>1152 rows × 5 columns</p>\n</div>\n    <div class=\"colab-df-buttons\">\n      \n  <div class=\"colab-df-container\">\n    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b583fe8-6f4e-4de5-ba7b-20223daca887')\"\n            title=\"Convert this dataframe to an interactive table.\"\n            style=\"display:none;\">\n      \n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n  </svg>\n    </button>\n    \n  <style>\n    .colab-df-container {\n      display:flex;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    .colab-df-buttons div {\n      margin-bottom: 4px;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n    <script>\n      const buttonEl =\n        document.querySelector('#df-2b583fe8-6f4e-4de5-ba7b-20223daca887 button.colab-df-convert');\n      buttonEl.style.display =\n        google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n      async function convertToInteractive(key) {\n        const element = document.querySelector('#df-2b583fe8-6f4e-4de5-ba7b-20223daca887');\n        const dataTable =\n          await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                    [key], {});\n        if (!dataTable) return;\n\n        const docLinkHtml = 'Like what you see? Visit the ' +\n          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n          + ' to learn more about interactive tables.';\n        element.innerHTML = '';\n        dataTable['output_type'] = 'display_data';\n        await google.colab.output.renderOutput(dataTable, element);\n        const docLink = document.createElement('div');\n        docLink.innerHTML = docLinkHtml;\n        element.appendChild(docLink);\n      }\n    </script>\n  </div>\n  \n    </div>\n  </div>\n  \n```\n:::\n:::\n\n\n---\n\n## Get the URL of each assay plate from the S3 bucket {.scrollable}\n\n- [ ] List the plates in the `cellpainting-gallery/cpg0016-jump` bucket\n\n::: {#1416a081 .cell execution_count=14}\n``` {.python .cell-code}\nimport s3fs\n\nfs = s3fs.S3FileSystem(anon=True)\n\nbatch_names = {}\nplate_paths = {}\nsource_names = {}\nplate_types = {}\n\nfor _, src_row in jump_plates_metadata.groupby([\"Metadata_Source\", \"Metadata_Batch\"]).describe().iterrows():\n    source_name, batch_name = src_row.name\n\n    # Ignore 'source_8' since the naming of the images is not standard\n    if source_name in [\"source_8\"]:\n        continue\n\n    plate_type = src_row[\"Metadata_PlateType\"].top\n\n    for plate_path in fs.ls(f\"cellpainting-gallery/cpg0016-jump/{source_name}/images/{batch_name}/images/\"):\n        plate_path = plate_path.split(\"/\")[-1]\n        if not plate_path:\n            continue\n\n        plate_name = plate_path.split(\"__\")[0]\n\n        source_names[plate_name] = source_name\n        batch_names[plate_name] = batch_name\n        plate_types[plate_name] = plate_type\n        plate_paths[plate_name] = plate_path\n```\n:::\n\n\n---\n\n## Get the URL of each assay plate from the S3 bucket {.scrollable}\n\n- [ ] Create a `pandas` data frame to associate each plate to its location within the S3 bucket\n\n::: {#1b7dee7e .cell execution_count=15}\n``` {.python .cell-code}\nplate_maps = pd.DataFrame()\n\nplate_maps[\"Plate_name\"] = batch_names.keys()\nplate_maps[\"Source_name\"] = plate_maps[\"Plate_name\"].map(source_names)\nplate_maps[\"Batch_name\"] = plate_maps[\"Plate_name\"].map(batch_names)\nplate_maps[\"Plate_type\"] = plate_maps[\"Plate_name\"].map(plate_types)\nplate_maps[\"Plate_path\"] = plate_maps[\"Plate_name\"].map(plate_paths)\n\nplate_maps\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n\n  <div id=\"df-693094a1-c76b-40ce-9ac8-22068ae6bed3\" class=\"colab-df-container\">\n    <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Plate_name</th>\n      <th>Source_name</th>\n      <th>Batch_name</th>\n      <th>Plate_type</th>\n      <th>Plate_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>UL000109</td>\n      <td>source_1</td>\n      <td>Batch1_20221004</td>\n      <td>COMPOUND</td>\n      <td>UL000109__2022-10-05T06_35_06-Measurement1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>UL001641</td>\n      <td>source_1</td>\n      <td>Batch1_20221004</td>\n      <td>COMPOUND</td>\n      <td>UL001641__2022-10-04T23_16_28-Measurement1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>UL001643</td>\n      <td>source_1</td>\n      <td>Batch1_20221004</td>\n      <td>COMPOUND</td>\n      <td>UL001643__2022-10-04T18_52_42-Measurement2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>UL001645</td>\n      <td>source_1</td>\n      <td>Batch1_20221004</td>\n      <td>COMPOUND</td>\n      <td>UL001645__2022-10-05T00_44_11-Measurement1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>UL001651</td>\n      <td>source_1</td>\n      <td>Batch1_20221004</td>\n      <td>COMPOUND</td>\n      <td>UL001651__2022-10-04T20_20_52-Measurement1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2333</th>\n      <td>GR00004417</td>\n      <td>source_9</td>\n      <td>20211103-Run16</td>\n      <td>COMPOUND</td>\n      <td>GR00004417</td>\n    </tr>\n    <tr>\n      <th>2334</th>\n      <td>GR00004418</td>\n      <td>source_9</td>\n      <td>20211103-Run16</td>\n      <td>COMPOUND</td>\n      <td>GR00004418</td>\n    </tr>\n    <tr>\n      <th>2335</th>\n      <td>GR00004419</td>\n      <td>source_9</td>\n      <td>20211103-Run16</td>\n      <td>COMPOUND</td>\n      <td>GR00004419</td>\n    </tr>\n    <tr>\n      <th>2336</th>\n      <td>GR00004420</td>\n      <td>source_9</td>\n      <td>20211103-Run16</td>\n      <td>COMPOUND</td>\n      <td>GR00004420</td>\n    </tr>\n    <tr>\n      <th>2337</th>\n      <td>GR00004421</td>\n      <td>source_9</td>\n      <td>20211103-Run16</td>\n      <td>COMPOUND</td>\n      <td>GR00004421</td>\n    </tr>\n  </tbody>\n</table>\n<p>2338 rows × 5 columns</p>\n</div>\n    <div class=\"colab-df-buttons\">\n      \n  <div class=\"colab-df-container\">\n    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-693094a1-c76b-40ce-9ac8-22068ae6bed3')\"\n            title=\"Convert this dataframe to an interactive table.\"\n            style=\"display:none;\">\n      \n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n  </svg>\n    </button>\n    \n  <style>\n    .colab-df-container {\n      display:flex;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    .colab-df-buttons div {\n      margin-bottom: 4px;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n    <script>\n      const buttonEl =\n        document.querySelector('#df-693094a1-c76b-40ce-9ac8-22068ae6bed3 button.colab-df-convert');\n      buttonEl.style.display =\n        google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n      async function convertToInteractive(key) {\n        const element = document.querySelector('#df-693094a1-c76b-40ce-9ac8-22068ae6bed3');\n        const dataTable =\n          await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                    [key], {});\n        if (!dataTable) return;\n\n        const docLinkHtml = 'Like what you see? Visit the ' +\n          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n          + ' to learn more about interactive tables.';\n        element.innerHTML = '';\n        dataTable['output_type'] = 'display_data';\n        await google.colab.output.renderOutput(dataTable, element);\n        const docLink = document.createElement('div');\n        docLink.innerHTML = docLinkHtml;\n        element.appendChild(docLink);\n      }\n    </script>\n  </div>\n  \n    </div>\n  </div>\n  \n```\n:::\n:::\n\n\n---\n\n## Subset the data frame to separate perturbation (*CRISPR/ORF/NONE*) plates from *COMPOUND* plates {.scrollable}\n\n- [ ] Subset the *COMPOUND* plates\n\n::: {#6282f0c2 .cell execution_count=16}\n``` {.python .cell-code}\ncomp_plate_maps = plate_maps.query(\"Plate_type=='COMPOUND'\")\ncomp_plate_maps\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n\n  <div id=\"df-496607a4-7041-47f2-b05e-54dfbeaed6f7\" class=\"colab-df-container\">\n    <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Plate_name</th>\n      <th>Source_name</th>\n      <th>Batch_name</th>\n      <th>Plate_type</th>\n      <th>Plate_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>UL000109</td>\n      <td>source_1</td>\n      <td>Batch1_20221004</td>\n      <td>COMPOUND</td>\n      <td>UL000109__2022-10-05T06_35_06-Measurement1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>UL001641</td>\n      <td>source_1</td>\n      <td>Batch1_20221004</td>\n      <td>COMPOUND</td>\n      <td>UL001641__2022-10-04T23_16_28-Measurement1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>UL001643</td>\n      <td>source_1</td>\n      <td>Batch1_20221004</td>\n      <td>COMPOUND</td>\n      <td>UL001643__2022-10-04T18_52_42-Measurement2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>UL001645</td>\n      <td>source_1</td>\n      <td>Batch1_20221004</td>\n      <td>COMPOUND</td>\n      <td>UL001645__2022-10-05T00_44_11-Measurement1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>UL001651</td>\n      <td>source_1</td>\n      <td>Batch1_20221004</td>\n      <td>COMPOUND</td>\n      <td>UL001651__2022-10-04T20_20_52-Measurement1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2333</th>\n      <td>GR00004417</td>\n      <td>source_9</td>\n      <td>20211103-Run16</td>\n      <td>COMPOUND</td>\n      <td>GR00004417</td>\n    </tr>\n    <tr>\n      <th>2334</th>\n      <td>GR00004418</td>\n      <td>source_9</td>\n      <td>20211103-Run16</td>\n      <td>COMPOUND</td>\n      <td>GR00004418</td>\n    </tr>\n    <tr>\n      <th>2335</th>\n      <td>GR00004419</td>\n      <td>source_9</td>\n      <td>20211103-Run16</td>\n      <td>COMPOUND</td>\n      <td>GR00004419</td>\n    </tr>\n    <tr>\n      <th>2336</th>\n      <td>GR00004420</td>\n      <td>source_9</td>\n      <td>20211103-Run16</td>\n      <td>COMPOUND</td>\n      <td>GR00004420</td>\n    </tr>\n    <tr>\n      <th>2337</th>\n      <td>GR00004421</td>\n      <td>source_9</td>\n      <td>20211103-Run16</td>\n      <td>COMPOUND</td>\n      <td>GR00004421</td>\n    </tr>\n  </tbody>\n</table>\n<p>1905 rows × 5 columns</p>\n</div>\n    <div class=\"colab-df-buttons\">\n      \n  <div class=\"colab-df-container\">\n    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-496607a4-7041-47f2-b05e-54dfbeaed6f7')\"\n            title=\"Convert this dataframe to an interactive table.\"\n            style=\"display:none;\">\n      \n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n  </svg>\n    </button>\n    \n  <style>\n    .colab-df-container {\n      display:flex;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    .colab-df-buttons div {\n      margin-bottom: 4px;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n    <script>\n      const buttonEl =\n        document.querySelector('#df-496607a4-7041-47f2-b05e-54dfbeaed6f7 button.colab-df-convert');\n      buttonEl.style.display =\n        google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n      async function convertToInteractive(key) {\n        const element = document.querySelector('#df-496607a4-7041-47f2-b05e-54dfbeaed6f7');\n        const dataTable =\n          await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                    [key], {});\n        if (!dataTable) return;\n\n        const docLinkHtml = 'Like what you see? Visit the ' +\n          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n          + ' to learn more about interactive tables.';\n        element.innerHTML = '';\n        dataTable['output_type'] = 'display_data';\n        await google.colab.output.renderOutput(dataTable, element);\n        const docLink = document.createElement('div');\n        docLink.innerHTML = docLinkHtml;\n        element.appendChild(docLink);\n      }\n    </script>\n  </div>\n  \n    </div>\n  </div>\n  \n```\n:::\n:::\n\n\n- [ ] Subset the *CRISPR/ORF/DMSO* plates\n\n::: {#e28b71f9 .cell execution_count=17}\n``` {.python .cell-code}\npert_plate_maps = plate_maps[plate_maps[\"Plate_type\"].isin([\"CRISPR\", \"ORF\", \"DMSO\"])]\npert_plate_maps\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```{=html}\n\n  <div id=\"df-90806d63-2489-49a7-997b-7ab9dbde3d27\" class=\"colab-df-container\">\n    <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Plate_name</th>\n      <th>Source_name</th>\n      <th>Batch_name</th>\n      <th>Plate_type</th>\n      <th>Plate_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>142</th>\n      <td>Dest210628-161651</td>\n      <td>source_10</td>\n      <td>2021_06_28_U2OS_48_hr_run9</td>\n      <td>DMSO</td>\n      <td>Dest210628-161651</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>Dest210628-162003</td>\n      <td>source_10</td>\n      <td>2021_06_28_U2OS_48_hr_run9</td>\n      <td>DMSO</td>\n      <td>Dest210628-162003</td>\n    </tr>\n    <tr>\n      <th>457</th>\n      <td>CP-CC9-R1-01</td>\n      <td>source_13</td>\n      <td>20220914_Run1</td>\n      <td>CRISPR</td>\n      <td>CP-CC9-R1-01</td>\n    </tr>\n    <tr>\n      <th>458</th>\n      <td>CP-CC9-R1-02</td>\n      <td>source_13</td>\n      <td>20220914_Run1</td>\n      <td>CRISPR</td>\n      <td>CP-CC9-R1-02</td>\n    </tr>\n    <tr>\n      <th>459</th>\n      <td>CP-CC9-R1-03</td>\n      <td>source_13</td>\n      <td>20220914_Run1</td>\n      <td>CRISPR</td>\n      <td>CP-CC9-R1-03</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1591</th>\n      <td>BR00127145</td>\n      <td>source_4</td>\n      <td>2021_08_30_Batch13</td>\n      <td>ORF</td>\n      <td>BR00127145__2021-09-22T04_01_46-Measurement1</td>\n    </tr>\n    <tr>\n      <th>1592</th>\n      <td>BR00127146</td>\n      <td>source_4</td>\n      <td>2021_08_30_Batch13</td>\n      <td>ORF</td>\n      <td>BR00127146__2021-09-22T12_25_07-Measurement1</td>\n    </tr>\n    <tr>\n      <th>1593</th>\n      <td>BR00127147</td>\n      <td>source_4</td>\n      <td>2021_08_30_Batch13</td>\n      <td>ORF</td>\n      <td>BR00127147__2021-09-18T10_27_12-Measurement1</td>\n    </tr>\n    <tr>\n      <th>1594</th>\n      <td>BR00127148</td>\n      <td>source_4</td>\n      <td>2021_08_30_Batch13</td>\n      <td>ORF</td>\n      <td>BR00127148__2021-09-21T11_44_23-Measurement1</td>\n    </tr>\n    <tr>\n      <th>1595</th>\n      <td>BR00127149</td>\n      <td>source_4</td>\n      <td>2021_08_30_Batch13</td>\n      <td>ORF</td>\n      <td>BR00127149__2021-09-18T02_10_04-Measurement1</td>\n    </tr>\n  </tbody>\n</table>\n<p>433 rows × 5 columns</p>\n</div>\n    <div class=\"colab-df-buttons\">\n      \n  <div class=\"colab-df-container\">\n    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90806d63-2489-49a7-997b-7ab9dbde3d27')\"\n            title=\"Convert this dataframe to an interactive table.\"\n            style=\"display:none;\">\n      \n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n  </svg>\n    </button>\n    \n  <style>\n    .colab-df-container {\n      display:flex;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    .colab-df-buttons div {\n      margin-bottom: 4px;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n    <script>\n      const buttonEl =\n        document.querySelector('#df-90806d63-2489-49a7-997b-7ab9dbde3d27 button.colab-df-convert');\n      buttonEl.style.display =\n        google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n      async function convertToInteractive(key) {\n        const element = document.querySelector('#df-90806d63-2489-49a7-997b-7ab9dbde3d27');\n        const dataTable =\n          await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                    [key], {});\n        if (!dataTable) return;\n\n        const docLinkHtml = 'Like what you see? Visit the ' +\n          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n          + ' to learn more about interactive tables.';\n        element.innerHTML = '';\n        dataTable['output_type'] = 'display_data';\n        await google.colab.output.renderOutput(dataTable, element);\n        const docLink = document.createElement('div');\n        docLink.innerHTML = docLinkHtml;\n        element.appendChild(docLink);\n      }\n    </script>\n  </div>\n  \n    </div>\n  </div>\n  \n```\n:::\n:::\n\n\n# Create a pipeline for loading images from the `cellpainting-gallery` storage\n\n## Split the perturbation plates into Training, Validation, and Test sets {.scrollable}\n\nWe'll separate the plates in each batch into the three sets to have batch-level effects in each of the sets\n\n- [ ] Assign $70$ % of plates for training, $20$ % for validation, and $10$ % for testing\n\n::: {#519e4428 .cell execution_count=18}\n``` {.python .cell-code}\nimport random\nimport math\n```\n:::\n\n\n::: {#d1e1cf0e .cell execution_count=19}\n``` {.python .cell-code}\ntrn_plates = []\nval_plates = []\ntst_plates = []\n\ntrn_proportion = 0.7\nval_proportion = 0.2\ntst_proportion = 0.1\n\nfor batch_name in pert_plate_maps[\"Batch_name\"].unique():\n    plate_names = pert_plate_maps.query(f\"Batch_name == '{batch_name}'\")[\"Plate_name\"].tolist()\n    random.shuffle(plate_names)\n\n    tst_plates_count = int(math.ceil(len(plate_names) * tst_proportion))\n    val_plates_count = int(math.ceil(len(plate_names) * val_proportion))\n\n    tst_plates += plate_names[:tst_plates_count]\n    val_plates += plate_names[tst_plates_count:tst_plates_count + val_plates_count]\n    trn_plates += plate_names[tst_plates_count + val_plates_count:]\n```\n:::\n\n\n::: {#8c9334e9 .cell execution_count=20}\n``` {.python .cell-code}\nprint(\"Training set size:\", len(trn_plates))\nprint(\"Validation set size:\", len(val_plates))\nprint(\"Testing set size:\", len(tst_plates))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining set size: 283\nValidation set size: 96\nTesting set size: 54\n```\n:::\n:::\n\n\n---\n\n## Create a PyTorch Dataset to load images from S3 storage {.scrollable}\n\nDefining a custom PyTorch dataset allows us to access the image data from S3 storage, even if it is not in a standard format across the distinct sources inside the database.\nMoreover, it is completely iterative, so no additional storage is used as the images are analyzed *on the fly*.\n\n::: {#637d0401 .cell execution_count=21}\n``` {.python .cell-code}\n# @title Definition of a Dataset class capable to pull images from S3 buckets\nimport random\nimport numpy as np\nimport string\nimport s3fs\n\nfrom itertools import product\n\nfrom PIL import Image\nimport tifffile\n\nfrom torch.utils.data import IterableDataset, get_worker_info\n```\n:::\n\n\n- [ ] Define a function to load an image from the `cellpainting-gallery` bucket given a *field-well* position from a specific *plate*\n\n::: {#b6ef2f4e .cell execution_count=22}\n``` {.python .cell-code}\ndef load_well(plate_metadata, well_row, well_col, field_id, channels, s3):\n    # Get the label of the current well\n    curr_well_image = []\n\n    plate_path = \"cellpainting-gallery/cpg0016-jump/\" + plate_metadata[\"Source_name\"] + \"/images/\" + plate_metadata[\"Batch_name\"] + \"/images/\" + plate_metadata[\"Plate_path\"]\n\n    for channel_id in range(channels):\n        if plate_metadata[\"Source_name\"] in [\"source_1\", \"source_3\", \"source_4\", \"source_9\", \"source_11\", \"source_15\"]:\n            image_suffix = f\"Images/r{well_row + 1:02d}c{well_col + 1:02d}f{field_id + 1:02d}p01-ch{channel_id + 1}sk1fk1fl1.tiff\"\n\n        else:\n            if plate_metadata[\"Source_name\"] in [\"source_2\", \"source_5\"]:\n                a_locs = [1, 2, 3, 4, 5]\n            elif plate_metadata[\"Source_name\"] in [\"source_6\", \"source_10\"]:\n                a_locs = [1, 2, 2, 3, 1, 4]\n            elif plate_metadata[\"Source_name\"] in [\"source_7\", \"source_13\"]:\n                a_locs = [1, 1, 2, 3, 4]\n\n            image_suffix = f\"{plate_metadata[\"Plate_name\"]}_{string.ascii_uppercase[well_row]}{well_col + 1:02d}_T0001F{field_id + 1:03d}L01A{a_locs[channel_id]:02d}Z01C{channel_id + 1:02d}.tif\"\n\n        image_url = \"s3://\" + plate_path + \"/\" + image_suffix\n\n        try:\n            with s3.open(image_url, 'rb') as f:\n                curr_image = tifffile.imread(f)\n\n        except FileNotFoundError:\n            print(\"Failed retrieving:\", image_url)\n            return None\n\n        curr_image = curr_image.astype(np.float32)\n        curr_image /= 2 ** 16 - 1\n\n        curr_well_image.append(curr_image)\n\n    curr_well_image = np.array(curr_well_image)\n\n    return curr_well_image\n```\n:::\n\n\n---\n\n## Create a PyTorch Dataset to load images from S3 storage {.scrollable}\n\n- [ ] Define an `IterableDataset` derived class to load images from S3 storage\n\n::: {#8f3f3b90 .cell execution_count=23}\n``` {.python .cell-code}\nclass TiffS3Dataset(IterableDataset):\n    \"\"\"This dataset could have virtually infinite samples.\n    \"\"\"\n    def __init__(self, plate_maps, wells_metadata, plate_names, well_rows=24, well_cols=16, fields=4, channels=5, shuffle=False):\n        super(TiffS3Dataset).__init__()\n\n        self._plate_maps = plate_maps\n        self._wells_metadata = wells_metadata\n\n        self._plate_names = plate_names\n        self._well_rows = well_rows\n        self._well_cols = well_cols\n        self._fields = fields\n        self._channels = channels\n\n        self._shuffle = shuffle\n\n        self._worker_sel = slice(0, len(plate_names) * self._well_rows * self._well_cols)\n        self._worker_id = 0\n        self._num_workers = 1\n\n        self._s3 = None\n\n    def __iter__(self):\n        # Select the barcodes that correspond to this worker\n        self._s3 = s3fs.S3FileSystem(anon=True)\n\n        self._plate_names = self._plate_names[self._worker_sel]\n\n        well_row_range = range(self._well_rows)\n        well_col_range = range(self._well_cols)\n        fields_range = range(self._fields)\n\n        for plate_name, well_row, well_col, field_id in product(self._plate_names, well_row_range, well_col_range, fields_range):\n            if self._shuffle:\n                plate_name = random.choice(self._plate_names)\n                well_row = random.randrange(self._well_rows)\n                well_col = random.randrange(self._well_cols)\n                field_id = random.randrange(self._fields)\n\n            curr_plate_map = self._plate_maps.query(f\"Plate_name == '{plate_name}'\")\n\n            curr_plate_metadata = curr_plate_map.to_dict(orient='records')[0]\n\n            if not len(curr_plate_metadata):\n                continue\n\n            curr_image = load_well(curr_plate_metadata, well_row, well_col, field_id, self._channels, self._s3)\n\n            if curr_image is None:\n                continue\n\n            curr_plate_metadata[\"Well_position\"] = f\"{string.ascii_uppercase[well_row]}{well_col + 1:02d}\"\n\n            curr_image = curr_image[:, :1080, :1080]\n            _, h, w = curr_image.shape\n            pad_h = 1080 - h\n            pad_w = 1080 - w\n\n            if pad_h or pad_w:\n                curr_image = np.pad(curr_image, ((0, 0), (0, pad_h), (0, pad_w)))\n            \n            if curr_plate_metadata[\"Plate_type\"] == \"DMSO\":\n                curr_label = 0\n\n            else:\n                curr_label = self._wells_metadata.query(f\"Plate_type=='{curr_plate_metadata[\"Plate_type\"]}' & well_position=='{string.ascii_uppercase[well_row]}{well_col + 1:02d}'\")[\"Plate_label\"]\n\n                if not len(curr_label):\n                    continue\n\n                curr_label = curr_label.item()\n\n            yield curr_image, curr_label, curr_plate_metadata\n\n        self._s3 = None\n```\n:::\n\n\n---\n\n## Create a PyTorch Dataset to load images from S3 storage {.scrollable}\n\n- [ ] Define an initialization function to separate the load when using *multi-thread* data loading\n\n::: {#782c35ca .cell execution_count=24}\n``` {.python .cell-code}\ndef dataset_worker_init_fn(worker_id):\n    \"\"\"ZarrDataset multithread workers initialization function.\n    \"\"\"\n    worker_info = torch.utils.data.get_worker_info()\n    w_sel = slice(worker_id, None, worker_info.num_workers)\n\n    dataset_obj = worker_info.dataset\n\n    # Reset the random number generators in each worker.\n    torch_seed = torch.initial_seed()\n\n    dataset_obj._worker_sel = w_sel\n    dataset_obj._worker_id = worker_id\n    dataset_obj._num_workers = worker_info.num_workers\n```\n:::\n\n\n---\n\n## Create the different datasets from the plates lists {.scrollable}\n\n- [ ] Instantiate a `TiffS3Dataset` from the plates assigned for training, validation, and testing\n\n::: {#e10b9000 .cell execution_count=25}\n``` {.python .cell-code}\ntraining_ds = TiffS3Dataset(pert_plate_maps, wells_metadata, trn_plates, 16, 24, 9, 5, shuffle=True)\nvalidation_ds = TiffS3Dataset(pert_plate_maps, wells_metadata, val_plates, 16, 24, 9, 5, shuffle=True)\ntesting_ds = TiffS3Dataset(pert_plate_maps, wells_metadata, tst_plates, 16, 24, 9, 5, shuffle=True)\n```\n:::\n\n\n# Compute field-level morphological profiles from perturbation plates\n\nWe'll use a pre-trained deep learning model for image recognition to extract morphological features at field-level.\n\nThis process is usually applied at cell level; however, we'll analyze the data at field-level for simplicity.\n\nThese morphological profiles will be used subsequently to train a *perturbation classifier*.\n\n---\n\n## Import a pre-trained model from `torchvision`\n\nWe'll start with a pre-trained *MobileNet* model for feature extraction since it is lightweight and fast.\nThis in terms of computation resources required to use this model.\n\nIn the literature, more complex models are used, such as Inception V3, DenseNet, or even Vision Transformers.\nHowever, these models require GPU acceleration to be efficiently applied.\n\n- [ ] Load a pre-trained *MobileNet* from the `torchvision` package\n\n::: {#9f6eb48d .cell execution_count=26}\n``` {.python .cell-code}\nfrom torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n```\n:::\n\n\n::: {#6b42696f .cell execution_count=27}\n``` {.python .cell-code}\nweights = MobileNet_V3_Small_Weights.DEFAULT\nmodel = mobilenet_v3_small(weights=weights)\n```\n:::\n\n\n---\n\n## Modify the model's architecture to convert it into a feature extraction *function* {.scrollable}\n\n- [ ] Save the original aggregation layer (Average Pooling) and replace it with an `Identity` layer\n\n::: {#982017e9 .cell execution_count=28}\n``` {.python .cell-code}\nimport torch\n```\n:::\n\n\n::: {#e649a4db .cell execution_count=29}\n``` {.python .cell-code}\norg_avgpool = model.avgpool\nmodel.avgpool = torch.nn.Identity()\n```\n:::\n\n\n- [ ] Replace the original `classifier` layer with an `Identity` layer\n\n::: {#340183c7 .cell execution_count=30}\n``` {.python .cell-code}\nmodel.classifier = torch.nn.Identity()\n```\n:::\n\n\n---\n\n## Modify the model's architecture to convert it into a feature extraction *function* {.scrollable}\n\n- [ ] Set the model into *evaluation* mode. (Optional) Move the model to the GPU if available\n\n::: {#fcd366d0 .cell execution_count=31}\n``` {.python .cell-code}\nif torch.cuda.is_available():\n    model.cuda()\n\nmodel.eval()\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\nMobileNetV3(\n  (features): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n      (2): Hardswish()\n    )\n    (1): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (1): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (2): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (3): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (4): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (5): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (6): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (7): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (8): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (9): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (10): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (11): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (12): Conv2dNormActivation(\n      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n      (2): Hardswish()\n    )\n  )\n  (avgpool): Identity()\n  (classifier): Identity()\n)\n```\n:::\n:::\n\n\n---\n\n## Load the pre-processing transforms form the original model {.scrollable}\n\nWe need to apply the same transforms to the images that we *feed* to the model to have the expected behavior.\n\n- [ ] Get the original transforms pipeline used when the model was originally trained\n\n::: {#3d5c4371 .cell execution_count=32}\n``` {.python .cell-code}\nmodel_transforms = weights.transforms()\nmodel_transforms\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\nImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n```\n:::\n:::\n\n\n---\n\n## Create a PyTorch `DataLoader`\n\nA `DataLoader` takes a `Dataset` (or `IterableDataset`) and serves *mini-batches* of samples that can be used for model training or evaluation.\nIt manages the *mini-batch* collation, and if enabled, the *multi-thread* loading of data.\n\n::: {#f5fe0df6 .cell execution_count=33}\n``` {.python .cell-code}\nfrom torch.utils.data.dataloader import DataLoader\n```\n:::\n\n\n::: {#1687ea1b .cell execution_count=34}\n``` {.python .cell-code}\nbatch_size = 10\n\ntraining_dl = DataLoader(training_ds, batch_size=batch_size, num_workers=2, worker_init_fn=dataset_worker_init_fn)\n```\n:::\n\n\n---\n\n## Execute the feature extraction with the deep learning model {.scrollable}\n\n- [ ] Use the *frozen* deep learning model to extract morphological features from the field-level images and create a features database\n\n::: {#fa04567d .cell execution_count=35}\n``` {.python .cell-code}\nfrom tqdm.auto import tqdm\n\nfeatures = []\ntargets = []\n\nfor i, (x, y, _) in tqdm(enumerate(training_dl)):\n    b, c, h, w = x.shape\n    x_t = model_transforms(torch.tile(x.reshape(-1, 1, h, w), (1, 3, 1, 1)))\n\n    if torch.cuda.is_available():\n        x_t = x_t.cuda()\n    \n    with torch.no_grad():\n        x_out = model(x_t)\n        x_out = x_out.detach().cpu().reshape(-1, c, 576, 7, 7).sum(dim=1)\n        x_out = org_avgpool(x_out).detach().reshape(b, -1)\n\n    features.append(x_out)\n    targets.append(y)\n\n    # This is for illustration purposes.\n    # We'll load the pre-extracted features from Cloud Storage, so no need to generate it here.\n    break\n\nfeatures = torch.cat(features, dim=0)\ntargets = torch.cat(targets, dim=0)\n\nfeatures.shape, targets.shape\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"d9b02cdbd72747a6a2306f40f0fb0ef3\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=35}\n```\n(torch.Size([10, 576]), torch.Size([10]))\n```\n:::\n:::\n\n\n---\n\n## Execute the feature extraction with the deep learning model {.scrollable}\n\n- [ ] Perform the same operation for the **validation** set\n\n::: {#75525e1d .cell execution_count=36}\n``` {.python .cell-code}\nval_features = []\nval_targets = []\n\nvalidation_dl = DataLoader(validation_ds, batch_size=batch_size, num_workers=2, worker_init_fn=dataset_worker_init_fn)\n\nfor i, (x, y, _) in tqdm(enumerate(validation_dl)):\n    b, c, h, w = x.shape\n    x_t = model_transforms(torch.tile(x.reshape(-1, 1, h, w), (1, 3, 1, 1)))\n\n    if torch.cuda.is_available():\n        x_t = x_t.cuda()\n    \n    with torch.no_grad():\n        x_out = model(x_t)\n        x_out = x_out.detach().reshape(-1, c, 576, 7, 7).sum(dim=1)\n        x_out = org_avgpool(x_out).detach().reshape(b, -1)\n\n    val_features.append(x_out)\n    val_targets.append(y)\n\n    break\n\nval_features = torch.cat(val_features, dim=0)\nval_targets = torch.cat(val_targets, dim=0)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"21a3d3cbae5f48f8975ac04eded252a0\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n:::\n\n\n---\n\n## Execute the feature extraction with the deep learning model {.scrollable}\n\n- [ ] And again for the testing set\n\n::: {#06af51a2 .cell execution_count=37}\n``` {.python .cell-code}\ntst_features = []\ntst_targets = []\n\ntesting_dl = DataLoader(testing_ds, batch_size=batch_size, num_workers=2, worker_init_fn=dataset_worker_init_fn)\n\nfor i, (x, y, _) in tqdm(enumerate(testing_dl)):\n    b, c, h, w = x.shape\n    x_t = model_transforms(torch.tile(x.reshape(-1, 1, h, w), (1, 3, 1, 1)))\n\n    if torch.cuda.is_available():\n        x_t = x_t.cuda()\n    \n    with torch.no_grad():\n        x_out = model(x_t)\n        x_out = x_out.detach().reshape(-1, c, 576, 7, 7).sum(dim=1)\n        x_out = org_avgpool(x_out).detach().reshape(b, -1)\n\n    tst_features.append(x_out)\n    tst_targets.append(y)\n\n    break\n\ntst_features = torch.cat(tst_features, dim=0)\ntst_targets = torch.cat(tst_targets, dim=0)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"158f7062a8f749c39f8533ba5ca18bc9\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n:::\n\n\n# Train a perturbation classifier model\n\nWe'll take the pre-extracted features and train a perturbation classifier.\n\nThis approach will use a Multilayer Perceptron (**MLP**) model to classify the field-level profiles into three categories: *NONE/DMSO* = 0, *CRISPR* = 1, and *ORF* = 2.\n\nThe model will be fitted using *Adam* optimizer, which objective is to reduce the *Cross Entropy* loss between the predicted and the ground-truth category of each field.\n\n---\n\n## Load the pre-extracted features to train the classifier {.scrollable}\n\n- [ ] Define an `IterableDataset` to load the features from the `trn_feature_XXX.pt` files\n\n::: {#35b15254 .cell execution_count=38}\n``` {.python .cell-code}\nclass GCPStorageDataset(IterableDataset):\n    \"\"\"This dataset loads the features from Cloud Storage\n    \"\"\"\n    def __init__(self, features_url, reducer=None, shuffle=False):\n        super(GCPStorageDataset).__init__()\n\n        self._features_url = features_url\n        self._features_dict = None\n        self._reducer = reducer\n\n        self._shuffle = shuffle\n\n        self._worker_sel = slice(0, len(self._features_url))\n        self._worker_id = 0\n        self._num_workers = 1\n\n    def __iter__(self):\n        # Select the barcodes that correspond to this worker\n        self._features_url = self._features_url[self._worker_sel]\n\n        if self._shuffle:\n            random.shuffle(self._features_url)\n\n        for url in self._features_url:\n            features_dict = torch.load(url)\n            \n            if self._reducer is not None:\n                embeddings = reducer.transform(features_dict[\"features\"])\n\n            curr_n_samples = len(features_dict[\"features\"])\n\n            for index in range(curr_n_samples):\n                if self._shuffle:\n                    index = random.randrange(curr_n_samples)\n\n                feats = features_dict[\"features\"][index]\n                target = features_dict[\"targets\"][index]\n\n                if self._reducer is not None:\n                    reduced_feats = embeddings[index]\n                else:\n                    reduced_feats = None\n\n                yield feats, target, reduced_feats\n```\n:::\n\n\n---\n\n## Inspect the distribution of the feature space\n\n- [ ] Fit a UMap to the training set of features to reduce the $576$ features into only $2$ dimensions for visualization\n\n::: {#332a3d06 .cell execution_count=39}\n``` {.python .cell-code}\nimport umap\n```\n:::\n\n\n::: {#cc16cc5d .cell execution_count=40}\n``` {.python .cell-code}\nreducer = umap.UMAP()\n```\n:::\n\n\n---\n\n## Inspect the distribution of the feature space\n\n- [ ] Load the training features and fit the UMap reducer\n\n::: {#76568337 .cell execution_count=41}\n``` {.python .cell-code}\ntrn_features_ds = GCPStorageDataset([\"trn_features_002.pt\"], shuffle=False)\n\ntrn_features, trn_targets, _ = list(zip(*trn_features_ds))\n\ntrn_targets = torch.tensor(trn_targets)\n\nreducer.fit(trn_features)\n\ntrn_embeddings = reducer.transform(trn_features)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">FileNotFoundError</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-828861056.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> trn_features_ds <span class=\"ansi-blue-fg\">=</span> GCPStorageDataset<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">\"trn_features_002.pt\"</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> shuffle<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>trn_features<span class=\"ansi-blue-fg\">,</span> trn_targets<span class=\"ansi-blue-fg\">,</span> _ <span class=\"ansi-blue-fg\">=</span> list<span class=\"ansi-blue-fg\">(</span>zip<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>trn_features_ds<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-fg ansi-bold\">      5</span> trn_targets <span class=\"ansi-blue-fg\">=</span> torch<span class=\"ansi-blue-fg\">.</span>tensor<span class=\"ansi-blue-fg\">(</span>trn_targets<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/tmp/ipython-input-2874583140.py</span> in <span class=\"ansi-cyan-fg\">__iter__</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">     23</span> \n<span class=\"ansi-green-fg ansi-bold\">     24</span>         <span class=\"ansi-green-fg\">for</span> url <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>_features_url<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 25</span><span class=\"ansi-red-fg\">             </span>features_dict <span class=\"ansi-blue-fg\">=</span> torch<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span>url<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     26</span> \n<span class=\"ansi-green-fg ansi-bold\">     27</span>             <span class=\"ansi-green-fg\">if</span> self<span class=\"ansi-blue-fg\">.</span>_reducer <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/serialization.py</span> in <span class=\"ansi-cyan-fg\">load</span><span class=\"ansi-blue-fg\">(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1482</span>         pickle_load_args<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">\"encoding\"</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">\"utf-8\"</span>\n<span class=\"ansi-green-fg ansi-bold\">   1483</span> \n<span class=\"ansi-green-fg\">-&gt; 1484</span><span class=\"ansi-red-fg\">     </span><span class=\"ansi-green-fg\">with</span> _open_file_like<span class=\"ansi-blue-fg\">(</span>f<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">\"rb\"</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> opened_file<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">   1485</span>         <span class=\"ansi-green-fg\">if</span> _is_zipfile<span class=\"ansi-blue-fg\">(</span>opened_file<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">   1486</span>             <span class=\"ansi-red-fg\"># The zipfile reader is going to advance the current file position.</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/serialization.py</span> in <span class=\"ansi-cyan-fg\">_open_file_like</span><span class=\"ansi-blue-fg\">(name_or_buffer, mode)</span>\n<span class=\"ansi-green-fg ansi-bold\">    757</span> <span class=\"ansi-green-fg\">def</span> _open_file_like<span class=\"ansi-blue-fg\">(</span>name_or_buffer<span class=\"ansi-blue-fg\">:</span> FileLike<span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">:</span> str<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">-&gt;</span> _opener<span class=\"ansi-blue-fg\">[</span>IO<span class=\"ansi-blue-fg\">[</span>bytes<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    758</span>     <span class=\"ansi-green-fg\">if</span> _is_path<span class=\"ansi-blue-fg\">(</span>name_or_buffer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 759</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> _open_file<span class=\"ansi-blue-fg\">(</span>name_or_buffer<span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    760</span>     <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    761</span>         <span class=\"ansi-green-fg\">if</span> <span class=\"ansi-blue-fg\">\"w\"</span> <span class=\"ansi-green-fg\">in</span> mode<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/serialization.py</span> in <span class=\"ansi-cyan-fg\">__init__</span><span class=\"ansi-blue-fg\">(self, name, mode)</span>\n<span class=\"ansi-green-fg ansi-bold\">    738</span> <span class=\"ansi-green-fg\">class</span> _open_file<span class=\"ansi-blue-fg\">(</span>_opener<span class=\"ansi-blue-fg\">[</span>IO<span class=\"ansi-blue-fg\">[</span>bytes<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    739</span>     <span class=\"ansi-green-fg\">def</span> __init__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> name<span class=\"ansi-blue-fg\">:</span> Union<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">,</span> os<span class=\"ansi-blue-fg\">.</span>PathLike<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">:</span> str<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">-&gt;</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 740</span><span class=\"ansi-red-fg\">         </span>super<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>__init__<span class=\"ansi-blue-fg\">(</span>open<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    741</span> \n<span class=\"ansi-green-fg ansi-bold\">    742</span>     <span class=\"ansi-green-fg\">def</span> __exit__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">FileNotFoundError</span>: [Errno 2] No such file or directory: 'trn_features_002.pt'</pre>\n```\n:::\n\n:::\n:::\n\n\n---\n\n## Inspect the distribution of the feature space {.scrollable}\n\n- [ ] Visualize the reduced feature space in two dimensions\n\n::: {#72aa3373 .cell execution_count=42}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nclass_names = [\"NONE/DMSO\", \"CRISPR\", \"ORF\", \"COMPOUND\"]\nclass_markers = [\"*\", \"s\", \"o\", \"^\"]\nclass_colors = [\"black\", \"red\", \"blue\", \"green\"]\nclass_facecolors = [\"black\", \"none\", \"none\", \"none\"]\n\nfor class_idx, class_name in enumerate(class_names):\n    plt.scatter(trn_embeddings[trn_targets == class_idx, 0][::10], trn_embeddings[trn_targets == class_idx, 1][::10], label=class_names[class_idx], marker=class_markers[class_idx], facecolors=class_facecolors[class_idx], edgecolors=class_colors[class_idx])\n\nplt.legend()\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-614479785.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg ansi-bold\">      7</span> \n<span class=\"ansi-green-fg ansi-bold\">      8</span> <span class=\"ansi-green-fg\">for</span> class_idx<span class=\"ansi-blue-fg\">,</span> class_name <span class=\"ansi-green-fg\">in</span> enumerate<span class=\"ansi-blue-fg\">(</span>class_names<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">----&gt; 9</span><span class=\"ansi-red-fg\">     </span>plt<span class=\"ansi-blue-fg\">.</span>scatter<span class=\"ansi-blue-fg\">(</span>trn_embeddings<span class=\"ansi-blue-fg\">[</span>trn_targets <span class=\"ansi-blue-fg\">==</span> class_idx<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-cyan-fg\">10</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> trn_embeddings<span class=\"ansi-blue-fg\">[</span>trn_targets <span class=\"ansi-blue-fg\">==</span> class_idx<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-cyan-fg\">10</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> label<span class=\"ansi-blue-fg\">=</span>class_names<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> marker<span class=\"ansi-blue-fg\">=</span>class_markers<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> facecolors<span class=\"ansi-blue-fg\">=</span>class_facecolors<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> edgecolors<span class=\"ansi-blue-fg\">=</span>class_colors<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     10</span> \n<span class=\"ansi-green-fg ansi-bold\">     11</span> plt<span class=\"ansi-blue-fg\">.</span>legend<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'trn_embeddings' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n---\n\n\n## Inspect the distribution of the feature space {.scrollable}\n\n- [ ] Confirm that the validation set is mapped to a similar space as the training set\n\n::: {#59d5c9c4 .cell execution_count=43}\n``` {.python .cell-code}\nval_features_ds = GCPStorageDataset([\"val_features.pt\"], shuffle=False)\n\nval_features, val_targets, _ = list(zip(*val_features_ds))\n\nval_targets = torch.tensor(val_targets)\n\nval_embedding = reducer.transform(val_features)\n\nval_embedding.shape\n\nfor class_idx, class_name in enumerate(class_names):\n    plt.scatter(val_embedding[val_targets == class_idx, 0][::10], val_embedding[val_targets == class_idx, 1][::10], label=class_names[class_idx], marker=class_markers[class_idx], facecolors=class_facecolors[class_idx], edgecolors=class_colors[class_idx])\n\nplt.legend()\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">FileNotFoundError</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-2608381846.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> val_features_ds <span class=\"ansi-blue-fg\">=</span> GCPStorageDataset<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">\"val_features.pt\"</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> shuffle<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>val_features<span class=\"ansi-blue-fg\">,</span> val_targets<span class=\"ansi-blue-fg\">,</span> _ <span class=\"ansi-blue-fg\">=</span> list<span class=\"ansi-blue-fg\">(</span>zip<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>val_features_ds<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-fg ansi-bold\">      5</span> val_targets <span class=\"ansi-blue-fg\">=</span> torch<span class=\"ansi-blue-fg\">.</span>tensor<span class=\"ansi-blue-fg\">(</span>val_targets<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/tmp/ipython-input-2874583140.py</span> in <span class=\"ansi-cyan-fg\">__iter__</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">     23</span> \n<span class=\"ansi-green-fg ansi-bold\">     24</span>         <span class=\"ansi-green-fg\">for</span> url <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>_features_url<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 25</span><span class=\"ansi-red-fg\">             </span>features_dict <span class=\"ansi-blue-fg\">=</span> torch<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span>url<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     26</span> \n<span class=\"ansi-green-fg ansi-bold\">     27</span>             <span class=\"ansi-green-fg\">if</span> self<span class=\"ansi-blue-fg\">.</span>_reducer <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/serialization.py</span> in <span class=\"ansi-cyan-fg\">load</span><span class=\"ansi-blue-fg\">(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1482</span>         pickle_load_args<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">\"encoding\"</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">\"utf-8\"</span>\n<span class=\"ansi-green-fg ansi-bold\">   1483</span> \n<span class=\"ansi-green-fg\">-&gt; 1484</span><span class=\"ansi-red-fg\">     </span><span class=\"ansi-green-fg\">with</span> _open_file_like<span class=\"ansi-blue-fg\">(</span>f<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">\"rb\"</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> opened_file<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">   1485</span>         <span class=\"ansi-green-fg\">if</span> _is_zipfile<span class=\"ansi-blue-fg\">(</span>opened_file<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">   1486</span>             <span class=\"ansi-red-fg\"># The zipfile reader is going to advance the current file position.</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/serialization.py</span> in <span class=\"ansi-cyan-fg\">_open_file_like</span><span class=\"ansi-blue-fg\">(name_or_buffer, mode)</span>\n<span class=\"ansi-green-fg ansi-bold\">    757</span> <span class=\"ansi-green-fg\">def</span> _open_file_like<span class=\"ansi-blue-fg\">(</span>name_or_buffer<span class=\"ansi-blue-fg\">:</span> FileLike<span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">:</span> str<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">-&gt;</span> _opener<span class=\"ansi-blue-fg\">[</span>IO<span class=\"ansi-blue-fg\">[</span>bytes<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    758</span>     <span class=\"ansi-green-fg\">if</span> _is_path<span class=\"ansi-blue-fg\">(</span>name_or_buffer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 759</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> _open_file<span class=\"ansi-blue-fg\">(</span>name_or_buffer<span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    760</span>     <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    761</span>         <span class=\"ansi-green-fg\">if</span> <span class=\"ansi-blue-fg\">\"w\"</span> <span class=\"ansi-green-fg\">in</span> mode<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/serialization.py</span> in <span class=\"ansi-cyan-fg\">__init__</span><span class=\"ansi-blue-fg\">(self, name, mode)</span>\n<span class=\"ansi-green-fg ansi-bold\">    738</span> <span class=\"ansi-green-fg\">class</span> _open_file<span class=\"ansi-blue-fg\">(</span>_opener<span class=\"ansi-blue-fg\">[</span>IO<span class=\"ansi-blue-fg\">[</span>bytes<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    739</span>     <span class=\"ansi-green-fg\">def</span> __init__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> name<span class=\"ansi-blue-fg\">:</span> Union<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">,</span> os<span class=\"ansi-blue-fg\">.</span>PathLike<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">:</span> str<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">-&gt;</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 740</span><span class=\"ansi-red-fg\">         </span>super<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>__init__<span class=\"ansi-blue-fg\">(</span>open<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    741</span> \n<span class=\"ansi-green-fg ansi-bold\">    742</span>     <span class=\"ansi-green-fg\">def</span> __exit__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">FileNotFoundError</span>: [Errno 2] No such file or directory: 'val_features.pt'</pre>\n```\n:::\n\n:::\n:::\n\n\n---\n\n## Create a classifier with a Multilayer Perceptron (**MLP**) architecture {.scrollable}\n\nThe *MobileNet* model extracts $576$ features per image, these will be the input features for the *MLP* classifer.\n\n- [ ] Define a Multilayer Perceptron *MLP* classifier that takes the pre-extracted features and predicts the perturbation applied to that sample\n\n::: {#d9a17412 .cell execution_count=44}\n``` {.python .cell-code}\nclass PerturbationClassifier(torch.nn.Module):\n    def __init__(self, num_features, num_classes):\n        super(PerturbationClassifier, self).__init__()\n\n        self._reducer = torch.nn.Sequential(\n            torch.nn.BatchNorm1d(num_features=num_features),\n            torch.nn.Linear(in_features=num_features, out_features=2, bias=False),\n        )\n\n        self._classifier = torch.nn.Sequential(\n            torch.nn.Dropout(0.1),\n            torch.nn.ReLU(),\n            torch.nn.Linear(in_features=2, out_features=num_classes, bias=False)\n        )\n\n    def forward(self, input):\n        fx = self._reducer(input)\n        \n        y_pred = self._classifier(fx)\n\n        return y_pred, fx\n```\n:::\n\n\n---\n\n## Create a classifier with a Multilayer Perceptron (**MLP**) architecture {.scrollable}\n\n- [ ] Instantiate the classifier for the set of $576$ features and $3$ classes\n\n::: {#13e3d2bb .cell execution_count=45}\n``` {.python .cell-code}\nclassifier = PerturbationClassifier(576, 3)\n```\n:::\n\n\n- [ ] (Optional) If a GPU is available, move the classifier model to it\n\n::: {#9e43e1fa .cell execution_count=46}\n``` {.python .cell-code}\nif torch.cuda.is_available():\n    classifier.cuda()\n```\n:::\n\n\n---\n\n## Create a classifier with a Multilayer Perceptron (**MLP**) architecture {.scrollable} \n\n- [ ] Define the optimizer algorithm\n\n::: {#c5fe70a0 .cell execution_count=47}\n``` {.python .cell-code}\noptimizer = torch.optim.Adam([\n    {'params': classifier._reducer.parameters(), 'lr': 1e-5, 'weight_decay': 0.001},\n    {'params': classifier._classifier.parameters(), 'lr': 1e-4}\n])\n```\n:::\n\n\n- [ ] Define the loss function used to assess the classification performance of the model\n\n::: {#fef64f1a .cell execution_count=48}\n``` {.python .cell-code}\nclassifier_loss_fn = torch.nn.CrossEntropyLoss()\nreducer_loss_fn = torch.nn.MSELoss()\n```\n:::\n\n\n---\n\n## Train the *MLP* model {.scrollable}\n\n- [ ] Instantiate the datasets that will load the features from Cloud Storage\n\n::: {#b478dc32 .cell execution_count=49}\n``` {.python .cell-code}\ntrn_feat_ds = GCPStorageDataset([f\"trn_features_{i:03d}.pt\" for i in range(10)], reducer=reducer, shuffle=True)\nval_feat_ds = GCPStorageDataset([\"val_features.pt\"], reducer=reducer, shuffle=False)\ntst_feat_ds = GCPStorageDataset([\"tst_features.pt\"], reducer=reducer, shuffle=False)\n```\n:::\n\n\n- [ ] Create one `DataLoader` for the training set and one for the validation set of pre-extracted features\n\n::: {#3cae185c .cell execution_count=50}\n``` {.python .cell-code}\ntrn_feat_dl = DataLoader(trn_feat_ds, batch_size=100, num_workers=2, worker_init_fn=dataset_worker_init_fn)\nval_feat_dl = DataLoader(val_feat_ds, batch_size=100)\ntst_feat_dl = DataLoader(tst_feat_ds, batch_size=100)\n```\n:::\n\n\n---\n\n## Train the *MLP* model {.scrollable}\n\n- [ ] Implement the training loop\n\n::: {#37f1d6cb .cell execution_count=51}\n``` {.python .cell-code}\nfrom torchmetrics.classification import Accuracy\n\nn_dmso = 0\nn_crispr = 0\nn_orf = 0\n\n# Training loop\nclassifier.train()\n\ncls_loss_epoch = 0\nred_loss_epoch = 0\n\ntrn_acc_metric = Accuracy(task=\"multiclass\", num_classes=3)\n\nfor x, y, fx in tqdm(trn_feat_dl, total=1000):\n    optimizer.zero_grad()\n\n    if torch.cuda.is_available():\n        x = x.cuda()\n\n    y_pred, fx_pred = classifier(x)\n\n    cls_loss = classifier_loss_fn(y_pred.cpu(), y)\n    red_loss = reducer_loss_fn(fx_pred.cpu(), fx)\n\n    cls_loss.backward(retain_graph=True)\n    red_loss.backward()\n\n    optimizer.step()\n\n    cls_loss_epoch += cls_loss.item()\n    red_loss_epoch += red_loss.item()\n\n    trn_acc_metric(y_pred.cpu().softmax(dim=1), y)\n\n    n_dmso += sum(y == 0)\n    n_crispr += sum(y == 1)\n    n_orf += sum(y == 2)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"6d3e635ee37d4c1ebde4339c905bf729\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">FileNotFoundError</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-4236613521.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg ansi-bold\">     13</span> trn_acc_metric <span class=\"ansi-blue-fg\">=</span> Accuracy<span class=\"ansi-blue-fg\">(</span>task<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">\"multiclass\"</span><span class=\"ansi-blue-fg\">,</span> num_classes<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     14</span> \n<span class=\"ansi-green-fg\">---&gt; 15</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">for</span> x<span class=\"ansi-blue-fg\">,</span> y<span class=\"ansi-blue-fg\">,</span> fx <span class=\"ansi-green-fg\">in</span> tqdm<span class=\"ansi-blue-fg\">(</span>trn_feat_dl<span class=\"ansi-blue-fg\">,</span> total<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">1000</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">     16</span>     optimizer<span class=\"ansi-blue-fg\">.</span>zero_grad<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     17</span> \n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/tqdm/notebook.py</span> in <span class=\"ansi-cyan-fg\">__iter__</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">    248</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    249</span>             it <span class=\"ansi-blue-fg\">=</span> super<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>__iter__<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 250</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">for</span> obj <span class=\"ansi-green-fg\">in</span> it<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    251</span>                 <span class=\"ansi-red-fg\"># return super(tqdm...) will not catch exception</span>\n<span class=\"ansi-green-fg ansi-bold\">    252</span>                 <span class=\"ansi-green-fg\">yield</span> obj\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/tqdm/std.py</span> in <span class=\"ansi-cyan-fg\">__iter__</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1179</span> \n<span class=\"ansi-green-fg ansi-bold\">   1180</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1181</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">for</span> obj <span class=\"ansi-green-fg\">in</span> iterable<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">   1182</span>                 <span class=\"ansi-green-fg\">yield</span> obj\n<span class=\"ansi-green-fg ansi-bold\">   1183</span>                 <span class=\"ansi-red-fg\"># Update and possibly print the progressbar.</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py</span> in <span class=\"ansi-cyan-fg\">__next__</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">    732</span>                 <span class=\"ansi-red-fg\"># TODO(https://github.com/pytorch/pytorch/issues/76750)</span>\n<span class=\"ansi-green-fg ansi-bold\">    733</span>                 self<span class=\"ansi-blue-fg\">.</span>_reset<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>  <span class=\"ansi-red-fg\"># type: ignore[call-arg]</span>\n<span class=\"ansi-green-fg\">--&gt; 734</span><span class=\"ansi-red-fg\">             </span>data <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_next_data<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    735</span>             self<span class=\"ansi-blue-fg\">.</span>_num_yielded <span class=\"ansi-blue-fg\">+=</span> <span class=\"ansi-cyan-fg\">1</span>\n<span class=\"ansi-green-fg ansi-bold\">    736</span>             if (\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py</span> in <span class=\"ansi-cyan-fg\">_next_data</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1514</span>                 worker_id <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_task_info<span class=\"ansi-blue-fg\">.</span>pop<span class=\"ansi-blue-fg\">(</span>idx<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-fg ansi-bold\">   1515</span>                 self<span class=\"ansi-blue-fg\">.</span>_rcvd_idx <span class=\"ansi-blue-fg\">+=</span> <span class=\"ansi-cyan-fg\">1</span>\n<span class=\"ansi-green-fg\">-&gt; 1516</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_process_data<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> worker_id<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1517</span> \n<span class=\"ansi-green-fg ansi-bold\">   1518</span>     <span class=\"ansi-green-fg\">def</span> _try_put_index<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py</span> in <span class=\"ansi-cyan-fg\">_process_data</span><span class=\"ansi-blue-fg\">(self, data, worker_idx)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1549</span>         self<span class=\"ansi-blue-fg\">.</span>_try_put_index<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1550</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> ExceptionWrapper<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1551</span><span class=\"ansi-red-fg\">             </span>data<span class=\"ansi-blue-fg\">.</span>reraise<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1552</span>         <span class=\"ansi-green-fg\">return</span> data\n<span class=\"ansi-green-fg ansi-bold\">   1553</span> \n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/_utils.py</span> in <span class=\"ansi-cyan-fg\">reraise</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">    767</span>             <span class=\"ansi-red-fg\"># be constructed, don't try to instantiate since we don't know how to</span>\n<span class=\"ansi-green-fg ansi-bold\">    768</span>             <span class=\"ansi-green-fg\">raise</span> RuntimeError<span class=\"ansi-blue-fg\">(</span>msg<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-fg\">--&gt; 769</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">raise</span> exception\n<span class=\"ansi-green-fg ansi-bold\">    770</span> \n<span class=\"ansi-green-fg ansi-bold\">    771</span> \n\n<span class=\"ansi-red-fg\">FileNotFoundError</span>: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 33, in fetch\n    data.append(next(self.dataset_iter))\n                ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipython-input-2874583140.py\", line 25, in __iter__\n    features_dict = torch.load(url)\n                    ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1484, in load\n    with _open_file_like(f, \"rb\") as opened_file:\n         ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 759, in _open_file_like\n    return _open_file(name_or_buffer, mode)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 740, in __init__\n    super().__init__(open(name, mode))\n                     ^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: 'trn_features_006.pt'\n</pre>\n```\n:::\n\n:::\n:::\n\n\n---\n\n## Train the *MLP* model {.scrollable}\n\n- [ ] Compute the proportion of samples per category in the training set\n\n::: {#5d08ee63 .cell execution_count=52}\n``` {.python .cell-code}\nn_total = n_dmso + n_crispr + n_orf\nn_dmso / n_total, n_crispr / n_total, n_orf / n_total\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ZeroDivisionError</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-351853166.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> n_total <span class=\"ansi-blue-fg\">=</span> n_dmso <span class=\"ansi-blue-fg\">+</span> n_crispr <span class=\"ansi-blue-fg\">+</span> n_orf\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>n_dmso <span class=\"ansi-blue-fg\">/</span> n_total<span class=\"ansi-blue-fg\">,</span> n_crispr <span class=\"ansi-blue-fg\">/</span> n_total<span class=\"ansi-blue-fg\">,</span> n_orf <span class=\"ansi-blue-fg\">/</span> n_total\n\n<span class=\"ansi-red-fg\">ZeroDivisionError</span>: division by zero</pre>\n```\n:::\n\n:::\n:::\n\n\n- [ ] Check the average loss metric of the model on the training set\n\n::: {#91b6bc37 .cell execution_count=53}\n``` {.python .cell-code}\ncls_loss_epoch / n_total\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ZeroDivisionError</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-2159926283.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>cls_loss_epoch <span class=\"ansi-blue-fg\">/</span> n_total\n\n<span class=\"ansi-red-fg\">ZeroDivisionError</span>: division by zero</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#4525fcaf .cell execution_count=54}\n``` {.python .cell-code}\nred_loss_epoch / n_total\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ZeroDivisionError</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-1461694865.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>red_loss_epoch <span class=\"ansi-blue-fg\">/</span> n_total\n\n<span class=\"ansi-red-fg\">ZeroDivisionError</span>: division by zero</pre>\n```\n:::\n\n:::\n:::\n\n\n- [ ] Compute the average accuracy of the model on the training set\n\n::: {#22e49043 .cell execution_count=55}\n``` {.python .cell-code}\ntrn_acc_metric.compute()\n```\n\n::: {.cell-output .cell-output-display execution_count=55}\n```\ntensor(0.)\n```\n:::\n:::\n\n\n---\n\n## Train the *MLP* model {.scrollable}\n\n- [ ] Implement the validation loop\n\n::: {#a6320e71 .cell execution_count=56}\n``` {.python .cell-code}\nn_dmso = 0\nn_crispr = 0\nn_orf = 0\n\ncls_loss_epoch = 0\nred_loss_epoch = 0\n\nval_acc_metric = Accuracy(task=\"multiclass\", num_classes=3)\n\nfor x_val, y_val, fx_val in tqdm(val_feat_dl):\n    with torch.no_grad():\n        if torch.cuda.is_available():\n            x_val = x_val.cuda()\n\n        y_val_pred, fx_val_pred = classifier(x_val)\n\n        cls_loss = classifier_loss_fn(y_val_pred.cpu(), y_val)\n        red_loss = reducer_loss_fn(fx_val_pred.cpu(), fx_val)\n\n    cls_loss_epoch += cls_loss.item()\n    red_loss_epoch += red_loss.item()\n    \n    val_acc_metric(y_val_pred.cpu().softmax(dim=1), y_val)\n\n    n_dmso += sum(y_val == 0)\n    n_crispr += sum(y_val == 1)\n    n_orf += sum(y_val == 2)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"0b3bc5a67d3646ebb0d9097f6fb9d5e3\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">FileNotFoundError</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-729096102.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg ansi-bold\">      8</span> val_acc_metric <span class=\"ansi-blue-fg\">=</span> Accuracy<span class=\"ansi-blue-fg\">(</span>task<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">\"multiclass\"</span><span class=\"ansi-blue-fg\">,</span> num_classes<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">      9</span> \n<span class=\"ansi-green-fg\">---&gt; 10</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">for</span> x_val<span class=\"ansi-blue-fg\">,</span> y_val<span class=\"ansi-blue-fg\">,</span> fx_val <span class=\"ansi-green-fg\">in</span> tqdm<span class=\"ansi-blue-fg\">(</span>val_feat_dl<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">     11</span>     <span class=\"ansi-green-fg\">with</span> torch<span class=\"ansi-blue-fg\">.</span>no_grad<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">     12</span>         <span class=\"ansi-green-fg\">if</span> torch<span class=\"ansi-blue-fg\">.</span>cuda<span class=\"ansi-blue-fg\">.</span>is_available<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/tqdm/notebook.py</span> in <span class=\"ansi-cyan-fg\">__iter__</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">    248</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    249</span>             it <span class=\"ansi-blue-fg\">=</span> super<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>__iter__<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 250</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">for</span> obj <span class=\"ansi-green-fg\">in</span> it<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    251</span>                 <span class=\"ansi-red-fg\"># return super(tqdm...) will not catch exception</span>\n<span class=\"ansi-green-fg ansi-bold\">    252</span>                 <span class=\"ansi-green-fg\">yield</span> obj\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/tqdm/std.py</span> in <span class=\"ansi-cyan-fg\">__iter__</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1179</span> \n<span class=\"ansi-green-fg ansi-bold\">   1180</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1181</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">for</span> obj <span class=\"ansi-green-fg\">in</span> iterable<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">   1182</span>                 <span class=\"ansi-green-fg\">yield</span> obj\n<span class=\"ansi-green-fg ansi-bold\">   1183</span>                 <span class=\"ansi-red-fg\"># Update and possibly print the progressbar.</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py</span> in <span class=\"ansi-cyan-fg\">__next__</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">    732</span>                 <span class=\"ansi-red-fg\"># TODO(https://github.com/pytorch/pytorch/issues/76750)</span>\n<span class=\"ansi-green-fg ansi-bold\">    733</span>                 self<span class=\"ansi-blue-fg\">.</span>_reset<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>  <span class=\"ansi-red-fg\"># type: ignore[call-arg]</span>\n<span class=\"ansi-green-fg\">--&gt; 734</span><span class=\"ansi-red-fg\">             </span>data <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_next_data<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    735</span>             self<span class=\"ansi-blue-fg\">.</span>_num_yielded <span class=\"ansi-blue-fg\">+=</span> <span class=\"ansi-cyan-fg\">1</span>\n<span class=\"ansi-green-fg ansi-bold\">    736</span>             if (\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py</span> in <span class=\"ansi-cyan-fg\">_next_data</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">    788</span>     <span class=\"ansi-green-fg\">def</span> _next_data<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    789</span>         index <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_next_index<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>  <span class=\"ansi-red-fg\"># may raise StopIteration</span>\n<span class=\"ansi-green-fg\">--&gt; 790</span><span class=\"ansi-red-fg\">         </span>data <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_dataset_fetcher<span class=\"ansi-blue-fg\">.</span>fetch<span class=\"ansi-blue-fg\">(</span>index<span class=\"ansi-blue-fg\">)</span>  <span class=\"ansi-red-fg\"># may raise StopIteration</span>\n<span class=\"ansi-green-fg ansi-bold\">    791</span>         <span class=\"ansi-green-fg\">if</span> self<span class=\"ansi-blue-fg\">.</span>_pin_memory<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    792</span>             data <span class=\"ansi-blue-fg\">=</span> _utils<span class=\"ansi-blue-fg\">.</span>pin_memory<span class=\"ansi-blue-fg\">.</span>pin_memory<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_pin_memory_device<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py</span> in <span class=\"ansi-cyan-fg\">fetch</span><span class=\"ansi-blue-fg\">(self, possibly_batched_index)</span>\n<span class=\"ansi-green-fg ansi-bold\">     31</span>             <span class=\"ansi-green-fg\">for</span> _ <span class=\"ansi-green-fg\">in</span> possibly_batched_index<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">     32</span>                 <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 33</span><span class=\"ansi-red-fg\">                     </span>data<span class=\"ansi-blue-fg\">.</span>append<span class=\"ansi-blue-fg\">(</span>next<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>dataset_iter<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     34</span>                 <span class=\"ansi-green-fg\">except</span> StopIteration<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">     35</span>                     self<span class=\"ansi-blue-fg\">.</span>ended <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">True</span>\n\n<span class=\"ansi-green-fg\">/tmp/ipython-input-2874583140.py</span> in <span class=\"ansi-cyan-fg\">__iter__</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">     23</span> \n<span class=\"ansi-green-fg ansi-bold\">     24</span>         <span class=\"ansi-green-fg\">for</span> url <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>_features_url<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 25</span><span class=\"ansi-red-fg\">             </span>features_dict <span class=\"ansi-blue-fg\">=</span> torch<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span>url<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     26</span> \n<span class=\"ansi-green-fg ansi-bold\">     27</span>             <span class=\"ansi-green-fg\">if</span> self<span class=\"ansi-blue-fg\">.</span>_reducer <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/serialization.py</span> in <span class=\"ansi-cyan-fg\">load</span><span class=\"ansi-blue-fg\">(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1482</span>         pickle_load_args<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">\"encoding\"</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">\"utf-8\"</span>\n<span class=\"ansi-green-fg ansi-bold\">   1483</span> \n<span class=\"ansi-green-fg\">-&gt; 1484</span><span class=\"ansi-red-fg\">     </span><span class=\"ansi-green-fg\">with</span> _open_file_like<span class=\"ansi-blue-fg\">(</span>f<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">\"rb\"</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> opened_file<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">   1485</span>         <span class=\"ansi-green-fg\">if</span> _is_zipfile<span class=\"ansi-blue-fg\">(</span>opened_file<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">   1486</span>             <span class=\"ansi-red-fg\"># The zipfile reader is going to advance the current file position.</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/serialization.py</span> in <span class=\"ansi-cyan-fg\">_open_file_like</span><span class=\"ansi-blue-fg\">(name_or_buffer, mode)</span>\n<span class=\"ansi-green-fg ansi-bold\">    757</span> <span class=\"ansi-green-fg\">def</span> _open_file_like<span class=\"ansi-blue-fg\">(</span>name_or_buffer<span class=\"ansi-blue-fg\">:</span> FileLike<span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">:</span> str<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">-&gt;</span> _opener<span class=\"ansi-blue-fg\">[</span>IO<span class=\"ansi-blue-fg\">[</span>bytes<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    758</span>     <span class=\"ansi-green-fg\">if</span> _is_path<span class=\"ansi-blue-fg\">(</span>name_or_buffer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 759</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> _open_file<span class=\"ansi-blue-fg\">(</span>name_or_buffer<span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    760</span>     <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    761</span>         <span class=\"ansi-green-fg\">if</span> <span class=\"ansi-blue-fg\">\"w\"</span> <span class=\"ansi-green-fg\">in</span> mode<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/serialization.py</span> in <span class=\"ansi-cyan-fg\">__init__</span><span class=\"ansi-blue-fg\">(self, name, mode)</span>\n<span class=\"ansi-green-fg ansi-bold\">    738</span> <span class=\"ansi-green-fg\">class</span> _open_file<span class=\"ansi-blue-fg\">(</span>_opener<span class=\"ansi-blue-fg\">[</span>IO<span class=\"ansi-blue-fg\">[</span>bytes<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    739</span>     <span class=\"ansi-green-fg\">def</span> __init__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> name<span class=\"ansi-blue-fg\">:</span> Union<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">,</span> os<span class=\"ansi-blue-fg\">.</span>PathLike<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">:</span> str<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">-&gt;</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 740</span><span class=\"ansi-red-fg\">         </span>super<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>__init__<span class=\"ansi-blue-fg\">(</span>open<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    741</span> \n<span class=\"ansi-green-fg ansi-bold\">    742</span>     <span class=\"ansi-green-fg\">def</span> __exit__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">FileNotFoundError</span>: [Errno 2] No such file or directory: 'val_features.pt'</pre>\n```\n:::\n\n:::\n:::\n\n\n---\n\n## Train the *MLP* model {.scrollable}\n\n- [ ] Compute the proportion of samples per category in the validation set\n\n::: {#319f5a97 .cell execution_count=57}\n``` {.python .cell-code}\nn_total = n_dmso + n_crispr + n_orf\nn_dmso / n_total, n_crispr / n_total, n_orf / n_total\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ZeroDivisionError</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-351853166.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> n_total <span class=\"ansi-blue-fg\">=</span> n_dmso <span class=\"ansi-blue-fg\">+</span> n_crispr <span class=\"ansi-blue-fg\">+</span> n_orf\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>n_dmso <span class=\"ansi-blue-fg\">/</span> n_total<span class=\"ansi-blue-fg\">,</span> n_crispr <span class=\"ansi-blue-fg\">/</span> n_total<span class=\"ansi-blue-fg\">,</span> n_orf <span class=\"ansi-blue-fg\">/</span> n_total\n\n<span class=\"ansi-red-fg\">ZeroDivisionError</span>: division by zero</pre>\n```\n:::\n\n:::\n:::\n\n\n- [ ] Check the average loss metric of the model on the validation set\n\n::: {#53dbded1 .cell execution_count=58}\n``` {.python .cell-code}\ncls_loss_epoch / n_total\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ZeroDivisionError</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-2159926283.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>cls_loss_epoch <span class=\"ansi-blue-fg\">/</span> n_total\n\n<span class=\"ansi-red-fg\">ZeroDivisionError</span>: division by zero</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#49b206e6 .cell execution_count=59}\n``` {.python .cell-code}\nred_loss_epoch / n_total\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ZeroDivisionError</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-1461694865.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>red_loss_epoch <span class=\"ansi-blue-fg\">/</span> n_total\n\n<span class=\"ansi-red-fg\">ZeroDivisionError</span>: division by zero</pre>\n```\n:::\n\n:::\n:::\n\n\n- [ ] Compute the average accuracy of the model on the validation set\n\n::: {#b0498079 .cell execution_count=60}\n``` {.python .cell-code}\nval_acc_metric.compute()\n```\n\n::: {.cell-output .cell-output-display execution_count=60}\n```\ntensor(0.)\n```\n:::\n:::\n\n\n---\n\n## Wrap the training and validation steps for multiple epochs {.scrollable}\n\nTrack the performance of the model throughout the epochs during training\n\n- [ ] Copy the training and validation steps into an epochs loop\n\n::: {#17783747 .cell execution_count=61}\n``` {.python .cell-code}\navg_cls_loss_trn = []\navg_red_loss_trn = []\navg_acc_trn = []\n\navg_cls_loss_val = []\navg_red_loss_val = []\navg_acc_val = []\n\nn_epochs = 20\nq = tqdm(total=n_epochs)\n\nfor e in range(n_epochs):\n    # Training loop\n    classifier.train()\n\n    loss_epoch = 0\n\n    trn_acc_metric.reset()\n    \n    total_samples = 0\n    for x, y, fx in trn_feat_dl:\n        optimizer.zero_grad()\n\n        if torch.cuda.is_available():\n            x = x.cuda()\n\n        y_pred, fx_pred = classifier(x)\n\n        cls_loss = classifier_loss_fn(y_pred.cpu(), y)\n        red_loss = reducer_loss_fn(fx_pred.cpu(), fx)\n\n        cls_loss.backward(retain_graph=True)\n        red_loss.backward()\n\n        optimizer.step()\n\n        cls_loss_epoch += cls_loss.item() * len(y)\n        red_loss_epoch += red_loss.item() * len(y)\n\n        trn_acc_metric(y_pred.cpu().softmax(dim=1), y)\n        total_samples += len(y)\n\n    avg_cls_loss_trn.append(cls_loss_epoch / total_samples)\n    avg_red_loss_trn.append(red_loss_epoch / total_samples)\n\n    avg_acc_trn.append(trn_acc_metric.compute())\n\n    # Validation loop\n    classifier.eval()\n\n    cls_loss_epoch = 0\n    red_loss_epoch = 0\n\n    val_acc_metric.reset()\n\n    total_samples = 0\n    for x_val, y_val, fx_val in val_feat_dl:\n        with torch.no_grad():\n            if torch.cuda.is_available():\n                x_val = x_val.cuda()\n\n            y_val_pred, fx_val_pred = classifier(x_val)\n\n            cls_loss = classifier_loss_fn(y_val_pred.cpu(), y_val)\n            red_loss = reducer_loss_fn(fx_val_pred.cpu(), fx_val)\n\n        cls_loss_epoch += cls_loss.item() * len(y_val)\n        red_loss_epoch += red_loss.item() * len(y_val)\n\n        val_acc_metric(y_val_pred.cpu().softmax(dim=1), y_val)\n        total_samples += len(y_val)\n\n    avg_cls_loss_val.append(cls_loss_epoch / total_samples)\n    avg_red_loss_val.append(red_loss_epoch / total_samples)\n\n    avg_acc_val.append(val_acc_metric.compute())\n\n    q.set_description(f\"Average training CE loss: {avg_cls_loss_trn[-1]:0.4f} / MSE loss: {avg_red_loss_trn[-1]:0.4f} (Accuracy: {100 * avg_acc_trn[-1]:0.2f} %). Average validation CE loss: {avg_cls_loss_val[-1]:04f} / MSE loss: {avg_red_loss_val[-1]:04f} (Accuracy: {100 * avg_acc_val[-1]:0.2f} %)\")\n    q.update()\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"7759eed263f94ea8bae0146112b94493\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">FileNotFoundError</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-4068602417.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg ansi-bold\">     19</span> \n<span class=\"ansi-green-fg ansi-bold\">     20</span>     total_samples <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-cyan-fg\">0</span>\n<span class=\"ansi-green-fg\">---&gt; 21</span><span class=\"ansi-red-fg\">     </span><span class=\"ansi-green-fg\">for</span> x<span class=\"ansi-blue-fg\">,</span> y<span class=\"ansi-blue-fg\">,</span> fx <span class=\"ansi-green-fg\">in</span> trn_feat_dl<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">     22</span>         optimizer<span class=\"ansi-blue-fg\">.</span>zero_grad<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     23</span> \n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py</span> in <span class=\"ansi-cyan-fg\">__next__</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">    732</span>                 <span class=\"ansi-red-fg\"># TODO(https://github.com/pytorch/pytorch/issues/76750)</span>\n<span class=\"ansi-green-fg ansi-bold\">    733</span>                 self<span class=\"ansi-blue-fg\">.</span>_reset<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>  <span class=\"ansi-red-fg\"># type: ignore[call-arg]</span>\n<span class=\"ansi-green-fg\">--&gt; 734</span><span class=\"ansi-red-fg\">             </span>data <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_next_data<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    735</span>             self<span class=\"ansi-blue-fg\">.</span>_num_yielded <span class=\"ansi-blue-fg\">+=</span> <span class=\"ansi-cyan-fg\">1</span>\n<span class=\"ansi-green-fg ansi-bold\">    736</span>             if (\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py</span> in <span class=\"ansi-cyan-fg\">_next_data</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1514</span>                 worker_id <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_task_info<span class=\"ansi-blue-fg\">.</span>pop<span class=\"ansi-blue-fg\">(</span>idx<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-fg ansi-bold\">   1515</span>                 self<span class=\"ansi-blue-fg\">.</span>_rcvd_idx <span class=\"ansi-blue-fg\">+=</span> <span class=\"ansi-cyan-fg\">1</span>\n<span class=\"ansi-green-fg\">-&gt; 1516</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_process_data<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> worker_id<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1517</span> \n<span class=\"ansi-green-fg ansi-bold\">   1518</span>     <span class=\"ansi-green-fg\">def</span> _try_put_index<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py</span> in <span class=\"ansi-cyan-fg\">_process_data</span><span class=\"ansi-blue-fg\">(self, data, worker_idx)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1549</span>         self<span class=\"ansi-blue-fg\">.</span>_try_put_index<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1550</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> ExceptionWrapper<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1551</span><span class=\"ansi-red-fg\">             </span>data<span class=\"ansi-blue-fg\">.</span>reraise<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1552</span>         <span class=\"ansi-green-fg\">return</span> data\n<span class=\"ansi-green-fg ansi-bold\">   1553</span> \n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/_utils.py</span> in <span class=\"ansi-cyan-fg\">reraise</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">    767</span>             <span class=\"ansi-red-fg\"># be constructed, don't try to instantiate since we don't know how to</span>\n<span class=\"ansi-green-fg ansi-bold\">    768</span>             <span class=\"ansi-green-fg\">raise</span> RuntimeError<span class=\"ansi-blue-fg\">(</span>msg<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-fg\">--&gt; 769</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">raise</span> exception\n<span class=\"ansi-green-fg ansi-bold\">    770</span> \n<span class=\"ansi-green-fg ansi-bold\">    771</span> \n\n<span class=\"ansi-red-fg\">FileNotFoundError</span>: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 33, in fetch\n    data.append(next(self.dataset_iter))\n                ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipython-input-2874583140.py\", line 25, in __iter__\n    features_dict = torch.load(url)\n                    ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1484, in load\n    with _open_file_like(f, \"rb\") as opened_file:\n         ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 759, in _open_file_like\n    return _open_file(name_or_buffer, mode)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 740, in __init__\n    super().__init__(open(name, mode))\n                     ^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: 'trn_features_002.pt'\n</pre>\n```\n:::\n\n:::\n:::\n\n\n---\n\n## Review the performance of the model throughout training {.scrollable}\n\n- [ ] Plot the loss function evaluation for the training and validation sets\n\n::: {#8c664f36 .cell execution_count=62}\n``` {.python .cell-code}\nplt.plot(avg_cls_loss_trn, \"k-\", label=\"Training loss\")\nplt.plot(avg_cls_loss_val, \"b:\", label=\"Validation loss\")\nplt.legend()\n```\n\n::: {.cell-output .cell-output-display}\n![](DL_image_analysis_2_3_files/figure-revealjs/cell-63-output-1.png){width=813 height=411}\n:::\n:::\n\n\n::: {#aa214409 .cell execution_count=63}\n``` {.python .cell-code}\nplt.plot(avg_red_loss_trn, \"k-\", label=\"Training loss\")\nplt.plot(avg_red_loss_val, \"b:\", label=\"Validation loss\")\nplt.legend()\n```\n\n::: {.cell-output .cell-output-display}\n![](DL_image_analysis_2_3_files/figure-revealjs/cell-64-output-1.png){width=813 height=411}\n:::\n:::\n\n\n---\n\n## Review the performance of the model throughout training {.scrollable}\n\n- [ ] Plot the accuracy of the model on the training and validation sets\n\n::: {#0c82df57 .cell execution_count=64}\n``` {.python .cell-code}\nplt.plot(avg_acc_trn, \"k-\", label=\"Training accuracy\")\nplt.plot(avg_acc_val, \"b:\", label=\"Validation accuracy\")\nplt.legend()\n```\n\n::: {.cell-output .cell-output-display}\n![](DL_image_analysis_2_3_files/figure-revealjs/cell-65-output-1.png){width=813 height=411}\n:::\n:::\n\n\n# Evaluate the model with the witheld testing data\n\n## Save the classifier model to be used later or shared with collaborators {.scrollable}\n\n- [ ] Evaluate the classifier on the testing set to measure its generalization capacity\n\n::: {#34b75efd .cell execution_count=65}\n``` {.python .cell-code}\nfrom torchmetrics.classification import ConfusionMatrix\n\nclassifier.eval()\n\nn_dmso = 0\nn_crispr = 0\nn_orf = 0\n\ncls_loss_epoch = 0\nred_loss_epoch = 0\n\ntst_acc_metric = Accuracy(\"multiclass\", num_classes=3)\ntst_confmat = ConfusionMatrix(\"multiclass\", num_classes=3)\n\nfor x_tst, y_tst, fx_tst in tst_feat_dl:\n    with torch.no_grad():\n        if torch.cuda.is_available():\n            x_tst = x_tst.cuda()\n\n        y_tst_pred, fx_tst_pred = classifier(x_tst)\n        cls_loss = classifier_loss_fn(y_tst_pred.cpu(), y_tst)\n        red_loss = reducer_loss_fn(fx_tst_pred.cpu(), fx_tst)\n\n    cls_loss_epoch += cls_loss.item() * len(y_tst)\n    red_loss_epoch += red_loss.item() * len(y_tst)\n    \n    y_tst_prob = y_tst_pred.cpu().softmax(dim=1)\n    tst_acc_metric.update(y_tst_prob, y_tst)\n    tst_confmat.update(y_tst_prob, y_tst)\n\n    n_dmso += sum(y_tst == 0)\n    n_crispr += sum(y_tst == 1)\n    n_orf += sum(y_tst == 2)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">FileNotFoundError</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-3055367780.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg ansi-bold\">     13</span> tst_confmat <span class=\"ansi-blue-fg\">=</span> ConfusionMatrix<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">\"multiclass\"</span><span class=\"ansi-blue-fg\">,</span> num_classes<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     14</span> \n<span class=\"ansi-green-fg\">---&gt; 15</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">for</span> x_tst<span class=\"ansi-blue-fg\">,</span> y_tst<span class=\"ansi-blue-fg\">,</span> fx_tst <span class=\"ansi-green-fg\">in</span> tst_feat_dl<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">     16</span>     <span class=\"ansi-green-fg\">with</span> torch<span class=\"ansi-blue-fg\">.</span>no_grad<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">     17</span>         <span class=\"ansi-green-fg\">if</span> torch<span class=\"ansi-blue-fg\">.</span>cuda<span class=\"ansi-blue-fg\">.</span>is_available<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py</span> in <span class=\"ansi-cyan-fg\">__next__</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">    732</span>                 <span class=\"ansi-red-fg\"># TODO(https://github.com/pytorch/pytorch/issues/76750)</span>\n<span class=\"ansi-green-fg ansi-bold\">    733</span>                 self<span class=\"ansi-blue-fg\">.</span>_reset<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>  <span class=\"ansi-red-fg\"># type: ignore[call-arg]</span>\n<span class=\"ansi-green-fg\">--&gt; 734</span><span class=\"ansi-red-fg\">             </span>data <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_next_data<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    735</span>             self<span class=\"ansi-blue-fg\">.</span>_num_yielded <span class=\"ansi-blue-fg\">+=</span> <span class=\"ansi-cyan-fg\">1</span>\n<span class=\"ansi-green-fg ansi-bold\">    736</span>             if (\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py</span> in <span class=\"ansi-cyan-fg\">_next_data</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">    788</span>     <span class=\"ansi-green-fg\">def</span> _next_data<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    789</span>         index <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_next_index<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>  <span class=\"ansi-red-fg\"># may raise StopIteration</span>\n<span class=\"ansi-green-fg\">--&gt; 790</span><span class=\"ansi-red-fg\">         </span>data <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_dataset_fetcher<span class=\"ansi-blue-fg\">.</span>fetch<span class=\"ansi-blue-fg\">(</span>index<span class=\"ansi-blue-fg\">)</span>  <span class=\"ansi-red-fg\"># may raise StopIteration</span>\n<span class=\"ansi-green-fg ansi-bold\">    791</span>         <span class=\"ansi-green-fg\">if</span> self<span class=\"ansi-blue-fg\">.</span>_pin_memory<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    792</span>             data <span class=\"ansi-blue-fg\">=</span> _utils<span class=\"ansi-blue-fg\">.</span>pin_memory<span class=\"ansi-blue-fg\">.</span>pin_memory<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_pin_memory_device<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py</span> in <span class=\"ansi-cyan-fg\">fetch</span><span class=\"ansi-blue-fg\">(self, possibly_batched_index)</span>\n<span class=\"ansi-green-fg ansi-bold\">     31</span>             <span class=\"ansi-green-fg\">for</span> _ <span class=\"ansi-green-fg\">in</span> possibly_batched_index<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">     32</span>                 <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 33</span><span class=\"ansi-red-fg\">                     </span>data<span class=\"ansi-blue-fg\">.</span>append<span class=\"ansi-blue-fg\">(</span>next<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>dataset_iter<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     34</span>                 <span class=\"ansi-green-fg\">except</span> StopIteration<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">     35</span>                     self<span class=\"ansi-blue-fg\">.</span>ended <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">True</span>\n\n<span class=\"ansi-green-fg\">/tmp/ipython-input-2874583140.py</span> in <span class=\"ansi-cyan-fg\">__iter__</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">     23</span> \n<span class=\"ansi-green-fg ansi-bold\">     24</span>         <span class=\"ansi-green-fg\">for</span> url <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>_features_url<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 25</span><span class=\"ansi-red-fg\">             </span>features_dict <span class=\"ansi-blue-fg\">=</span> torch<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span>url<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     26</span> \n<span class=\"ansi-green-fg ansi-bold\">     27</span>             <span class=\"ansi-green-fg\">if</span> self<span class=\"ansi-blue-fg\">.</span>_reducer <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/serialization.py</span> in <span class=\"ansi-cyan-fg\">load</span><span class=\"ansi-blue-fg\">(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1482</span>         pickle_load_args<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">\"encoding\"</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">\"utf-8\"</span>\n<span class=\"ansi-green-fg ansi-bold\">   1483</span> \n<span class=\"ansi-green-fg\">-&gt; 1484</span><span class=\"ansi-red-fg\">     </span><span class=\"ansi-green-fg\">with</span> _open_file_like<span class=\"ansi-blue-fg\">(</span>f<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">\"rb\"</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> opened_file<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">   1485</span>         <span class=\"ansi-green-fg\">if</span> _is_zipfile<span class=\"ansi-blue-fg\">(</span>opened_file<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">   1486</span>             <span class=\"ansi-red-fg\"># The zipfile reader is going to advance the current file position.</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/serialization.py</span> in <span class=\"ansi-cyan-fg\">_open_file_like</span><span class=\"ansi-blue-fg\">(name_or_buffer, mode)</span>\n<span class=\"ansi-green-fg ansi-bold\">    757</span> <span class=\"ansi-green-fg\">def</span> _open_file_like<span class=\"ansi-blue-fg\">(</span>name_or_buffer<span class=\"ansi-blue-fg\">:</span> FileLike<span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">:</span> str<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">-&gt;</span> _opener<span class=\"ansi-blue-fg\">[</span>IO<span class=\"ansi-blue-fg\">[</span>bytes<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    758</span>     <span class=\"ansi-green-fg\">if</span> _is_path<span class=\"ansi-blue-fg\">(</span>name_or_buffer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 759</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> _open_file<span class=\"ansi-blue-fg\">(</span>name_or_buffer<span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    760</span>     <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    761</span>         <span class=\"ansi-green-fg\">if</span> <span class=\"ansi-blue-fg\">\"w\"</span> <span class=\"ansi-green-fg\">in</span> mode<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/serialization.py</span> in <span class=\"ansi-cyan-fg\">__init__</span><span class=\"ansi-blue-fg\">(self, name, mode)</span>\n<span class=\"ansi-green-fg ansi-bold\">    738</span> <span class=\"ansi-green-fg\">class</span> _open_file<span class=\"ansi-blue-fg\">(</span>_opener<span class=\"ansi-blue-fg\">[</span>IO<span class=\"ansi-blue-fg\">[</span>bytes<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    739</span>     <span class=\"ansi-green-fg\">def</span> __init__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> name<span class=\"ansi-blue-fg\">:</span> Union<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">,</span> os<span class=\"ansi-blue-fg\">.</span>PathLike<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">:</span> str<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">-&gt;</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 740</span><span class=\"ansi-red-fg\">         </span>super<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>__init__<span class=\"ansi-blue-fg\">(</span>open<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    741</span> \n<span class=\"ansi-green-fg ansi-bold\">    742</span>     <span class=\"ansi-green-fg\">def</span> __exit__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">FileNotFoundError</span>: [Errno 2] No such file or directory: 'tst_features.pt'</pre>\n```\n:::\n\n:::\n:::\n\n\n---\n\n## Save the classifier model to be used later or shared with collaborators {.scrollable}\n\n- [ ] Check the proportion of types of perturbation in the testing set\n\n::: {#413a34ec .cell execution_count=66}\n``` {.python .cell-code}\nn_total = n_dmso + n_crispr + n_orf\nn_dmso / n_total, n_crispr / n_total, n_orf / n_total\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ZeroDivisionError</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-351853166.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> n_total <span class=\"ansi-blue-fg\">=</span> n_dmso <span class=\"ansi-blue-fg\">+</span> n_crispr <span class=\"ansi-blue-fg\">+</span> n_orf\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>n_dmso <span class=\"ansi-blue-fg\">/</span> n_total<span class=\"ansi-blue-fg\">,</span> n_crispr <span class=\"ansi-blue-fg\">/</span> n_total<span class=\"ansi-blue-fg\">,</span> n_orf <span class=\"ansi-blue-fg\">/</span> n_total\n\n<span class=\"ansi-red-fg\">ZeroDivisionError</span>: division by zero</pre>\n```\n:::\n\n:::\n:::\n\n\n- [ ] Check the loss metrics on the testing set\n\n::: {#2e7da225 .cell execution_count=67}\n``` {.python .cell-code}\ncls_loss_epoch / n_total\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ZeroDivisionError</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-2159926283.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>cls_loss_epoch <span class=\"ansi-blue-fg\">/</span> n_total\n\n<span class=\"ansi-red-fg\">ZeroDivisionError</span>: division by zero</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#cf508a9c .cell execution_count=68}\n``` {.python .cell-code}\nred_loss_epoch / n_total\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ZeroDivisionError</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-1461694865.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>red_loss_epoch <span class=\"ansi-blue-fg\">/</span> n_total\n\n<span class=\"ansi-red-fg\">ZeroDivisionError</span>: division by zero</pre>\n```\n:::\n\n:::\n:::\n\n\n---\n\n## Save the classifier model to be used later or shared with collaborators {.scrollable}\n\n- [ ] Check the accuracy metrics on the testing set\n\n::: {#d25fc3e7 .cell execution_count=69}\n``` {.python .cell-code}\ntst_acc_metric.compute()\ntst_confmat.compute()\ntst_confmat.plot()\n```\n\n::: {.cell-output .cell-output-display execution_count=69}\n```\n(<Figure size 640x480 with 1 Axes>,\n <Axes: xlabel='Predicted class', ylabel='True class'>)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](DL_image_analysis_2_3_files/figure-revealjs/cell-70-output-2.png){width=489 height=491}\n:::\n:::\n\n\n---\n\n## Evaluate the capacity to mimic the UMap dimensionality reduction {.scrollable}\n\n- [ ] Get the embeddings from the trained model for the training set\n\n::: {#6ff6c073 .cell execution_count=70}\n``` {.python .cell-code}\ntrn_fx = []\ntrn_fx_pred = []\ntrn_y = []\n\nfor i, (x, y, fx) in enumerate(trn_feat_dl):\n    if i >= 10:\n        break\n\n    with torch.no_grad():\n        if torch.cuda.is_available():\n            x = x.cuda()\n\n        _, fx_pred = classifier(x)\n        trn_fx_pred.append(fx_pred.detach().cpu())\n        trn_fx.append(fx)\n        trn_y.append(y)\n\ntrn_fx = torch.cat(trn_fx, dim=0)\ntrn_fx_pred = torch.cat(trn_fx_pred, dim=0)\ntrn_y = torch.cat(trn_y, dim=0)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">FileNotFoundError</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-296939953.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> trn_y <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">for</span> i<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">,</span> y<span class=\"ansi-blue-fg\">,</span> fx<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">in</span> enumerate<span class=\"ansi-blue-fg\">(</span>trn_feat_dl<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">      6</span>     <span class=\"ansi-green-fg\">if</span> i <span class=\"ansi-blue-fg\">&gt;=</span> <span class=\"ansi-cyan-fg\">10</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">      7</span>         <span class=\"ansi-green-fg\">break</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py</span> in <span class=\"ansi-cyan-fg\">__next__</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">    732</span>                 <span class=\"ansi-red-fg\"># TODO(https://github.com/pytorch/pytorch/issues/76750)</span>\n<span class=\"ansi-green-fg ansi-bold\">    733</span>                 self<span class=\"ansi-blue-fg\">.</span>_reset<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>  <span class=\"ansi-red-fg\"># type: ignore[call-arg]</span>\n<span class=\"ansi-green-fg\">--&gt; 734</span><span class=\"ansi-red-fg\">             </span>data <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_next_data<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    735</span>             self<span class=\"ansi-blue-fg\">.</span>_num_yielded <span class=\"ansi-blue-fg\">+=</span> <span class=\"ansi-cyan-fg\">1</span>\n<span class=\"ansi-green-fg ansi-bold\">    736</span>             if (\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py</span> in <span class=\"ansi-cyan-fg\">_next_data</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1514</span>                 worker_id <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_task_info<span class=\"ansi-blue-fg\">.</span>pop<span class=\"ansi-blue-fg\">(</span>idx<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-fg ansi-bold\">   1515</span>                 self<span class=\"ansi-blue-fg\">.</span>_rcvd_idx <span class=\"ansi-blue-fg\">+=</span> <span class=\"ansi-cyan-fg\">1</span>\n<span class=\"ansi-green-fg\">-&gt; 1516</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_process_data<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> worker_id<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1517</span> \n<span class=\"ansi-green-fg ansi-bold\">   1518</span>     <span class=\"ansi-green-fg\">def</span> _try_put_index<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py</span> in <span class=\"ansi-cyan-fg\">_process_data</span><span class=\"ansi-blue-fg\">(self, data, worker_idx)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1549</span>         self<span class=\"ansi-blue-fg\">.</span>_try_put_index<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1550</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> ExceptionWrapper<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1551</span><span class=\"ansi-red-fg\">             </span>data<span class=\"ansi-blue-fg\">.</span>reraise<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1552</span>         <span class=\"ansi-green-fg\">return</span> data\n<span class=\"ansi-green-fg ansi-bold\">   1553</span> \n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/_utils.py</span> in <span class=\"ansi-cyan-fg\">reraise</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">    767</span>             <span class=\"ansi-red-fg\"># be constructed, don't try to instantiate since we don't know how to</span>\n<span class=\"ansi-green-fg ansi-bold\">    768</span>             <span class=\"ansi-green-fg\">raise</span> RuntimeError<span class=\"ansi-blue-fg\">(</span>msg<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-fg\">--&gt; 769</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">raise</span> exception\n<span class=\"ansi-green-fg ansi-bold\">    770</span> \n<span class=\"ansi-green-fg ansi-bold\">    771</span> \n\n<span class=\"ansi-red-fg\">FileNotFoundError</span>: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 33, in fetch\n    data.append(next(self.dataset_iter))\n                ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipython-input-2874583140.py\", line 25, in __iter__\n    features_dict = torch.load(url)\n                    ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1484, in load\n    with _open_file_like(f, \"rb\") as opened_file:\n         ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 759, in _open_file_like\n    return _open_file(name_or_buffer, mode)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 740, in __init__\n    super().__init__(open(name, mode))\n                     ^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: 'trn_features_006.pt'\n</pre>\n```\n:::\n\n:::\n:::\n\n\n---\n\n## Evaluate the capacity to mimic the UMap dimensionality reduction {.scrollable}\n\n- [ ] Compate the embedding spaces (UMap vs learned) with the training set\n\n::: {#c948abce .cell execution_count=71}\n``` {.python .cell-code}\nfor class_idx, class_name in enumerate(class_names):\n    plt.scatter(trn_fx[trn_y == class_idx, 0], trn_fx[trn_y == class_idx, 1], label=class_names[class_idx], marker=class_markers[class_idx], facecolors=class_facecolors[class_idx], edgecolors=class_colors[class_idx])\n\nplt.legend()\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-4170264153.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">for</span> class_idx<span class=\"ansi-blue-fg\">,</span> class_name <span class=\"ansi-green-fg\">in</span> enumerate<span class=\"ansi-blue-fg\">(</span>class_names<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\">     </span>plt<span class=\"ansi-blue-fg\">.</span>scatter<span class=\"ansi-blue-fg\">(</span>trn_fx<span class=\"ansi-blue-fg\">[</span>trn_y <span class=\"ansi-blue-fg\">==</span> class_idx<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> trn_fx<span class=\"ansi-blue-fg\">[</span>trn_y <span class=\"ansi-blue-fg\">==</span> class_idx<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> label<span class=\"ansi-blue-fg\">=</span>class_names<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> marker<span class=\"ansi-blue-fg\">=</span>class_markers<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> facecolors<span class=\"ansi-blue-fg\">=</span>class_facecolors<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> edgecolors<span class=\"ansi-blue-fg\">=</span>class_colors<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> \n<span class=\"ansi-green-fg ansi-bold\">      4</span> plt<span class=\"ansi-blue-fg\">.</span>legend<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: list indices must be integers or slices, not tuple</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#f9ae57cb .cell execution_count=72}\n``` {.python .cell-code}\nfor class_idx, class_name in enumerate(class_names):\n    plt.scatter(trn_fx_pred[trn_y == class_idx, 0], trn_fx_pred[trn_y == class_idx, 1], label=class_names[class_idx], marker=class_markers[class_idx], facecolors=class_facecolors[class_idx], edgecolors=class_colors[class_idx])\n\nplt.legend()\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-1926387892.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">for</span> class_idx<span class=\"ansi-blue-fg\">,</span> class_name <span class=\"ansi-green-fg\">in</span> enumerate<span class=\"ansi-blue-fg\">(</span>class_names<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\">     </span>plt<span class=\"ansi-blue-fg\">.</span>scatter<span class=\"ansi-blue-fg\">(</span>trn_fx_pred<span class=\"ansi-blue-fg\">[</span>trn_y <span class=\"ansi-blue-fg\">==</span> class_idx<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> trn_fx_pred<span class=\"ansi-blue-fg\">[</span>trn_y <span class=\"ansi-blue-fg\">==</span> class_idx<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> label<span class=\"ansi-blue-fg\">=</span>class_names<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> marker<span class=\"ansi-blue-fg\">=</span>class_markers<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> facecolors<span class=\"ansi-blue-fg\">=</span>class_facecolors<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> edgecolors<span class=\"ansi-blue-fg\">=</span>class_colors<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> \n<span class=\"ansi-green-fg ansi-bold\">      4</span> plt<span class=\"ansi-blue-fg\">.</span>legend<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: list indices must be integers or slices, not tuple</pre>\n```\n:::\n\n:::\n:::\n\n\n---\n\n## Evaluate the capacity to mimic the UMap dimensionality reduction {.scrollable}\n\n- [ ] Get the embeddings from the trained model for the validation set\n\n::: {#35e1daf0 .cell execution_count=73}\n``` {.python .cell-code}\nval_fx = []\nval_fx_pred = []\nval_y = []\n\nfor i, (x_val, y_val, fx_val) in enumerate(val_feat_dl):\n    if i >= 10:\n        break\n\n    with torch.no_grad():\n        if torch.cuda.is_available():\n            x_val = x_val.cuda()\n\n        _, fx_pred_val = classifier(x_val)\n        val_fx_pred.append(fx_pred_val.detach().cpu())\n        val_fx.append(fx_val)\n        val_y.append(y_val)\n\nval_fx = torch.cat(val_fx, dim=0)\nval_fx_pred = torch.cat(val_fx_pred, dim=0)\nval_y = torch.cat(val_y, dim=0)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">FileNotFoundError</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-402407764.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> val_y <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">for</span> i<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">(</span>x_val<span class=\"ansi-blue-fg\">,</span> y_val<span class=\"ansi-blue-fg\">,</span> fx_val<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">in</span> enumerate<span class=\"ansi-blue-fg\">(</span>val_feat_dl<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">      6</span>     <span class=\"ansi-green-fg\">if</span> i <span class=\"ansi-blue-fg\">&gt;=</span> <span class=\"ansi-cyan-fg\">10</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">      7</span>         <span class=\"ansi-green-fg\">break</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py</span> in <span class=\"ansi-cyan-fg\">__next__</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">    732</span>                 <span class=\"ansi-red-fg\"># TODO(https://github.com/pytorch/pytorch/issues/76750)</span>\n<span class=\"ansi-green-fg ansi-bold\">    733</span>                 self<span class=\"ansi-blue-fg\">.</span>_reset<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>  <span class=\"ansi-red-fg\"># type: ignore[call-arg]</span>\n<span class=\"ansi-green-fg\">--&gt; 734</span><span class=\"ansi-red-fg\">             </span>data <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_next_data<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    735</span>             self<span class=\"ansi-blue-fg\">.</span>_num_yielded <span class=\"ansi-blue-fg\">+=</span> <span class=\"ansi-cyan-fg\">1</span>\n<span class=\"ansi-green-fg ansi-bold\">    736</span>             if (\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py</span> in <span class=\"ansi-cyan-fg\">_next_data</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">    788</span>     <span class=\"ansi-green-fg\">def</span> _next_data<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    789</span>         index <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_next_index<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>  <span class=\"ansi-red-fg\"># may raise StopIteration</span>\n<span class=\"ansi-green-fg\">--&gt; 790</span><span class=\"ansi-red-fg\">         </span>data <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_dataset_fetcher<span class=\"ansi-blue-fg\">.</span>fetch<span class=\"ansi-blue-fg\">(</span>index<span class=\"ansi-blue-fg\">)</span>  <span class=\"ansi-red-fg\"># may raise StopIteration</span>\n<span class=\"ansi-green-fg ansi-bold\">    791</span>         <span class=\"ansi-green-fg\">if</span> self<span class=\"ansi-blue-fg\">.</span>_pin_memory<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    792</span>             data <span class=\"ansi-blue-fg\">=</span> _utils<span class=\"ansi-blue-fg\">.</span>pin_memory<span class=\"ansi-blue-fg\">.</span>pin_memory<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_pin_memory_device<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py</span> in <span class=\"ansi-cyan-fg\">fetch</span><span class=\"ansi-blue-fg\">(self, possibly_batched_index)</span>\n<span class=\"ansi-green-fg ansi-bold\">     31</span>             <span class=\"ansi-green-fg\">for</span> _ <span class=\"ansi-green-fg\">in</span> possibly_batched_index<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">     32</span>                 <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 33</span><span class=\"ansi-red-fg\">                     </span>data<span class=\"ansi-blue-fg\">.</span>append<span class=\"ansi-blue-fg\">(</span>next<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>dataset_iter<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     34</span>                 <span class=\"ansi-green-fg\">except</span> StopIteration<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">     35</span>                     self<span class=\"ansi-blue-fg\">.</span>ended <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">True</span>\n\n<span class=\"ansi-green-fg\">/tmp/ipython-input-2874583140.py</span> in <span class=\"ansi-cyan-fg\">__iter__</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-fg ansi-bold\">     23</span> \n<span class=\"ansi-green-fg ansi-bold\">     24</span>         <span class=\"ansi-green-fg\">for</span> url <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>_features_url<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 25</span><span class=\"ansi-red-fg\">             </span>features_dict <span class=\"ansi-blue-fg\">=</span> torch<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span>url<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     26</span> \n<span class=\"ansi-green-fg ansi-bold\">     27</span>             <span class=\"ansi-green-fg\">if</span> self<span class=\"ansi-blue-fg\">.</span>_reducer <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/serialization.py</span> in <span class=\"ansi-cyan-fg\">load</span><span class=\"ansi-blue-fg\">(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1482</span>         pickle_load_args<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">\"encoding\"</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">\"utf-8\"</span>\n<span class=\"ansi-green-fg ansi-bold\">   1483</span> \n<span class=\"ansi-green-fg\">-&gt; 1484</span><span class=\"ansi-red-fg\">     </span><span class=\"ansi-green-fg\">with</span> _open_file_like<span class=\"ansi-blue-fg\">(</span>f<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">\"rb\"</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> opened_file<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">   1485</span>         <span class=\"ansi-green-fg\">if</span> _is_zipfile<span class=\"ansi-blue-fg\">(</span>opened_file<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">   1486</span>             <span class=\"ansi-red-fg\"># The zipfile reader is going to advance the current file position.</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/serialization.py</span> in <span class=\"ansi-cyan-fg\">_open_file_like</span><span class=\"ansi-blue-fg\">(name_or_buffer, mode)</span>\n<span class=\"ansi-green-fg ansi-bold\">    757</span> <span class=\"ansi-green-fg\">def</span> _open_file_like<span class=\"ansi-blue-fg\">(</span>name_or_buffer<span class=\"ansi-blue-fg\">:</span> FileLike<span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">:</span> str<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">-&gt;</span> _opener<span class=\"ansi-blue-fg\">[</span>IO<span class=\"ansi-blue-fg\">[</span>bytes<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    758</span>     <span class=\"ansi-green-fg\">if</span> _is_path<span class=\"ansi-blue-fg\">(</span>name_or_buffer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 759</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> _open_file<span class=\"ansi-blue-fg\">(</span>name_or_buffer<span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    760</span>     <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    761</span>         <span class=\"ansi-green-fg\">if</span> <span class=\"ansi-blue-fg\">\"w\"</span> <span class=\"ansi-green-fg\">in</span> mode<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/usr/local/lib/python3.12/dist-packages/torch/serialization.py</span> in <span class=\"ansi-cyan-fg\">__init__</span><span class=\"ansi-blue-fg\">(self, name, mode)</span>\n<span class=\"ansi-green-fg ansi-bold\">    738</span> <span class=\"ansi-green-fg\">class</span> _open_file<span class=\"ansi-blue-fg\">(</span>_opener<span class=\"ansi-blue-fg\">[</span>IO<span class=\"ansi-blue-fg\">[</span>bytes<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg ansi-bold\">    739</span>     <span class=\"ansi-green-fg\">def</span> __init__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> name<span class=\"ansi-blue-fg\">:</span> Union<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">,</span> os<span class=\"ansi-blue-fg\">.</span>PathLike<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">:</span> str<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">-&gt;</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 740</span><span class=\"ansi-red-fg\">         </span>super<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>__init__<span class=\"ansi-blue-fg\">(</span>open<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    741</span> \n<span class=\"ansi-green-fg ansi-bold\">    742</span>     <span class=\"ansi-green-fg\">def</span> __exit__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">FileNotFoundError</span>: [Errno 2] No such file or directory: 'val_features.pt'</pre>\n```\n:::\n\n:::\n:::\n\n\n---\n\n## Evaluate the capacity to mimic the UMap dimensionality reduction {.scrollable}\n\n- [ ] Compate the embedding spaces (UMap vs learned) with the validation set\n\n::: {#9ee075dd .cell execution_count=74}\n``` {.python .cell-code}\nfor class_idx, class_name in enumerate(class_names):\n    plt.scatter(val_fx[val_y == class_idx, 0], val_fx[val_y == class_idx, 1], label=class_names[class_idx], marker=class_markers[class_idx], facecolors=class_facecolors[class_idx], edgecolors=class_colors[class_idx])\n\nplt.legend()\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-684093476.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">for</span> class_idx<span class=\"ansi-blue-fg\">,</span> class_name <span class=\"ansi-green-fg\">in</span> enumerate<span class=\"ansi-blue-fg\">(</span>class_names<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\">     </span>plt<span class=\"ansi-blue-fg\">.</span>scatter<span class=\"ansi-blue-fg\">(</span>val_fx<span class=\"ansi-blue-fg\">[</span>val_y <span class=\"ansi-blue-fg\">==</span> class_idx<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> val_fx<span class=\"ansi-blue-fg\">[</span>val_y <span class=\"ansi-blue-fg\">==</span> class_idx<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> label<span class=\"ansi-blue-fg\">=</span>class_names<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> marker<span class=\"ansi-blue-fg\">=</span>class_markers<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> facecolors<span class=\"ansi-blue-fg\">=</span>class_facecolors<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> edgecolors<span class=\"ansi-blue-fg\">=</span>class_colors<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> \n<span class=\"ansi-green-fg ansi-bold\">      4</span> plt<span class=\"ansi-blue-fg\">.</span>legend<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: list indices must be integers or slices, not tuple</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#3228f00b .cell execution_count=75}\n``` {.python .cell-code}\nfor class_idx, class_name in enumerate(class_names):\n    plt.scatter(val_fx_pred[val_y == class_idx, 0], val_fx_pred[val_y == class_idx, 1], label=class_names[class_idx], marker=class_markers[class_idx], facecolors=class_facecolors[class_idx], edgecolors=class_colors[class_idx])\n\nplt.legend()\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-2487391935.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">for</span> class_idx<span class=\"ansi-blue-fg\">,</span> class_name <span class=\"ansi-green-fg\">in</span> enumerate<span class=\"ansi-blue-fg\">(</span>class_names<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\">     </span>plt<span class=\"ansi-blue-fg\">.</span>scatter<span class=\"ansi-blue-fg\">(</span>val_fx_pred<span class=\"ansi-blue-fg\">[</span>val_y <span class=\"ansi-blue-fg\">==</span> class_idx<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> val_fx_pred<span class=\"ansi-blue-fg\">[</span>val_y <span class=\"ansi-blue-fg\">==</span> class_idx<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> label<span class=\"ansi-blue-fg\">=</span>class_names<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> marker<span class=\"ansi-blue-fg\">=</span>class_markers<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> facecolors<span class=\"ansi-blue-fg\">=</span>class_facecolors<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> edgecolors<span class=\"ansi-blue-fg\">=</span>class_colors<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> \n<span class=\"ansi-green-fg ansi-bold\">      4</span> plt<span class=\"ansi-blue-fg\">.</span>legend<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: list indices must be integers or slices, not tuple</pre>\n```\n:::\n\n:::\n:::\n\n\n# Use the pre-trained model to identify the behavior of compounds\n\nBecause the model has learned to recognize *CRISPR*, *ORF*, and *NONE/DMSO* effects, it can be used to determine the behavior of any treatment based on their morphological profile.\n\nTo do so, first extract the the morphological features using the pre-trained *MobileNet*, and then use the extracted features as input for the classifier model.\n\n---\n\n##  Execute the feature extraction and classification pipeline for *compound* data {.scrollable}\n\n::: {#809f8d55 .cell execution_count=76}\n``` {.python .cell-code}\ncompounds_ds = TiffS3Dataset(comp_plate_maps, wells_metadata, comp_plate_maps[\"Plate_name\"].tolist(), 16, 24, 9, 5, shuffle=True)\n\nbatch_size = 5\n\ncompounds_dl = DataLoader(compounds_ds, batch_size=batch_size, num_workers=2, worker_init_fn=dataset_worker_init_fn)\n\nmetadata_list = []\nfor i, (x, y, metadata) in tqdm(enumerate(compounds_dl)):\n    metadata_list.append(metadata)\n\n    b, c, h, w = x.shape\n    x_t = model_transforms(torch.tile(x.reshape(-1, 1, h, w), (1, 3, 1, 1)))\n\n    if torch.cuda.is_available():\n        x_t = x_t.cuda()\n\n    with torch.no_grad():\n        x_out = model(x_t)\n        x_out = x_out.detach().reshape(-1, c, 576, 7, 7).sum(dim=1)\n        x_out = org_avgpool(x_out).detach().reshape(b, -1)\n\n        y_pred, fx_pred = classifier(x_out)\n\n    break\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"6ebf69a3e3b74e76a1e4a0445a3e9b06\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_2/images/20210823_Batch_10/images/1086292105/1086292105_L14_T0001F009L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_2/images/20210816_Batch_9/images/1086292396/1086292396_G01_T0001F008L01A01Z01C01.tif\n```\n:::\n:::\n\n\n---\n\n##  Execute the feature extraction and classification pipeline for *compound* data {.scrollable}\n\n- [ ] Plot the embeddings of the compound samples\n\n::: {#3687e777 .cell execution_count=77}\n``` {.python .cell-code}\nfor class_idx, class_name in enumerate(class_names[:3]):\n    plt.scatter(trn_fx_pred[trn_y == class_idx, 0][::10], trn_fx_pred[trn_y == class_idx, 1][::10], label=class_names[class_idx], marker=class_markers[class_idx], facecolors=class_facecolors[class_idx], edgecolors=class_colors[class_idx])\n\nplt.scatter(fx_pred.cpu()[:, 0], fx_pred.cpu()[:, 1], label=class_names[3], marker=class_markers[3], facecolors=class_facecolors[3], edgecolors=class_colors[3])\n\nplt.legend()\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/tmp/ipython-input-755015200.py</span> in <span class=\"ansi-cyan-fg\">&lt;cell line: 0&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">for</span> class_idx<span class=\"ansi-blue-fg\">,</span> class_name <span class=\"ansi-green-fg\">in</span> enumerate<span class=\"ansi-blue-fg\">(</span>class_names<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\">     </span>plt<span class=\"ansi-blue-fg\">.</span>scatter<span class=\"ansi-blue-fg\">(</span>trn_fx_pred<span class=\"ansi-blue-fg\">[</span>trn_y <span class=\"ansi-blue-fg\">==</span> class_idx<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-cyan-fg\">10</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> trn_fx_pred<span class=\"ansi-blue-fg\">[</span>trn_y <span class=\"ansi-blue-fg\">==</span> class_idx<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-cyan-fg\">10</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> label<span class=\"ansi-blue-fg\">=</span>class_names<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> marker<span class=\"ansi-blue-fg\">=</span>class_markers<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> facecolors<span class=\"ansi-blue-fg\">=</span>class_facecolors<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> edgecolors<span class=\"ansi-blue-fg\">=</span>class_colors<span class=\"ansi-blue-fg\">[</span>class_idx<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> \n<span class=\"ansi-green-fg ansi-bold\">      4</span> plt<span class=\"ansi-blue-fg\">.</span>scatter<span class=\"ansi-blue-fg\">(</span>fx_pred<span class=\"ansi-blue-fg\">.</span>cpu<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> fx_pred<span class=\"ansi-blue-fg\">.</span>cpu<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> label<span class=\"ansi-blue-fg\">=</span>class_names<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> marker<span class=\"ansi-blue-fg\">=</span>class_markers<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> facecolors<span class=\"ansi-blue-fg\">=</span>class_facecolors<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> edgecolors<span class=\"ansi-blue-fg\">=</span>class_colors<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">      5</span> \n\n<span class=\"ansi-red-fg\">TypeError</span>: list indices must be integers or slices, not tuple</pre>\n```\n:::\n\n:::\n:::\n\n\n---\n\n##  Execute the feature extraction and classification pipeline for *compound* data {.scrollable}\n\n- [ ] Look for any interesting compounds in [PubChem](https://pubchem.ncbi.nlm.nih.gov/)\n\n::: {#008ec759 .cell execution_count=78}\n``` {.python .cell-code}\nmetadata\n```\n\n::: {.cell-output .cell-output-display execution_count=78}\n```\n{'Plate_name': ['GR00004377',\n  'J12455d',\n  'P01_ACPJUM062',\n  '110000296341',\n  '1086293409'],\n 'Source_name': ['source_9', 'source_3', 'source_5', 'source_6', 'source_2'],\n 'Batch_name': ['20210918-Run12',\n  'CP_26_all_Phenix1',\n  'JUMPCPE-20210716-Run12_20210719_162047',\n  'p210920CPU2OS48hw384exp028JUMP',\n  '20210726_Batch_7'],\n 'Plate_type': ['COMPOUND', 'COMPOUND', 'COMPOUND', 'COMPOUND', 'COMPOUND'],\n 'Plate_path': ['GR00004377',\n  'J12455d__2021-09-24T16_30_28-Measurement1',\n  'P01_ACPJUM062',\n  '110000296341',\n  '1086293409'],\n 'Well_position': ['H02', 'P20', 'K14', 'G20', 'C01']}\n```\n:::\n:::\n\n\n::: {#2c723a2e .cell execution_count=79}\n``` {.python .cell-code}\ny_pred.argmax(dim=1)\n```\n\n::: {.cell-output .cell-output-display execution_count=79}\n```\ntensor([1, 1, 1, 1, 1])\n```\n:::\n:::\n\n\n::: {#590cf821 .cell execution_count=80}\n``` {.python .cell-code}\nwells_metadata.query(\"well_position == 'H13' & Plate_type=='COMPOUND'\")\n```\n\n::: {.cell-output .cell-output-display execution_count=80}\n```{=html}\n\n  <div id=\"df-0fdbd1cb-4c34-47e6-8361-d0a2b362b9e8\" class=\"colab-df-container\">\n    <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>well_position</th>\n      <th>broad_sample</th>\n      <th>solvent</th>\n      <th>Plate_type</th>\n      <th>Plate_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>180</th>\n      <td>H13</td>\n      <td>NaN</td>\n      <td>DMSO</td>\n      <td>COMPOUND</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n    <div class=\"colab-df-buttons\">\n      \n  <div class=\"colab-df-container\">\n    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0fdbd1cb-4c34-47e6-8361-d0a2b362b9e8')\"\n            title=\"Convert this dataframe to an interactive table.\"\n            style=\"display:none;\">\n      \n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n  </svg>\n    </button>\n    \n  <style>\n    .colab-df-container {\n      display:flex;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    .colab-df-buttons div {\n      margin-bottom: 4px;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n    <script>\n      const buttonEl =\n        document.querySelector('#df-0fdbd1cb-4c34-47e6-8361-d0a2b362b9e8 button.colab-df-convert');\n      buttonEl.style.display =\n        google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n      async function convertToInteractive(key) {\n        const element = document.querySelector('#df-0fdbd1cb-4c34-47e6-8361-d0a2b362b9e8');\n        const dataTable =\n          await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                    [key], {});\n        if (!dataTable) return;\n\n        const docLinkHtml = 'Like what you see? Visit the ' +\n          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n          + ' to learn more about interactive tables.';\n        element.innerHTML = '';\n        dataTable['output_type'] = 'display_data';\n        await google.colab.output.renderOutput(dataTable, element);\n        const docLink = document.createElement('div');\n        docLink.innerHTML = docLinkHtml;\n        element.appendChild(docLink);\n      }\n    </script>\n  </div>\n  \n    </div>\n  </div>\n  \n```\n:::\n:::\n\n\n",
    "supporting": [
      "DL_image_analysis_2_3_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n<script src=\"https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js\" crossorigin=\"anonymous\"></script>\n"
      ],
      "include-after-body": [
        "<script type=application/vnd.jupyter.widget-state+json>\n{\"state\":{\"01569450f9224691b2ffe5254a8c89fe\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":\"20px\"}},\"02c1cf745c534bceb98c12b5bdb9dab0\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"08a658df8f65443ba424a102af65ed57\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"DescriptionStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"DescriptionStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"description_width\":\"\"}},\"08d30d0518f1469aa9a71498bc5f5165\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":\"20px\"}},\"0a51b40ec6db4465bd743aa053b7164e\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_2cab4de73cc64a0f9d087d15104e4d0c\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_26193c4768d742d293c77098291b39bc\",\"value\":\"\"}},\"0b3bc5a67d3646ebb0d9097f6fb9d5e3\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_0a51b40ec6db4465bd743aa053b7164e\",\"IPY_MODEL_0d5a946d3f4e4841abbd81d0a8c28aa6\",\"IPY_MODEL_4f34c087e6334ef4ab938c376a88c406\"],\"layout\":\"IPY_MODEL_4ea6d1b8244f46df9a2a43852de002fc\"}},\"0c82ac7a455949cfab50624b6fb2b456\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_855096cb661c4a8eba0a2221e0baf181\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_c9c4f52d21db4e36865ab7bdc51c391a\",\"value\":\"\"}},\"0d5a946d3f4e4841abbd81d0a8c28aa6\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"danger\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_08d30d0518f1469aa9a71498bc5f5165\",\"max\":1,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_3fb63a4fa21f4821b2d87965917c32d0\",\"value\":0}},\"1360be5ed1d147818f8c088c6efe4ef4\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_6e665e4308d1429baf9e196568f9a434\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_d2bbc9e1a4d84bc08baf6f6faf225192\",\"value\":\" 0/? [00:11&lt;?, ?it/s]\"}},\"13cdb7a769584a5c992671c21614bc82\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"DescriptionStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"DescriptionStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"description_width\":\"\"}},\"158f7062a8f749c39f8533ba5ca18bc9\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_c60d19912255458ca10d72454d0e891e\",\"IPY_MODEL_45b331c0bf6b4e0991cedf9608a0d064\",\"IPY_MODEL_a3c6bd43f3b641ebb542572ced0b0dd3\"],\"layout\":\"IPY_MODEL_18e7da66e26e4b54b56735164ca5b458\"}},\"18e7da66e26e4b54b56735164ca5b458\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"1a54639c62b94e14a3ec32f1ae9ab9e5\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_ed55ca8a0e484319a3751a0708c8c948\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_fac7b5ac8c4e48f697dab61308f92f2f\",\"value\":\" 0/? [00:05&lt;?, ?it/s]\"}},\"21a3d3cbae5f48f8975ac04eded252a0\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_0c82ac7a455949cfab50624b6fb2b456\",\"IPY_MODEL_50fd9687c6d74cbb9728837ac836beff\",\"IPY_MODEL_1360be5ed1d147818f8c088c6efe4ef4\"],\"layout\":\"IPY_MODEL_7e433d24f19742408dc6b22be2fc56e1\"}},\"26193c4768d742d293c77098291b39bc\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"DescriptionStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"DescriptionStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"description_width\":\"\"}},\"2ca2bac1c72a49dc9f28526deb863178\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"DescriptionStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"DescriptionStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"description_width\":\"\"}},\"2cab4de73cc64a0f9d087d15104e4d0c\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"2cd0e0dbc0f540f5b473d0f9d88181de\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"2dc06d1e51c843b78aaf6733ac3a59a5\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"DescriptionStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"DescriptionStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"description_width\":\"\"}},\"2f6fee7aebbf4f95bc841b55fc58e556\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"30e35cc355184b1d8aefdd48f8db8240\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"36156fc8e12146a0a5122d06efeb5b92\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"3e48840baa2f4cebb9721e3fa0eb264f\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_b8feb144726f4611b4f5b855db4fae2a\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_2ca2bac1c72a49dc9f28526deb863178\",\"value\":\" 0/20 [00:00&lt;?, ?it/s]\"}},\"3eb72541a3a5487f8663a267b47e0c62\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"danger\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_7fb587d905f14c55a831e58d08c6115f\",\"max\":1000,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_7234c1c1fdc2400197bf9ed066304cd0\",\"value\":0}},\"3fb63a4fa21f4821b2d87965917c32d0\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"45b331c0bf6b4e0991cedf9608a0d064\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"danger\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_cfca5fb6bdf846edb5e724af62007ec5\",\"max\":1,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_9f79d43ac1e64114a47df9673fe5cbed\",\"value\":0}},\"4b26eb588d02435ca5908f43d36c4509\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":\"20px\"}},\"4ea6d1b8244f46df9a2a43852de002fc\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"4f34c087e6334ef4ab938c376a88c406\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_b100676b348d4e8786544bfbbc3bc5f7\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_db228e2759ee470a927e3bc1de9ef0a2\",\"value\":\" 0/? [00:00&lt;?, ?it/s]\"}},\"50fd9687c6d74cbb9728837ac836beff\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"danger\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_b80a62f70250487aa4c3324191a72e94\",\"max\":1,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_584575b5db524682b6af9917764ff5f4\",\"value\":0}},\"54a06fe920a24ab297a91603b6d46c5e\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"DescriptionStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"DescriptionStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"description_width\":\"\"}},\"584575b5db524682b6af9917764ff5f4\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"5c7c7ea87af8477d947de6cce5ad87ab\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"67fab24f8a1e425aae223ee9f5608dc8\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"6a016012285844f9974cc93f340412fb\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"6d3e635ee37d4c1ebde4339c905bf729\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_c8db8d8029bc4aeeb6ae7a6b97cebefc\",\"IPY_MODEL_3eb72541a3a5487f8663a267b47e0c62\",\"IPY_MODEL_8f3c24d4ffe0436fb6ebcc7653177478\"],\"layout\":\"IPY_MODEL_b06656687c43468d99646778e4a1d64b\"}},\"6e665e4308d1429baf9e196568f9a434\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"6ebf69a3e3b74e76a1e4a0445a3e9b06\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_e557df0b004e428aaa4deaf5dc749dc5\",\"IPY_MODEL_a9e9b1eee0244538abbb17eba1f233fc\",\"IPY_MODEL_1a54639c62b94e14a3ec32f1ae9ab9e5\"],\"layout\":\"IPY_MODEL_ea687b4ac76847f49f8bab2f61d4d326\"}},\"7234c1c1fdc2400197bf9ed066304cd0\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"7759eed263f94ea8bae0146112b94493\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_c9844ee3e0694b80948ca177a6c0e091\",\"IPY_MODEL_fe4838d1490e40da87d267f8229d96e1\",\"IPY_MODEL_3e48840baa2f4cebb9721e3fa0eb264f\"],\"layout\":\"IPY_MODEL_02c1cf745c534bceb98c12b5bdb9dab0\"}},\"7bdf30486b93409f986e1d33c504ce88\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"DescriptionStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"DescriptionStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"description_width\":\"\"}},\"7e433d24f19742408dc6b22be2fc56e1\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"7f4eb26c464143cc8632282e8b64ac72\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"7fb587d905f14c55a831e58d08c6115f\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"855096cb661c4a8eba0a2221e0baf181\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"8a77e23ad4e44946a863fd65cbce3ba5\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"DescriptionStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"DescriptionStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"description_width\":\"\"}},\"8f3c24d4ffe0436fb6ebcc7653177478\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_6a016012285844f9974cc93f340412fb\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_08a658df8f65443ba424a102af65ed57\",\"value\":\" 0/1000 [00:00&lt;?, ?it/s]\"}},\"8fb130c860914a9a95a479f9ad3721fd\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"9da81a911caf41479a3e1ef2bdabf25c\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"9f79d43ac1e64114a47df9673fe5cbed\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"a3c6bd43f3b641ebb542572ced0b0dd3\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_67fab24f8a1e425aae223ee9f5608dc8\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_8a77e23ad4e44946a863fd65cbce3ba5\",\"value\":\" 0/? [00:11&lt;?, ?it/s]\"}},\"a985e0e77ef94a5da75da3f15574b9c1\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"danger\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_01569450f9224691b2ffe5254a8c89fe\",\"max\":1,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_de28e9a4c50d48dbb290206f3d44e281\",\"value\":0}},\"a9dbde349d21442b93acbb7e58aae552\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_30e35cc355184b1d8aefdd48f8db8240\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_13cdb7a769584a5c992671c21614bc82\",\"value\":\" 0/? [00:11&lt;?, ?it/s]\"}},\"a9e9b1eee0244538abbb17eba1f233fc\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"danger\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_4b26eb588d02435ca5908f43d36c4509\",\"max\":1,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_36156fc8e12146a0a5122d06efeb5b92\",\"value\":0}},\"b06656687c43468d99646778e4a1d64b\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"b100676b348d4e8786544bfbbc3bc5f7\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"b80a62f70250487aa4c3324191a72e94\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":\"20px\"}},\"b8feb144726f4611b4f5b855db4fae2a\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"c60d19912255458ca10d72454d0e891e\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_7f4eb26c464143cc8632282e8b64ac72\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_c677961f2a6e42afb0eb98b13f9bb304\",\"value\":\"\"}},\"c677961f2a6e42afb0eb98b13f9bb304\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"DescriptionStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"DescriptionStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"description_width\":\"\"}},\"c8db8d8029bc4aeeb6ae7a6b97cebefc\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_2cd0e0dbc0f540f5b473d0f9d88181de\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_7bdf30486b93409f986e1d33c504ce88\",\"value\":\"  0%\"}},\"c9844ee3e0694b80948ca177a6c0e091\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_f076624978454011a3b8294b169fed15\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_2dc06d1e51c843b78aaf6733ac3a59a5\",\"value\":\"  0%\"}},\"c9c4f52d21db4e36865ab7bdc51c391a\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"DescriptionStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"DescriptionStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"description_width\":\"\"}},\"cfca5fb6bdf846edb5e724af62007ec5\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":\"20px\"}},\"d2bbc9e1a4d84bc08baf6f6faf225192\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"DescriptionStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"DescriptionStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"description_width\":\"\"}},\"d9b02cdbd72747a6a2306f40f0fb0ef3\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_feffb03d858149bda0e01a7bd2f7bb30\",\"IPY_MODEL_a985e0e77ef94a5da75da3f15574b9c1\",\"IPY_MODEL_a9dbde349d21442b93acbb7e58aae552\"],\"layout\":\"IPY_MODEL_e9b0e212bb894121b28e1c4fdc17f50d\"}},\"db228e2759ee470a927e3bc1de9ef0a2\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"DescriptionStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"DescriptionStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"description_width\":\"\"}},\"de28e9a4c50d48dbb290206f3d44e281\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"e557df0b004e428aaa4deaf5dc749dc5\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_2f6fee7aebbf4f95bc841b55fc58e556\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_54a06fe920a24ab297a91603b6d46c5e\",\"value\":\"\"}},\"e9b0e212bb894121b28e1c4fdc17f50d\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"ea687b4ac76847f49f8bab2f61d4d326\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"ed55ca8a0e484319a3751a0708c8c948\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"f076624978454011a3b8294b169fed15\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"1.2.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"1.2.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"overflow_x\":null,\"overflow_y\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"fac7b5ac8c4e48f697dab61308f92f2f\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"DescriptionStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"DescriptionStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"description_width\":\"\"}},\"fc64036223c84f9c9950d815a56b5a4c\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"DescriptionStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"DescriptionStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"1.2.0\",\"_view_name\":\"StyleView\",\"description_width\":\"\"}},\"fe4838d1490e40da87d267f8229d96e1\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_9da81a911caf41479a3e1ef2bdabf25c\",\"max\":20,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_8fb130c860914a9a95a479f9ad3721fd\",\"value\":0}},\"feffb03d858149bda0e01a7bd2f7bb30\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"1.5.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"1.5.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"1.5.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_tooltip\":null,\"layout\":\"IPY_MODEL_5c7c7ea87af8477d947de6cce5ad87ab\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_fc64036223c84f9c9950d815a56b5a4c\",\"value\":\"\"}}},\"version_major\":2,\"version_minor\":0}\n</script>\n"
      ]
    }
  }
}