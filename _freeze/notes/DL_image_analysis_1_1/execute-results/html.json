{
  "hash": "fc021719ad32ecfb527f7cd77343c4a4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Advanced Machine Learning with Python (Session 1)\nauthor: Fernando Cervantes (fernando.cervantes@jax.org)\nformat: \n  revealjs:\n    code-fold: false\n    progress: true\n    controls: true\n    output-file: \"Adv_ML_Python_presentation_1\"\n    fontsize: 20pt\n\nexecute:\n  error: true\n  echo: true\n  cache: true\n  freeze: true\n  keep-ipynb: true\n\njupyter: adv-ml\n---\n\n\n\n# Workshop outcomes\n\n* Understand the process of training ML models.\n*\tLoad pre-trained ML models and fine-tune them with new data.\n*\tEvaluate the performance of ML models.\n*\tAdapt ML models for different tasks from pre-trained models.\n\n\n# 0. Setup environment\n\n## Select runtime and connect\n\nOn the top right corner of the page, click the drop-down arrow to the right of the `Connect` button and select `Change runtime type`.\n\n![](../imgs/connect_runtime.png){width=50%}\n\n---\n\nMake sure `Python 3` runtime is selected. For this part of the workshop `CPU` acceleration is enough.\n\n![](../imgs/select_runtime.png){width=50%}\n\n---\n\nNow we can connect to the runtime by clicking `Connect`. This will create a **V**irtual **M**achine (**VM**) with compute resources we can use for a limited amount of time.\n\n![](../imgs/connect.png){height=25%}\n\n:::{.callout-caution}\nIn free Colab accounts these resources are not guaranteed and can be taken away without notice (preemptible machines).\n\nData stored in this runtime will be lost if not moved into other storage when the runtime is deleted.\n:::\n\n# 1. What is **M**achine **L**earning (**ML**)?\n\n---\n\n## **M**achine **L**earning (**ML**)\n\nSub-field of **Artificial Intelligence** that develops methods to address tasks that require human intelligence\n\n![](../imgs/Diagram_AI.png){width=50%}\n\n\n## Artificial intelligence tasks\n\n---\n\n## Common tasks\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n\n**Classification** \n\n\n![](../imgs/Object.png)\n\nwhat is this?\n\n:::\n\n::: {.column width=\"30%\"}\n\n**Detection** \n\n![](../imgs/Detect.png)\n\nwhere is something?\n\n:::\n\n::: {.column width=\"30%\"}\n\n**Segmentation**\n\n![](../imgs/Segment.png)\n\nwhere *specifically* is something?\n\n:::\n\n::::\n\n---\n\n### More tasks addressed in recent years\n\n* Style transference\n\n* Compression of image/video/etc...\n\n* Generation of content\n\n* Language processing\n\n# Types of machine learning\n\n---\n\n## Types of machine learning\n\n### Depending on how the model is trained\n\n* Supervised\n\n* Unsupervised\n\n* Weakly supervised\n\n* Reinforced\n\n* ...\n\n::: {.notes}\nSupervised learning: teach the machine to perform a task with a set of inputs and their respective expected outcome ($X$, $Y$).\n\nUnsupervised learning: let the machine learn to perform a task on its own ($X$) without any specific expected outcome.\n\nWeakly supervised: teach the machine to perform a task using a limited set of expected outcomes.\n\nReinforced learning: let the machine learn to perform a task on its own, then give it a reward relative to its performance.\n:::\n\n\n# Inputs and outputs\n\n---\n\n## Inputs and outputs\n\nFor a task, we want to *model* the **outcome/output** ($y$) obtained by a given **input** ($x$)\n\n$f(x) \\approx y$\n\n::: {.callout-note}\nThe complete set of ($x$, $y$) pairs is known as dataset ($X$, $Y$).\n:::\n\n::: {.callout-note}\nInputs can be virtually anything, including images, texts, video, audio, electrical signals, etc.\n\nWhile outputs are expected to be some meaningful piece of information, such as a category, position, value, etc.\n:::\n\n---\n\n## Use case: Image classification with the CIFAR-100 dataset\n\n\n\n- [ ] Load the CIFAR-100 dataset from torchvision.datasets\n\n::: {#da840dd9 .cell execution_count=2}\n``` {.python .cell-code}\nimport torch\nimport torchvision\n\ncifar_ds = torchvision.datasets.CIFAR100(root=\"/tmp\", train=True, download=True)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFiles already downloaded and verified\n```\n:::\n:::\n\n\n- [ ] Explore the CIFAR-100 dataset\n\n::: {#6d212c36 .cell execution_count=3}\n``` {.python .cell-code}\nx_im, y = cifar_ds[0]\n\nlen(cifar_ds), type(x_im), type(y)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n(50000, PIL.Image.Image, int)\n```\n:::\n:::\n\n\n::: {#d8892205 .cell execution_count=4}\n\n::: {.cell-output .cell-output-stdout}\n```\ny = 19 (cattle)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](DL_image_analysis_1_1_files/figure-revealjs/cell-5-output-2.png){}\n:::\n:::\n\n\n# Introduction to PyTorch\n\n## What is a tensor (PyTorch)?\n\nA tensor is a multi-dimensional array. In PyTorch, this comes from a generalization of the notation of variables that exists on more than two dimensions.\n\n*   zero-dimensional variables are points,\n*   one-dimensional variables are vectors,\n*   two-dimensional variables are matrices,\n*   and three or more dimensional variables, are tensors.\n\n::: {#9cfb3025 .cell execution_count=5}\n``` {.python .cell-code}\nimport torch\n\nx0 = torch.Tensor([7]) # This is a point\n\nx1 = torch.Tensor([15, 64, 123]) # This is a vector\n\nx2 = torch.Tensor([[3, 6, 5],\n                   [7, 9, 12],\n                   [10, 33, 1]]) # This is a matrix\n\nx3 = torch.Tensor([[[[1, 0, 0],\n                     [0, 1, 0],\n                     [0, 0, 1]],\n                    [[2, 0, 1],\n                     [0, 2, 3],\n                     [4, 1, 5]]]]) # This is a tensor\n```\n:::\n\n\n---\n\n- [ ] Convert the example image `x_im` to a PyTorch tensor, and cast it to floating point data type\n\n::: {.callout-tip}\nWe can use the utilities in `torchvision` to convert an image from PIL to tensor\n:::\n\n::: {#2e6a9878 .cell execution_count=6}\n``` {.python .cell-code}\nfrom torchvision.transforms.v2 import PILToTensor\n\npre_process = PILToTensor()\n\nx = pre_process(x_im)\n\nx = x.float()\n\ntype(x), x.shape, x.dtype, x.min(), x.max()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n(torch.Tensor,\n torch.Size([3, 32, 32]),\n torch.float32,\n tensor(1.),\n tensor(255.))\n```\n:::\n:::\n\n\n::: {.callout-note}\nFor convenience, PyTorch's tensors have their channels axis before the spatial axes.\n:::\n\n---\n\n- [ ] Create a composed transformation to carry out the conversion, casting to float, and rescaling to $[0, 1]$ range in the same function.\n\n::: {#46a616d6 .cell execution_count=7}\n``` {.python .cell-code}\nfrom torchvision.transforms.v2 import Compose, PILToTensor, ToDtype\n\npre_process = Compose([\n  PILToTensor(),\n  ToDtype(torch.float32, scale=True)\n])\n\nx = pre_process(x_im)\n\ntype(x), x.shape, x.dtype, x.min(), x.max()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n(torch.Tensor,\n torch.Size([3, 32, 32]),\n torch.float32,\n tensor(0.0039),\n tensor(1.))\n```\n:::\n:::\n\n\n::: {.callout-note}\nFor convenience, PyTorch's tensors have their channels axis before the spatial axes.\n:::\n\n---\n\n## Exercise: Add the preprocessing pipeline to the CIFAR-100 dataset\n\n- [ ] Re-load the CIFAR-100 dataset, this time passing the `pre_process` function as argument.\n\n::: {#5d3ce07c .cell execution_count=8}\n``` {.python .cell-code}\ncifar_ds = torchvision.datasets.CIFAR100(root=\"/tmp\", train=True, download=True, transform=pre_process)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFiles already downloaded and verified\n```\n:::\n:::\n\n\n# Training, Validation, and Test data\n\n---\n\n## Training set\n\nThe examples ($x$, $y$) used to teach a machine/model to perform a task\n\n---\n\n## Validation set\n\nUsed to measure the performance of a model during training\n\nThis subset is not used for training the model, so it is *unseen* data.\n\n::: {.notes}\nThis is a subset from the *training set* and can be used to test the generalization capacity of the model or to select the best configuration of a model.\n:::\n\n---\n\n## Test set\n\nThis set of samples is **not** used when training\n\nIts purpose is to measure the *generalization* capacity of the model\n\n---\n\n## Exercise: Load the test set and split the train set into train and validation subsets\n\n- [ ] Load the CIFAR-100 test set\n\n::: {#cf05ddb2 .cell execution_count=9}\n``` {.python .cell-code}\ncifar_test_ds = torchvision.datasets.CIFAR100(root=\"/tmp\", train=False, download=True, transform=pre_process)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFiles already downloaded and verified\n```\n:::\n:::\n\n\n- [ ] Split the training set into train and validation subsets\n\n::: {#50bf1c4a .cell execution_count=10}\n``` {.python .cell-code}\nfrom torch.utils.data import random_split\n\ncifar_train_ds, cifar_val_ds = random_split(cifar_ds, (40_000, 10_000))\n```\n:::\n\n\n# **D**eep **L**earning (**DL**) models\n\n---\n\n## Deep Learning (DL) models\n\nModels that construct knowledge in a hierarchical manner are considered **deep models**.\n\n![<a href=https://www.mdpi.com/595982>From Cervantes-Sanchez et al.</a>](https://www.mdpi.com/applsci/applsci-09-05507/article_deploy/html/images/applsci-09-05507-g003-550.jpg){height=50%}\n\n---\n\n## Exercise: Create a Logisic Regression model with PyTorch {.scrollable}\n\n- [ ] Use the `nn` (Neural Networks) module from `pytorch` to create a Logistic Regression model\n\n::: {#33f1c1b1 .cell execution_count=11}\n``` {.python .cell-code}\nimport torch.nn as nn\n\nlr_clf_1 = nn.Linear(in_features=3 * 32 * 32, out_features=100, bias=True)\nlr_clf_2 = nn.Softmax()\n```\n:::\n\n\n- [ ] *Feed* the model with a sample `x`\n\n::: {.callout-important}\nWe have to *reshape* `x` before feeding it to the model because `x` is an image with axes: Channels, Height, Width (CHW), but the Logistic Regression input should be a vector.\n:::\n\n::: {#a439b930 .cell execution_count=12}\n``` {.python .cell-code}\ny_hat = lr_clf_2( lr_clf_1( x.reshape(1, -1) ))\n\ntype(y_hat), y_hat.shape, y_hat.dtype\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n(torch.Tensor, torch.Size([1, 100]), torch.float32)\n```\n:::\n:::\n\n\n---\n\n## Exercise: Create a MultiLayer Perceptron (MLP) model with PyTorch {.scrollable}\n\n- [ ] Use the `nn.Sequential` module to build sequential models\n\n::: {#aa4e0300 .cell execution_count=13}\n``` {.python .cell-code}\nmlp_clf = nn.Sequential(\n  nn.Linear(in_features=3 * 32 * 32, out_features=1024, bias=True),\n  nn.Tanh(),\n  nn.Linear(in_features=1024, out_features=100, bias=True),\n  nn.Softmax()\n)\n```\n:::\n\n\n- [ ] *Feed* the model with a sample `x`\n\n::: {#9da7f2bd .cell execution_count=14}\n``` {.python .cell-code}\ny_hat = mlp_clf(x.reshape(1, -1))\n\ntype(y_hat), y_hat.shape, y_hat.dtype\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n(torch.Tensor, torch.Size([1, 100]), torch.float32)\n```\n:::\n:::\n\n\n# Model optimization\n\n---\n\n## Model fitting/training\n\nModels behavior depends directly on the value of their set of parameters $\\theta$.\n\n* $f(x) \\approx y$\n* $f_\\theta(x) = y + \\epsilon = \\hat{y}$\n\n::: {.callout-note}\nAs models increase their number of parameters, they become more *complex*\n:::\n\n**Training** is the process of optimizing the values of $\\theta$\n\n::: {.notes}\nTraining is often an expensive process in terms of computational resources and time.\n:::\n\n# Loss function\n\n---\n\n## Loss function\n\nThis is measure of the difference between the expected outputs and the predictions made by a model $L(Y, \\hat{Y})$.\n\n::: {.callout-note}\nWe look for *smooth* loss functions for which we can compute their gradient\n:::\n\n---\n\n## 11.1 Loss function for regression\n\nIn the case of regression tasks we generally use the Mean Squared Error (MSE).\n\n$MSE=\\frac{1}{N}\\sum \\left(Y - \\hat{Y}\\right)^2$\n\n---\n\n## Loss function for classification\n\nAnd for classification tasks we use the **C**ross **E**ntropy (**CE**) function.\n\n$CE = -\\frac{1}{N}\\sum\\limits_i^N\\sum\\limits_k^C y_{i,k} log(\\hat{y_{i,k}})$\n\nwhere $C$ is the number of classes.\n\n::: {.callout-note}\nFor the binary classification case:\n\n$BCE = -\\frac{1}{N}\\sum\\limits_i^N \\left(y_i log(\\hat{y_i}) + (1 - y_i) log(1 - \\hat{y_i})\\right)$\n:::\n\n---\n\n## Exercise: Define the loss function for the CIFAR-100 classification problem\n\n- [ ] Define a Cross Entropy loss function with `nn.CrossEntropyLoss`\n\n::: {#84c34c8c .cell execution_count=15}\n``` {.python .cell-code}\nloss_fun = nn.CrossEntropyLoss()\n```\n:::\n\n\n- [ ] Remove the `nn.Softmax` layer from the MLP model.\n\n::: {.callout-note}\nAccording to the PyTorch documentation, the CrossEntropyLoss function takes as inputs the *logits* of the probabilities and not the probabilities themselves.\nSo, we don't need to *squash* the output of the MLP model.\n:::\n\n::: {#964186ad .cell execution_count=16}\n``` {.python .cell-code}\nmlp_clf = nn.Sequential(\n  nn.Linear(in_features=3 * 32 * 32, out_features=1024, bias=True),\n  nn.Tanh(),\n  nn.Linear(in_features=1024, out_features=100, bias=True),\n  # nn.Softmax() # <- remove this line\n)\n```\n:::\n\n\n---\n\n## Exercise: Define the loss function for the CIFAR-100 classification problem\n\n- [ ] Measure the prediction loss (error) of our MLP with respect to the grund-truth\n\n::: {.callout-important}\nWe are using a PyTorch loss function, and it expects PyTorch's tensors as arguments, so we have to convert `y` to tensor before computing the loss function.\n:::\n\n::: {#4f884333 .cell execution_count=17}\n``` {.python .cell-code}\nloss = loss_fun(y_hat, torch.LongTensor([y]))\n\nloss\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\ntensor(4.6085, grad_fn=<NllLossBackward0>)\n```\n:::\n:::\n\n\n# Gradient based optimization\n\n---\n\n## Gradient based optimization\n\n*Gradient*-based methods are able to fit large numbers of parameters when using a *smooth* Loss function as target.\n\n::: {.callout-note}\nWe compute the gradient of the loss function with respect to the model parameters using the chain rule from calculous.\nGenerally, this is managed by the machine learning packages such as PyTorch and Tensorflow with a method called *back propagation*.\n:::\n\n### **Gradient Descent**\n\n* $\\theta^{t+1} = \\theta^t - \\eta \\nabla_\\theta L(Y, \\hat{Y})$\n\n---\n\n## Exercise: Compute the gradient of the loss function with respect to the parameters of the MLP.\n\n- [ ] Check what are the gradients of the MLP parameters before back propagating the gradient.\n\n::: {#fb162ff6 .cell execution_count=18}\n``` {.python .cell-code}\nmlp_clf[0].bias.grad\n```\n:::\n\n\n- [ ] Compute the gradient of the loss function with respect to the MLP parameters.\n\n::: {.callout-note}\nTo *back propagate* the gradients we use the `loss.backward()` method of the loss function.\n:::\n\n::: {#efc79a21 .cell execution_count=19}\n``` {.python .cell-code}\nloss = loss_fun(y_hat, torch.LongTensor([y]))\n\nloss.backward()\n```\n:::\n\n\n- [ ] Verify that the gradients have been propagated to the model parameters.\n\n::: {#738c6c0b .cell execution_count=20}\n``` {.python .cell-code}\nmlp_clf[0].bias.grad\n```\n:::\n\n\n---\n\n## Stochastic methods\n\n::: {.callout-caution}\nThe Gradient descent method require to obtain the Loss function for the whole training set before doing a single update.\n\nThis can be inefficient when large volumes of data are used for training the model.\n:::\n\n* These methods use a relative small sample from the training data called *mini-batch* at a time.\n\n* This reduces the amount of memory used for computing intermediate operations carried out during optimization process.\n\n---\n\n## **S**tochastic **G**radient **D**escent (**SGD**)\n\n::: {.notes}\nThis strategy defines $\\theta$'s' update rule for iteration $t+1$ using a *mini-batch* sampled at random from the training set as follows.\n:::\n\n* $\\theta^{t+1} = \\theta^t - \\eta \\nabla_\\theta L(Y_{b}, \\hat{Y_{b}})$\n\n* $\\eta$ controls the update we perform on the current parameter's values\n\n::: {.callout-note}\nThis parameter in Deep Learning is known as the **learning rate**\n:::\n\n---\n\n## Training with mini-batches\n\n::: {.callout-note}\nPyTorch can operate efficiently on multiple inputs at the same time.\nTo do that, we can use a `DataLoader` to serve mini-batches of inputs.\n:::\n\n---\n\n## Exercise: Train the MLP classifier\n\n- [ ] Use a `DataLoader` to serve mini-batches of images to train our MLP.\n\n::: {#7ac55684 .cell execution_count=21}\n``` {.python .cell-code}\nfrom torch.utils.data import DataLoader\n\ncifar_train_dl = DataLoader(cifar_train_ds, batch_size=128, shuffle=True)\ncifar_val_dl = DataLoader(cifar_val_ds, batch_size=256)\ncifar_test_dl = DataLoader(cifar_test_ds, batch_size=256)\n```\n:::\n\n\n- [ ] Create a Stochastic Gradient Descent optimizer for our MLP classifier.\n\n::: {#af972269 .cell execution_count=22}\n``` {.python .cell-code}\nimport torch.optim as optim\n\noptimizer = optim.SGD(mlp_clf.parameters(), lr=0.01, )\n```\n:::\n\n\n---\n\n## Exercise: Train the MLP classifier {.scrollable}\n\n- [ ] Implement the *training-loop* to fit the parameters of our MLP classifier.\n\n::: {.callout-note}\nGradients are accumulated on every iteration, so we need to reset the accumulator with `optimizer.zero_grad()` for every new batch.\n:::\n\n::: {.callout-note}\nTo perform get the new iteration's parameter values $\\theta^{t+1}$ we use `optimizer.step()` to compute the update step.\n:::\n\n```{.python code-line-numbers=\"|3|9|11\"}\nmlp_clf.train()\nfor x, y in cifar_train_dl:\n  optimizer.zero_grad()\n\n  y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) ) # Reshape it into a batch of vectors\n\n  loss = loss_fun(y_hat, y)\n\n  loss.backward()\n\n  optimizer.step()\n```\n\n\n\n---\n\n## Exercise: Train the MLP classifier and track the training and validation loss\n\n- [ ] Save the loss function of each batch and the overall average loss during training.\n\n::: {.callout-note}\nTo extract the loss function's value without anything else attached use `loss.item()`.\n:::\n\n```{.python code-line-numbers=\"1-3,13-15,21\"}\ntrain_loss = []\ntrain_loss_avg = 0\ntotal_train_samples = 0\n\nmlp_clf.train()\nfor x, y in cifar_train_dl:\n  optimizer.zero_grad()\n\n  y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) ) # Reshape it into a batch of vectors\n\n  loss = loss_fun(y_hat, y)\n\n  train_loss.append(loss.item())\n  train_loss_avg += loss.item() * len(x)\n  total_train_samples += len(x)\n\n  loss.backward()\n\n  optimizer.step()\n\ntrain_loss_avg /= total_train_samples\n```\n\n\n\n---\n\n## Exercise: Train the MLP classifier and track the training and validation loss\n- [ ] Compute the average loss function for the validation set.\n\n::: {.callout-note}\nBecause we don't train the model with the validation set, back-propagation and optimization steps are not needed.\n\nAdditionally, we wrap the loop `with torch.no_grad()` to prevent the generation of gradients that could fill the memory unnecessarily.\n:::\n\n``` {.python code-line-numbers=\"|5\"}\nval_loss_avg = 0\ntotal_val_samples = 0\n\nmlp_clf.eval()\nwith torch.no_grad():\n  for x, y in cifar_val_dl:\n    y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) ) # Reshape it into a batch of vectors\n    loss = loss_fun(y_hat, y)\n\n    val_loss_avg += loss.item() * len(x)\n    total_val_samples += len(x)\n\nval_loss_avg /= total_val_samples\n```\n\n\n\n---\n\n## Exercise: Train the MLP classifier and track the training and validation loss\n\n- [ ] Plot the training loss for this *epoch*.\n\n::: {#fb87e26a .cell execution_count=26}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nplt.plot(train_loss, \"b-\", label=\"Training loss\")\nplt.plot([0, len(train_loss)], [train_loss_avg, train_loss_avg], \"r:\", label=\"Average training loss\")\nplt.plot([0, len(train_loss)], [val_loss_avg, val_loss_avg], \"b:\", label=\"Average validation loss\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](DL_image_analysis_1_1_files/figure-revealjs/cell-27-output-1.png){width=802 height=411}\n:::\n:::\n\n\n---\n\n## Exercise: Train the MLP classifier and track the training and validation loss through several *epochs* {.scrollable}\n\n```{.python code-line-numbers=\"1-5|\"}\nnum_epochs = 10\ntrain_loss = []\nval_loss = []\n\nfor e in range(num_epochs):\n  train_loss_avg = 0\n  total_train_samples = 0\n\n  mlp_clf.train()\n  for x, y in cifar_train_dl:\n    optimizer.zero_grad()\n\n    y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) ) # Reshape it into a batch of vectors\n\n    loss = loss_fun(y_hat, y)\n\n    train_loss_avg += loss.item() * len(x)\n    total_train_samples += len(x)\n\n    loss.backward()\n\n    optimizer.step()\n\n  train_loss_avg /= total_train_samples\n  train_loss.append(train_loss_avg)\n\n  val_loss_avg = 0\n  total_val_samples = 0\n\n  mlp_clf.eval()\n  with torch.no_grad():\n    for x, y in cifar_val_dl:\n      y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) ) # Reshape it into a batch of vectors\n      loss = loss_fun(y_hat, y)\n\n      val_loss_avg += loss.item() * len(x)\n      total_val_samples += len(x)\n\n  val_loss_avg /= total_val_samples\n  val_loss.append(val_loss_avg)\n```\n\n\n\n---\n\n## Exercise: Show the progress of the training throughout the epochs\n\n- [ ] Plot the average train and validation losses\n\n::: {#08f15458 .cell execution_count=28}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nplt.plot(train_loss, \"b-\", label=\"Average training loss\")\nplt.plot(val_loss, \"r-\", label=\"Average validation loss\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](DL_image_analysis_1_1_files/figure-revealjs/cell-29-output-1.png){width=802 height=411}\n:::\n:::\n\n\n# Performance metrics\n\n---\n\n## Performance metrics\n\nUsed to measure how good or bad a model carries out a task\n\n* $f(x) \\approx y$\n\n* $f(x) = y + \\epsilon = \\hat{y}$\n\n::: {.notes}\nGiven the set of parameters, as well as other factors, the output of a model can deviate from the expected outcome. So, the actual output of a model is $f(x) = \\hat{y}$.\n:::\n\n::: {.callout-note}\nThe output $\\hat{y}$ is called **prediction ** given the context taken from statistical regression analysis.\n:::\n\n::: {.callout-important}\nSelecting the correct performance metrics depends on the training type, task, and even the distribution of the data.\n:::\n\n---\n\n## Exercise: Measure the accuracy of the MLP trained to classify images from CIFAR-100 {.scrollable}\n\n- [ ] Install the `torchmetrics` package.\n\n``` {.python}\n!pip install torchmetrics\n```\n\n- [ ] Compute the average accuracy for the Train set.\n\n::: {#97a5daf4 .cell execution_count=29}\n``` {.python .cell-code}\nfrom torchmetrics.classification import Accuracy\n\nmlp_clf.eval()\n\ntrain_acc_metric = Accuracy(task=\"multiclass\", num_classes=100)\n\nwith torch.no_grad():\n  for x, y in cifar_train_dl:\n    y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) )\n    train_acc_metric(y_hat.softmax(dim=1), y)\n\n  train_acc = train_acc_metric.compute()\n\nprint(f\"Training acc={train_acc}\")\ntrain_acc_metric.reset()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining acc=0.12927499413490295\n```\n:::\n:::\n\n\n---\n\n## Exercise: Measure the accuracy of the MLP trained to classify images from CIFAR-100 {.scrollable}\n\n- [ ] Compute the average accuracy for the Validation and Test sets.\n\n::: {#d03fff04 .cell execution_count=30}\n``` {.python .cell-code}\nval_acc_metric = Accuracy(task=\"multiclass\", num_classes=100)\ntest_acc_metric = Accuracy(task=\"multiclass\", num_classes=100)\n\nwith torch.no_grad():\n  for x, y in cifar_val_dl:\n    y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) )\n    val_acc_metric(y_hat.softmax(dim=1), y)\n\n  val_acc = val_acc_metric.compute()\n\n  for x, y in cifar_test_dl:\n    y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) )\n    test_acc_metric(y_hat.softmax(dim=1), y)\n\n  test_acc = test_acc_metric.compute()\n\nprint(f\"Validation acc={val_acc}\")\nprint(f\"Test acc={test_acc}\")\n\nval_acc_metric.reset()\ntest_acc_metric.reset()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValidation acc=0.125\nTest acc=0.12290000170469284\n```\n:::\n:::\n\n\n# **C**onvolutional **N**eural **N**etwork (**CNN** or **ConvNet**)\n\n---\n\n## Convolution layers\n\nThe most common operation in DL models for image processing are Convolution operations.\n\n![2D Convolution](https://upload.wikimedia.org/wikipedia/commons/8/85/Convolution_arithmetic_-_Full_padding_no_strides_transposed.gif)\n\nThe animation shows the convolution of a 7x7 pixels input image (bottom) with a 3x3 pixels kernel (moving window), that results in a 5x5 pixels output (top).\n\n---\n\n## Exercise: Visualize the effect of the convolution operation\n\n- [ ] Create a convolution layer with `nn.Conv2D` using 3 channels as input, and a single one for output.\n\n::: {#373d671d .cell execution_count=31}\n``` {.python .cell-code}\nconv_1 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=7, padding=0, bias=True)\n\nx, _ = next(iter(cifar_train_dl))\n\nfx = conv_1(x)\n\ntype(fx), fx.dtype, fx.shape, fx.min(), fx.max()\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\n(torch.Tensor,\n torch.float32,\n torch.Size([128, 1, 26, 26]),\n tensor(-0.1479, grad_fn=<MinBackward1>),\n tensor(1.0583, grad_fn=<MaxBackward1>))\n```\n:::\n:::\n\n\n::: {.callout-warning}\nThe convolution layer is initialized with random values, so the results will vary.\n:::\n\n---\n\n## Exercise: Visualize the effect of the convolution operation\n\n- [ ] Create a convolution layer with `nn.Conv2D` using 3 channels as input, and a single one for output.\n\n::: {#482bec97 .cell execution_count=32}\n``` {.python .cell-code}\nplt.rcParams['figure.figsize'] = [5, 5]\n\nfig, ax = plt.subplots(1, 2)\nax[0].imshow(x[0].permute(1, 2, 0))\nax[1].imshow(fx.detach()[0, 0], cmap=\"gray\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](DL_image_analysis_1_1_files/figure-revealjs/cell-33-output-1.png){width=418 height=213}\n:::\n:::\n\n\n::: {.callout-important}\nBy default, outputs from PyTorch modules are tracked for back-propagation.\n\nTo visualize it with `matplotlib` we have to `.detach()` the tensor first.\n:::\n\n---\n\n## Exercise: Visualize the effect of the convolution operation\n\n- [ ] Visualize the weights of the convolution layer.\n\n::: {#6f50a2cf .cell execution_count=33}\n``` {.python .cell-code}\nconv_1.weight.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```\ntorch.Size([1, 3, 7, 7])\n```\n:::\n:::\n\n\n::: {#5b85c327 .cell execution_count=34}\n``` {.python .cell-code}\nfig, ax = plt.subplots(2, 2)\nax[0, 0].imshow(conv_1.weight.detach()[0, 0], cmap=\"gray\")\nax[0, 1].imshow(conv_1.weight.detach()[0, 1], cmap=\"gray\")\nax[1, 0].imshow(conv_1.weight.detach()[0, 2], cmap=\"gray\")\nax[1, 1].set_axis_off()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](DL_image_analysis_1_1_files/figure-revealjs/cell-35-output-1.png){width=408 height=411}\n:::\n:::\n\n\n---\n\n## Exercise: Visualize the effect of the convolution operation\n\n- [ ] Modify the weights of the convolution layer.\n\n::: {#846889e5 .cell execution_count=35}\n``` {.python .cell-code}\nconv_1 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, padding=0, bias=False)\n\nconv_1.weight.data[:] = torch.FloatTensor([\n  [\n    [\n      [0, 0, 0],\n      [0, 0, 0],\n      [0, 0, 0],\n    ],\n    [\n      [0, 0, 0],\n      [0, 1, 0],\n      [0, 0, 0],\n    ],\n    [\n      [0, 0, 0],\n      [0, 0, 0],\n      [0, 0, 0],\n    ],\n  ]\n])\n```\n:::\n\n\n---\n\n## Exercise: Visualize the effect of the convolution operation\n\n- [ ] Visualize the effects after changing the values.\n\n::: {#7cc57205 .cell execution_count=36}\n``` {.python .cell-code}\nfx = conv_1(x)\n\nfig, ax = plt.subplots(1, 2)\nax[0].imshow(x[0].permute(1, 2, 0))\nax[1].imshow(fx.detach()[0].permute(1, 2, 0))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](DL_image_analysis_1_1_files/figure-revealjs/cell-37-output-1.png){width=418 height=213}\n:::\n:::\n\n\nExperiment with different values and shapes of the kernel\nhttps://en.wikipedia.org/wiki/Kernel_(image_processing)\n\n---\n\n## Exercise: Visualize the effect of the convolution operation\n\n- [ ] Modify the weights of the convolution layer.\n\n::: {#f1fe3b71 .cell execution_count=37}\n``` {.python .cell-code}\nconv_1 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, padding=0, bias=False)\n\nconv_1.weight.data[:] = torch.FloatTensor([\n  [[[0, -1, 0], [-1, 5, -1], [0, -1, 0]],\n   [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n   [[0, 0, 0], [0, 0, 0], [0, 0, 0]]]\n])\n\nfx = conv_1(x)\n\nfig, ax = plt.subplots(1, 2)\nax[0].imshow(x[0].permute(1, 2, 0))\nax[1].imshow(fx.detach()[0, 0], cmap=\"gray\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](DL_image_analysis_1_1_files/figure-revealjs/cell-38-output-1.png){width=418 height=213}\n:::\n:::\n\n\nExperiment with different values and shapes of the kernel\nhttps://en.wikipedia.org/wiki/Kernel_(image_processing)\n\n---\n\n## Exercise: Visualize the effect of the convolution operation\n\n- [ ] Modify the weights of the convolution layer.\n\n::: {#717b878c .cell execution_count=38}\n``` {.python .cell-code}\nconv_1 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, padding=0, bias=False)\n\nconv_1.weight.data[:] = torch.FloatTensor([\n  [[[1, 0, -1], [1, 0, -1], [1, 0, -1]],\n   [[1, 0, -1], [1, 0, -1], [1, 0, -1]],\n   [[1, 0, -1], [1, 0, -1], [1, 0, -1]]]\n])\n\nfx = conv_1(x)\n\nfig, ax = plt.subplots(1, 2)\nax[0].imshow(x[0].permute(1, 2, 0))\nax[1].imshow(fx.detach()[0, 0], cmap=\"gray\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](DL_image_analysis_1_1_files/figure-revealjs/cell-39-output-1.png){width=418 height=213}\n:::\n:::\n\n\nExperiment with different values and shapes of the kernel\nhttps://en.wikipedia.org/wiki/Kernel_(image_processing)\n\n---\n\n## Examples of popular Deep Learning models in computer vision\n\n* Inception v3 for image classification\n\n![InceptionV3](https://cloud.google.com/static/tpu/docs/images/inceptionv3onc--oview.png)\n\n---\n\n## Examples of popular Deep Learning models in computer vision\n\n* U-Net for cell segmentation\n\n![U-Net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)\n\n---\n\n## Examples of popular Deep Learning models in computer vision\n\n* LeNet-5 for handwritten digits classification (<a href=http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf>LeCun et al.</a>)\n\n![LeNet-5](https://upload.wikimedia.org/wikipedia/commons/6/61/LeNet_architecture.png)\nBy Daniel Voigt Godoy - <a rel=\"nofollow\" class=\"external free\" href=\"https://github.com/dvgodoy/dl-visuals/\">https://github.com/dvgodoy/dl-visuals/</a>, <a href=\"https://creativecommons.org/licenses/by/4.0\" title=\"Creative Commons Attribution 4.0\">CC BY 4.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=150820922\">Link</a>\n\n---\n\n## Exercise: Implement and train the LetNet-5 model with PyTorch\n\n- [ ] Build the convolutional neural network using `nn.Sequential`, and the `nn.ReLU()` activation function.\n\n::: {#fdee84a5 .cell execution_count=39}\n``` {.python .cell-code}\nlenet_clf = nn.Sequential(\n    nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, bias=True),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2),\n    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, bias=True),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2),\n    nn.Flatten(),\n    nn.Linear(in_features=16*5*5, out_features=120, bias=True),\n    nn.ReLU(),\n    nn.Linear(in_features=120, out_features=84, bias=True),\n    nn.ReLU(),\n    nn.Linear(in_features=84, out_features=100, bias=True),\n)\n```\n:::\n\n\n::: {.callout-note}\nPooling layers are used to downsample feature maps to summarize information from large regions.\n:::\n\n---\n\n## Exercise: Implement and train the LetNet-5 model with PyTorch\n\n- [ ] Test our implementation.\n\n::: {#424d75d7 .cell execution_count=40}\n``` {.python .cell-code}\ny_hat = lenet_clf(x)\n\ntype(y_hat), y_hat.dtype, y_hat.shape, y_hat.min(), y_hat.max()\n```\n\n::: {.cell-output .cell-output-display execution_count=40}\n```\n(torch.Tensor,\n torch.float32,\n torch.Size([128, 100]),\n tensor(-0.1779, grad_fn=<MinBackward1>),\n tensor(0.1641, grad_fn=<MaxBackward1>))\n```\n:::\n:::\n\n\n---\n\n## Exercise: Implement and train the LetNet-5 model with PyTorch {.scrollable}\n\n- [ ] Train the model to classify images from CIFAR-100.\n\n::: {#4530663f .cell execution_count=41}\n``` {.python .cell-code}\nnum_epochs = 10\ntrain_loss = []\nval_loss = []\n\nif torch.cuda.is_available():\n  lenet_clf.cuda()\n\noptimizer = optim.SGD(lenet_clf.parameters(), lr=0.01)\n\nfor e in range(num_epochs):\n  train_loss_avg = 0\n  total_train_samples = 0\n\n  lenet_clf.train()\n  for x, y in cifar_train_dl:\n    optimizer.zero_grad()\n\n    if torch.cuda.is_available():\n      x = x.cuda()\n    \n    y_hat = lenet_clf( x ).cpu()\n\n    loss = loss_fun(y_hat, y)\n\n    train_loss_avg += loss.item() * len(x)\n    total_train_samples += len(x)\n\n    loss.backward()\n\n    optimizer.step()\n\n  train_loss_avg /= total_train_samples\n  train_loss.append(train_loss_avg)\n\n  val_loss_avg = 0\n  total_val_samples = 0\n\n  lenet_clf.eval()\n  with torch.no_grad():\n    for x, y in cifar_val_dl:\n      if torch.cuda.is_available():\n        x = x.cuda()\n      \n      y_hat = lenet_clf( x ).cpu()\n      loss = loss_fun(y_hat, y)\n\n      val_loss_avg += loss.item() * len(x)\n      total_val_samples += len(x)\n\n  val_loss_avg /= total_val_samples\n  val_loss.append(val_loss_avg)\n```\n:::\n\n\n---\n\n## Exercise: Implement and train the LetNet-5 model with PyTorch\n\n- [ ] Plot the average train and validation losses\n\n::: {#e36ac04b .cell execution_count=42}\n``` {.python .cell-code}\nplt.plot(train_loss, \"b-\", label=\"Average training loss\")\nplt.plot(val_loss, \"r-\", label=\"Average validation loss\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](DL_image_analysis_1_1_files/figure-revealjs/cell-43-output-1.png){width=439 height=415}\n:::\n:::\n\n\n---\n\n## Exercise: Implement and train the LetNet-5 model with PyTorch {.scrollable}\n\n- [ ] Compute the average accuracy for the Validation and Test sets.\n\n::: {#b7e0de03 .cell execution_count=43}\n``` {.python .cell-code}\nlenet_clf.eval()\n\nval_acc_metric = Accuracy(task=\"multiclass\", num_classes=100)\ntest_acc_metric = Accuracy(task=\"multiclass\", num_classes=100)\ntrain_acc_metric = Accuracy(task=\"multiclass\", num_classes=100)\n\nwith torch.no_grad():\n  for x, y in cifar_train_dl:\n    if torch.cuda.is_available():\n      x = x.cuda()\n    y_hat = lenet_clf( x ).cpu()\n    train_acc_metric(y_hat.softmax(dim=1), y)\n\n  train_acc = train_acc_metric.compute()\n\n  for x, y in cifar_val_dl:\n    if torch.cuda.is_available():\n      x = x.cuda()\n    y_hat = lenet_clf( x ).cpu()\n    val_acc_metric(y_hat.softmax(dim=1), y)\n\n  val_acc = val_acc_metric.compute()\n\n  for x, y in cifar_test_dl:\n    if torch.cuda.is_available():\n      x = x.cuda()\n    y_hat = lenet_clf( x ).cpu()\n    test_acc_metric(y_hat.softmax(dim=1), y)\n\n  test_acc = test_acc_metric.compute()\n\nprint(f\"Training acc={train_acc}\")\nprint(f\"Validation acc={val_acc}\")\nprint(f\"Test acc={test_acc}\")\n\ntrain_acc_metric.reset()\nval_acc_metric.reset()\ntest_acc_metric.reset()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining acc=0.02437499910593033\nValidation acc=0.020899999886751175\nTest acc=0.02250000089406967\n```\n:::\n:::\n\n\n",
    "supporting": [
      "DL_image_analysis_1_1_files"
    ],
    "filters": [],
    "includes": {}
  }
}