{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO6X7daVkYfSzFBZB+ERY8M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# 2.- Implement the training process for a DL model using an off-the-shelf PyTorch model"],"metadata":{"id":"9RBZP6J7H6O9"}},{"cell_type":"markdown","source":["## Load an off-the-shelf model"],"metadata":{"id":"rIOmDtmjir9b"}},{"cell_type":"markdown","source":["PyTorch provides several state-of-the-art models with their corresponding pre-trained paramaeters through the **torchvision** package"],"metadata":{"id":"lwRm9s5c5x2m"}},{"cell_type":"code","source":["import torchvision"],"metadata":{"id":"mkP2sFrF3EQh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Take an off-the-shelf and load its pre-trained parameters\n","model =\n","model.eval()"],"metadata":{"id":"Gz2pOQIpIhp6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To train a model from scratch, lets download some data and use it as our *training set*"],"metadata":{"id":"sAf_-i8vhm7Z"}},{"cell_type":"code","source":["#@title Install FiftyOne package to use open source image datasets\n","!pip install fiftyone \n","import fiftyone as fo\n","import fiftyone.zoo as foz"],"metadata":{"id":"jTVRT4iN7uun","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We'll train a DL model to identify cats in images. For that we gather some images with cats, and other images without cats.\n","That will make the model more robust when inferring if the image contains or not a cat "],"metadata":{"id":"QQZAw1tgh0FJ"}},{"cell_type":"code","source":["#@title Download some images from the Fiftyone dataset\n","dataset = foz.load_zoo_dataset(\n","    \"open-images-v7\",\n","    split=\"train\",\n","    label_types=[\"classifications\"],\n","    classes = [\"Cat\"],\n","    max_samples=1000,\n","    dataset_dir=\"sample_data\",\n","    download_if_necessary=True\n",")"],"metadata":{"id":"96_09EUs77Nf","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To load the images that we downloaded into python, we use a **Dataset**. This object will load each image and its corresponding label so we can use it to feed our model."],"metadata":{"id":"1HwqeJuMi0HE"}},{"cell_type":"code","source":["#@title This is a wrapper that allows us to use FiftyOne datasets with PyTorch\n","import matplotlib.pyplot as plt\n","import torch\n","from PIL import Image\n","\n","\n","class FiftyOneTorchDataset(torch.utils.data.Dataset):\n","    \"\"\"A class to construct a PyTorch dataset from a FiftyOne dataset.\n","    \n","    Args:\n","        fiftyone_dataset: a FiftyOne dataset or view that will be used for training or testing\n","        transforms (None): a list of PyTorch transforms to apply to images and targets when loading\n","        gt_field (\"ground_truth\"): the name of the field in fiftyone_dataset that contains the \n","            desired labels to load\n","        classes (None): a list of class strings that are used to define the mapping between\n","            class names and indices. If None, it will use all classes present in the given fiftyone_dataset.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        fiftyone_dataset,\n","        transforms=None,\n","        classes=None,\n","    ):\n","        self.samples = fiftyone_dataset\n","        self.transforms = transforms\n","        self.img_paths = self.samples.values(\"filepath\")\n","\n","        self.classes = classes\n","\n","    def __getitem__(self, idx):\n","        img_path = self.img_paths[idx]\n","        sample = self.samples[img_path]\n","        metadata = sample.metadata\n","        img = Image.open(img_path).convert(\"RGB\")\n","\n","        label = any(lab[\"label\"] in self.classes\n","                    for lab in sample[\"positive_labels\"][\"classifications\"])\n","        target = torch.as_tensor(label, dtype=torch.float32)\n","\n","        if self.transforms is not None:\n","            img = self.transforms(img)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def get_classes(self):\n","        return self.classes"],"metadata":{"id":"9FCVw5ul-y0r","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Explore the dataset"],"metadata":{"id":"eraiKeRsj5On"}},{"cell_type":"markdown","source":["Each image can have an arbitrary shape, so we resize all of them to have a standard shape of 299x299 pixels. This is the shape *Inception V3* expects as inputs."],"metadata":{"id":"cnhuw6eEjgQ3"}},{"cell_type":"code","source":["preprocess_fun = torchvision.transforms.Compose([\n","    torchvision.transforms.Resize((299, 299)),\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n"],"metadata":{"id":"bQtm1Jb2afPJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pt_dataset = FiftyOneTorchDataset(dataset,\n","                                  classes=[\"Cat\"],\n","                                  transforms=\n","                                  )"],"metadata":{"id":"aJje4HwGQQFk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x, l = pt_dataset[100]\n","x.shape"],"metadata":{"id":"WEF9oiXHMS8Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Show one example image\n","im = (x - x.min()) / (x.max() - x.min())\n","plt.imshow(im.permute(1, 2, 0))"],"metadata":{"id":"3G1vUCRoce4-","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xhiGG662o-hI"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["Check the label assigned by Inception to this image here https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/"],"metadata":{"id":"4X822ScCblME"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"vMozfC92eWPS"}},{"cell_type":"markdown","source":["## Train an off-the-shelf model from scratch\n","\n","In this exercise we'll train the Inception V3 model to idenfity cats.\n","This is a classification tasks, and we only need that the model returns 1 if a cat is present in an image, and 0 otherwise."],"metadata":{"id":"oafK-1OueXdq"}},{"cell_type":"code","source":["# Load an off-the-shelf model (Inception V3), without pre-trained parameters\n","model ="],"metadata":{"id":"9SApIu7slZtb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This time we use the *Binary Cross Entropy (BCE)* loss function because there are only two possible outcomes [0, 1].\n","\n","In PyTorch this function is implemented as **nn.BCEWithLogitsLoss**. The *WithLogits* part of the name means that PyTorch applies the appropriate transformations to the output of the model to map them into a [0, 1] response."],"metadata":{"id":"q0DecDjH5AXj"}},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.optim as optim\n","\n","from torch.utils.data import DataLoader"],"metadata":{"id":"tUayB8fheeVR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion = nn.BCEWithLogitsLoss()\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","trn_queue = DataLoader(pt_dataset, batch_size=16, shuffle=True, pin_memory=True)"],"metadata":{"id":"bR7ibwV5ecm0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Move the model to the GPU memory\n","model.train()\n","model.cuda()\n","\n","for e in range(10):\n","  for i, (x, y) in enumerate(trn_queue):\n","    # Empty the accumulated gradients from any previous iteration\n","    optimizer.zero_grad()\n","\n","    # Move the input images and their respective classes to the GPU\n","    x = x.cuda()\n","    y = y.cuda()\n","\n","    y_hat = model(x)\n","\n","    # Compute the error/loss function\n","    loss = criterion(y_hat.logits, y.view(-1, 1))\n","\n","    # Perform the backward pass to generate the gradients of the loss function with respect to the inputs\n","    loss.backward()\n","\n","    # Update the model parameters\n","    optimizer.step()\n","\n","    # Log the progress of the model\n","    if i % 10 == 0:\n","      acc = torch.sum(y == y_hat.logits.detach().argmax(dim=1)) / x.shape[0]\n","\n","      print(f\"Epoch {e}, step {i}: loss={loss.item()}, acc={acc}\")"],"metadata":{"id":"of8WycQseu8D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"H11lvVtH9ESk"},"execution_count":null,"outputs":[]}]}