---
jupyter:
  jupytext:
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python 3
    name: python3
---

<!-- #region id="aycU0VtozoFm" -->
# 1. Load pre-trained models

## Lets use one from the PyTorch's `torchvision` module for computer vision
<!-- #endregion -->

<!-- #region id="roBXfNY0ob_d" -->
Try first with the InceptionV3 model.
<!-- #endregion -->

<!-- #region id="QgZcQReZQPrO" -->
![InceptionV3](https://cloud.google.com/static/tpu/docs/images/inceptionv3onc--oview.png)
<!-- #endregion -->

<!-- #region id="azB6AQi8ZNxT" -->
âœ… *Import torchvision and check how to use the pre-trained Inception V3 from torchvision.models.inception_v3* *italicized text*
<!-- #endregion -->

```python id="3GVIprQanRVR"
import torch
```

```python id="oi1bhTOSlEcq"
import torchvision
```

```python id="V_SWxwWb0U-Y"
torchvision.models.inception_v3?
```

<!-- #region id="tym_TvlcpGxH" -->
A little more info about Inception V3 implementation in `torchvision` here: https://pytorch.org/vision/main/models/generated/torchvision.models.inception_v3.html
<!-- #endregion -->

<!-- #region id="H4Gas_l-ppko" -->
ðŸš§ Note that the pre-trained weights have to be passed as argument when creating the Deep Learning model.
<!-- #endregion -->

<!-- #region id="Ld1g7nozZz8a" -->
âœ… *Use the pre-trained parameters from torchvision.models.inception.Inception_V3_Weights.IMAGENET1K_V1*
<!-- #endregion -->

```python id="50wcgH7FD6bn"
inception_weights = torchvision.models.inception.Inception_V3_Weights.IMAGENET1K_V1
```

<!-- #region id="k2Rwl0_Vaa_1" -->
âœ… *Inspect the contents of the pre-trained weights object by accessing inception_weights.meta*
<!-- #endregion -->

```python colab={"base_uri": "https://localhost:8080/"} id="yR4wiZgxp3xJ" outputId="62ff4615-3550-49ce-8cd5-545172a084e9"
inception_weights.meta
```

<!-- #region id="z2UwyepXt1uV" -->
âœ… *Store the categoies in a variable to use them later*
<!-- #endregion -->

```python id="8m4Hw--6vtlY"
categories = inception_weights.meta["categories"]
```

<!-- #region id="U0Ns9NM1CSyh" -->
âœ… *Load a pre-trained Inception V3 model from the torchvision.models.inception_v3 and set it to evaluation mode with .eval()*
<!-- #endregion -->

```python id="X4bPfu3_zycs" colab={"base_uri": "https://localhost:8080/"} outputId="66fe43ba-e0e7-4bd8-ee07-0df2b8507d97"
dl_model = torchvision.models.inception_v3(inception_weights, progress=True)
```

```python colab={"base_uri": "https://localhost:8080/"} id="GlUa3Grqz1RL" outputId="a2211e00-9ae1-469f-9785-b6cfd8629df5"
dl_model.eval()
```

<!-- #region id="Gbsse761fiOR" -->


---


<!-- #endregion -->

<!-- #region id="J2EyyRdyDavE" -->
# 2. Using pre-trained models for inference
<!-- #endregion -->

<!-- #region id="463oGySJnhuI" -->
âœ… *Load a sample image and predict its class. Use an exisiting image from skimage.data as example.*
<!-- #endregion -->

```python id="8Wum_OD5V23v"
import skimage
import matplotlib.pyplot as plt
```

```python colab={"base_uri": "https://localhost:8080/"} id="JgcycHGvruVl" outputId="7464a458-046a-43dc-a8bd-9fefbc73fc25"
sample_im = skimage.data.rocket()
sample_im.shape
```

```python colab={"base_uri": "https://localhost:8080/", "height": 467} id="_pgkcklCsIkY" outputId="671e12e0-93c8-4128-95a2-9e70e9e0253b"
sample_im
```

<!-- #region id="zrgIS7kbDiBN" -->


---


<!-- #endregion -->

<!-- #region id="jvKCRNqTqxxe" -->
## What transforms this model requires to be applied to the input data to work correctly?
<!-- #endregion -->

<!-- #region id="WdHi27exDlDc" -->
âœ… *Inspect the .transforms member to check what transforms are required by the Inception V3 model*
<!-- #endregion -->

```python colab={"base_uri": "https://localhost:8080/"} id="u_lh64LzuFyq" outputId="1e021ea9-ef54-4f40-8273-6085c87128da"
inception_weights.transforms
```

<!-- #region id="6A-tWtixuWmz" -->
ðŸš§ `functools.partial` is a function to define functions with static arguments. So ðŸ‘† returns a function when it is called!!
<!-- #endregion -->

<!-- #region id="fg8tppRIDvYl" -->
âœ… *Define those transforms as a preprocessing pipeline by calling the inception_weights.transforms() method*
<!-- #endregion -->

```python id="IdKukiSeuEm6" colab={"base_uri": "https://localhost:8080/"} outputId="1ff06cb5-201d-4bcc-96c0-810c582d109b"
pipeline = inception_weights.transforms()
pipeline
```

<!-- #region id="NvdgLV4xlkq3" -->
â„¹ The transforms used by the Inception V3 are 1) resize the image to 342 x 342 pixels, 2) crop the center 299x299 pixels window, 3) normalize the RGB channels.
<!-- #endregion -->

<!-- #region id="wmv5sb0eFZSw" -->
âœ… *Convert our image from `numpy.ndarray` into a `torch.tensor` and reorder its axes as Channels, Height, Width (HWC -> CHW)*

<!-- #endregion -->

```python id="i4nZpfYsvAa0" colab={"base_uri": "https://localhost:8080/"} outputId="89b5eedc-bfb8-4d36-99ac-4040d6c046a2"
sample_im_pt = torch.from_numpy(sample_im.transpose(2, 0, 1))
sample_im_pt.shape
```

<!-- #region id="tHXp-am2uoWE" -->
âœ… *Use `pipeline` to normalize, resize, and crop our tensors into one the model understands (i.e. similar to the training data used for training the model originally).*

<!-- #endregion -->

```python id="JaGfPy5WrzIK" colab={"base_uri": "https://localhost:8080/"} outputId="5760ee2a-195a-4edd-f218-4b40ff54fd73"
sample_x = pipeline(sample_im_pt)
sample_x.shape
```

<!-- #region id="HxAG7mKdfjm5" -->


---


<!-- #endregion -->

<!-- #region id="8RJ81s4yrYI-" -->
# 3. Use the pre-trained model to predict the class of our sample image
<!-- #endregion -->

<!-- #region id="S9ksAhKRrc8_" -->
ðŸš§ PyTorch modules expect the data be fed as *batches*.

That is becase it can efficiently apply the prediction workflow to several images at once.

For now we can simply treat our image as a batch of one image.
<!-- #endregion -->

<!-- #region id="UwsnQMY_FvNJ" -->
âœ… *Apply the model on sample_x[None, ...], so it is treated as a one-sample batch*
<!-- #endregion -->

```python id="WHuO2RrmssiA"
sample_y = dl_model(sample_x[None, ...])
```

<!-- #region id="3MihgQfrr8hX" -->
âœ… *Inspect the output of the model*

â„¹ The model's output are the log-probabilities of `sample_x` belonging to each of the 1000 classes.
<!-- #endregion -->

```python colab={"base_uri": "https://localhost:8080/"} id="BRbX7lBmsyQf" outputId="4c481a5f-563c-478e-9af7-36c0cbd8fb72"
sample_y.shape
```

```python colab={"base_uri": "https://localhost:8080/"} id="iPh_3BvOr4d_" outputId="ffda2760-ed05-46bc-a173-60efb40f3c70"
sample_y
```

```python colab={"base_uri": "https://localhost:8080/"} id="X9P4lYnRsz-v" outputId="5a308c5a-9b85-4ef0-8a01-bbb5d838da09"
sample_y.argmax(dim=1)
```

<!-- #region id="v4IjU0dZGIkp" -->
âœ… *Use the categories list to translate the predicted class into its category*
<!-- #endregion -->

```python colab={"base_uri": "https://localhost:8080/", "height": 35} id="TKjw3PCVt9Sz" outputId="86cb6946-75d9-4682-f594-280579310419"
categories[657]
```

```python colab={"base_uri": "https://localhost:8080/"} id="3cA2ymRgteRH" outputId="025c672d-c11d-4c44-b8db-ba12e79c21a3"
sorted_predicted_classes = sample_y.argsort(dim=1, descending=True)[0, :10]
sorted_probs = torch.softmax(sample_y, dim=1)[0, sorted_predicted_classes]

for idx, prob in zip(sorted_predicted_classes, sorted_probs):
    print(categories[idx], "%3.2f %%" % (prob * 100))
```

<!-- #region id="N2HKxVslfk4h" -->


---


<!-- #endregion -->

<!-- #region id="WNj7-1UEvcjD" -->
# 4. Try with other sample images (only works with RGB!)
###Maybe pictures of dogs from here https://en.wikipedia.org/wiki/Dog.
### Copy the image address url and use `skimage.io.imread` to open the image.
<!-- #endregion -->

```python colab={"base_uri": "https://localhost:8080/", "height": 626} id="hLWlFL9vvhas" outputId="0d19b437-18c1-4113-92c3-c89c6d9abe5d"
sample_im = skimage.data.astronaut()[:300, 350:]

# sample_im = skimage.io.imread(url)

# Try this if you want to see how it fails when there is no category in the label space for an image
# https://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Giraffe_Mikumi_National_Park.jpg/800px-Giraffe_Mikumi_National_Park.jpg

sample_im_pt = torch.from_numpy(sample_im.transpose(2, 0, 1))

sample_x = pipeline(sample_im_pt)

sample_y = dl_model(sample_x[None, ...])

plt.imshow(sample_im)
plt.title(categories[sample_y.argmax(dim=1)])

sorted_predicted_classes = sample_y.argsort(dim=1, descending=True)[0, :10]
sorted_probs = torch.softmax(sample_y, dim=1)[0, sorted_predicted_classes]

for idx, prob in zip(sorted_predicted_classes, sorted_probs):
    print(categories[idx], "%3.2f %%" % (prob * 100))
```

<!-- #region id="IuejYgQgq234" -->


---


<!-- #endregion -->

<!-- #region id="pSw2ddPDqyVI" -->
#5. Use a Deep Learning model as feature extractor
<!-- #endregion -->

<!-- #region id="MdnLKWteoAI0" -->
âœ… *Modify the final classifier layer dl_model.fc to return the features extracted from the input image by changing it with a torch.nn.Identity module*
<!-- #endregion -->

<!-- #region id="y92l_93Uo76r" -->
ðŸš§ The classifier layer is commonly implemented as a MultiLayer Perceptron (Fully connected) at the end of the models.
The specific name of that layer can vary between implementations.
<!-- #endregion -->

```python colab={"base_uri": "https://localhost:8080/"} id="8QExX5erJqTi" outputId="b1674987-de22-43a4-bbe4-516ad512dbbc"
dl_extractor = torchvision.models.inception_v3(inception_weights, progress=True)
dl_extractor.eval()
```

```python colab={"base_uri": "https://localhost:8080/"} id="v3G1vw0CC2Xy" outputId="7b890f14-dc3c-44d0-d585-1d197a53587d"
dl_extractor.fc
```

```python id="2Xw0IcR4hu1W"
dl_extractor.fc = torch.nn.Identity()
```

<!-- #region id="ipKCgmVmrgjx" -->
âœ… *Use the model for feature extraction in the same way it is used for image classification.*
<!-- #endregion -->

```python colab={"base_uri": "https://localhost:8080/"} id="jt8ZA9nVh2Fe" outputId="fe637409-937f-445e-acda-f1a398638870"
sample_im = skimage.data.astronaut()[:300, 350:]

#sample_im = skimage.io.imread("url")

sample_im_pt = torch.from_numpy(sample_im.transpose(2, 0, 1))

sample_x = pipeline(sample_im_pt)

sample_fx = dl_extractor(sample_x[None, ...])

sample_fx.shape
```

```python id="5jBIJEzdq2M_"

```
