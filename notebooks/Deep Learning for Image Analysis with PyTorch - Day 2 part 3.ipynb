{"cells":[{"cell_type":"markdown","metadata":{"id":"abYP2fj8H1AL"},"source":["# 3.- Implement a custom data loading pipeline and evaluate the performance of a DL model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ni6iSiYFAqAI"},"outputs":[],"source":["!pip install fiftyone"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ny5TekvHAZ9F"},"outputs":[],"source":["import fiftyone as fo\n","import fiftyone.zoo as foz"]},{"cell_type":"markdown","metadata":{"id":"H2630ti8ZJet"},"source":["Download a train and validation set of images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2HxGxVz_A0Pc","cellView":"form"},"outputs":[],"source":["#@title Download FiftyOne training and validation dataset\n","trn_dataset = foz.load_zoo_dataset(\n","    \"open-images-v7\",\n","    split=\"train\",\n","    label_types=[\"classifications\"],\n","    classes = [\"Cat\"],\n","    max_samples=1000,\n","    dataset_dir=\"sample_data\",\n","    download_if_necessary=True\n",")\n","\n","val_dataset = foz.load_zoo_dataset(\n","    \"open-images-v7\",\n","    split=\"validation\",\n","    label_types=[\"classifications\"],\n","    classes = [\"Cat\"],\n","    max_samples=50,\n","    dataset_dir=\"sample_data\",\n","    download_if_necessary=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"309Yj0bFA7w6","cellView":"form"},"outputs":[],"source":["#@title FiftyOne PyTorch Dataset\n","import matplotlib.pyplot as plt\n","import torch\n","from PIL import Image\n","\n","\n","class FiftyOneTorchDataset(torch.utils.data.Dataset):\n","    \"\"\"A class to construct a PyTorch dataset from a FiftyOne dataset.\n","    \n","    Args:\n","        fiftyone_dataset: a FiftyOne dataset or view that will be used for training or testing\n","        transforms (None): a list of PyTorch transforms to apply to images and targets when loading\n","        gt_field (\"ground_truth\"): the name of the field in fiftyone_dataset that contains the \n","            desired labels to load\n","        classes (None): a list of class strings that are used to define the mapping between\n","            class names and indices. If None, it will use all classes present in the given fiftyone_dataset.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        fiftyone_dataset,\n","        transforms=None,\n","        classes=None,\n","    ):\n","        self.samples = fiftyone_dataset\n","        self.transforms = transforms\n","        self.img_paths = self.samples.values(\"filepath\")\n","\n","        self.classes = classes\n","\n","    def __getitem__(self, idx):\n","        img_path = self.img_paths[idx]\n","        sample = self.samples[img_path]\n","        metadata = sample.metadata\n","        img = Image.open(img_path).convert(\"RGB\")\n","\n","        label = any(lab[\"label\"] in self.classes\n","                    for lab in sample[\"positive_labels\"][\"classifications\"])\n","        target = torch.as_tensor(label, dtype=torch.float32)\n","\n","        if self.transforms is not None:\n","            img = self.transforms(img)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def get_classes(self):\n","        return self.classes"]},{"cell_type":"markdown","metadata":{"id":"zvK6GSTLKT3h"},"source":["## Data augmentation as a pre-processing pipeline"]},{"cell_type":"markdown","metadata":{"id":"3fs31oUC9q3K"},"source":["Deep learning models require large amounts of data to train. One reason is that DL models are complex and have lots of parameters.\n","\n","*   LeNet5 has 60,000 parameters\n","*   InceptionV3 has 25 million parameters\n","*   Vision Transformer has 110 million parameters"]},{"cell_type":"markdown","metadata":{"id":"L7W12UQb-WPR"},"source":["Other reason is that we want our model to learn from as many examples as possible to make it robust to variations in the input data.\n","That way the model would ouput the same response to an image that is different from the ones in the training set.\n","\n","The most common approach to make a model robust to these variations is through *data augmentation*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rK_OxiNYHwFS"},"outputs":[],"source":["from torchvision import transforms"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDOBoD3Q_Qgy"},"outputs":[],"source":["# Use transforms.Compose and other transforms to create the data augmentation pipeline\n","augment_pipeline = transforms.Compose([\n","    # Add here other image augmentation functions to apply\n","    \n","    transforms.Resize((299, 299)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rq2NO12IArXo"},"outputs":[],"source":["pt_trn_dataset = FiftyOneTorchDataset(trn_dataset,\n","                                      classes=[\"Cat\"],\n","                                      transforms=\n","                                      )"]},{"cell_type":"code","source":["# Get only cat images\n","cats = [i for i, (x, l) in enumerate(pt_trn_dataset) if l > 0.5]"],"metadata":{"id":"SkhZPGtMJWEW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMWUJQx7BG0q"},"outputs":[],"source":["x, l = pt_trn_dataset[cats[5]]\n","\n","print(\"Augmented image shape\", x.shape)\n","im = (x.permute(1, 2, 0) - x.min()) / (x.max() - x.min())\n","\n","plt.imshow(im)\n","plt.title(f\"Is a cat? {l > 0.5}\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"-KZ7MIqcKcgC"},"source":["## Train the InceptionV3 model using data augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xo6wJrZpEaDT"},"outputs":[],"source":["from torchvision.models import inception_v3\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","# The classifier model\n","model = inception_v3(weights=None, progress=True, num_classes=1)\n","\n","# The Error/Loss function\n","criterion = nn.BCEWithLogitsLoss()\n","\n","# The optimizer algorithm\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# The batches data loader\n","trn_queue = DataLoader(pt_trn_dataset, batch_size=16, shuffle=True, pin_memory=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kCFRpCL9EvPb"},"outputs":[],"source":["# Move the model to the GPU memory\n","model.train()\n","\n","if torch.cuda.is_available():\n","  model.cuda()\n","\n","\n","for e in range(5):\n","  for i, (x, y) in enumerate(trn_queue):\n","    # Empty the accumulated gradients from any previous iteration\n","    optimizer.zero_grad()\n","\n","    # Move the input images and their respective classes to the GPU\n","    if torch.cuda.is_available():\n","      x = x.cuda()\n","      y = y.cuda()\n","\n","    y_hat = model(x)\n","\n","    # Compute the error/loss function\n","    loss = criterion(y_hat.logits, y.view(-1, 1))\n","\n","    # Perform the backward pass to generate the gradients of the loss function with respect to the inputs\n","    loss.backward()\n","\n","    # Update the model parameters\n","    optimizer.step()\n","\n","    # Log the progress of the model\n","    if i % 10 == 0:\n","      acc = torch.sum(y == y_hat.logits.detach().argmax(dim=1)) / x.shape[0]\n","\n","      print(f\"Epoch {e}, step {i}: loss={loss.item()}, acc={acc}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5hq2rRtIuSjO"},"outputs":[],"source":["torch.save(model.state_dict(), \"my_model_checkpoint.pth\")"]},{"cell_type":"markdown","metadata":{"id":"_ErzujytKjFr"},"source":["## Evaluate the performance of the model"]},{"cell_type":"markdown","metadata":{"id":"aEYDsOFxKlyi"},"source":["To evaluate the performance of the model we'll use a set of images that have not been *seen* by our model during training.\n","This ensures that the performance metrics are an approximation of how our model will behave in production."]},{"cell_type":"markdown","metadata":{"id":"1pG1vAA8NKcy"},"source":["Because we are not training our model anymore, we can pass the validation images as they are without any augmentation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fcFc3NtUKi6-"},"outputs":[],"source":["val_pipeline = transforms.Compose([\n","    transforms.Resize((299, 299)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NwTsp9OBNW8R"},"outputs":[],"source":["pt_val_dataset = FiftyOneTorchDataset(val_dataset, classes=[\"Cat\"], transforms=val_pipeline)\n","\n","val_queue = DataLoader(pt_trn_dataset, batch_size=16, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2INiGKRtwSM"},"outputs":[],"source":["model.eval()\n","\n","tp = 0\n","tn = 0\n","p = 0\n","n = 0\n","\n","with torch.no_grad():\n","  for i, (x, y) in enumerate(val_queue):\n","\n","    # Move the input images and their respective classes to the GPU\n","    if torch.cuda.is_available():\n","      x = x.cuda()\n","      y = y.cuda()\n","\n","    y_hat = model(x)\n","\n","    p += y.sum().item()\n","    n += x.shape[0] - y.sum().item()\n","    tp += torch.sum(y * y_hat.detach().argmax(dim=1)).item()\n","    tn += torch.sum((1-y) * (1-y_hat.detach().argmax(dim=1))).item()\n","\n","    # Log the progress of the model\n","    if i % 10 == 0:\n","      acc = (tp + tn) / (p + n)\n","      print(f\"Step {i}/{len(val_queue)}: acc={acc}\")"]},{"cell_type":"markdown","metadata":{"id":"ttI1Tu3LZYP6"},"source":["Save our model to use in production and share"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyPhwdZ7m4/tKhrWGycNxOoL"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}