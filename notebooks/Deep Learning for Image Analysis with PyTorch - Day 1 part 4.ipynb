{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"-Lwtrz9vZ_zo"},"source":["# 4.- Define training process of a DL model as an optimization problem."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"n4-jh2pLCnHv"},"source":["Fitting the parameters of our DL model to achieve high performance is achieved through an optimization process called *training*."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"a-lyYhcmDHYR"},"source":["1.   Select the model's architecture for the task\n","2.   Define the error function that will evaluate the performance of our DL model\n","3.   Choose an optimization algorithm to fit the model's parameters\n","4.   Run the training loop until convergence\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"UmSSsr7zHIoJ"},"source":["## The DL model\n","\n","The implementation of the architecture of our model $f_\\theta(x)$, that can be a LeNet5, InceptionV3, U-Net, etc."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bQZD42zgQQPr"},"source":["## The Loss/Error function\n","\n","This function depends on the task our model is being trained to perform.\n","\n","This loss/error function is a metric of the distance between the model's output $\\hat{y}$ and the expected output $y$.\n","\n","$Err(\\hat{y}, y) = L(\\hat{y}, y)$\n","\n","And the optimization problem is defined in general as\n","\n","$\\theta^* = argmin~Err(\\hat{y}=f_\\theta(x), y)$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZgKIdPKuQGp6"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"iIseg1JzQsfq"},"source":["## The Optimization algorithm\n","\n","The optimization algorithm is the method that updates the model parameters $\\theta$ during the training loop."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"zv6eGmqOQ5j8"},"outputs":[],"source":["#@title A simple example model \n","model = nn.Sequential(\n","    nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3),\n","    nn.Flatten(),\n","    nn.Linear(in_features=8 * 26 * 26, out_features=10)\n","    )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z0kOWAE_qjHD"},"outputs":[],"source":["import torch.optim as optim\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9TxsgCnwRYeb"},"source":["## Run the training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2474,"status":"ok","timestamp":1683655245950,"user":{"displayName":"Fernando Cervantes","userId":"15218495656973493127"},"user_tz":240},"id":"MG4whfdvD7Z6","outputId":"f24facce-6e28-43ae-d140-6ac12090107c"},"outputs":[],"source":["#@title a preprocessing pipeline (more about this on Day 2)\n","from torchvision.datasets import MNIST\n","from torchvision.transforms import Compose, ToTensor, Normalize\n","\n","# This will retrieve the images and apzply the pre-processing pipeline\n","prep_pipeline = Compose([\n","    ToTensor(),\n","    Normalize(mean=0.5, std=0.5)\n","])\n","\n","trn_data = MNIST(\"sample_data\", train=True, download=True, transform=prep_pipeline)\n","\n","from torch.utils.data import DataLoader\n","\n","trn_queue = DataLoader(trn_data, batch_size=128, shuffle=True, pin_memory=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gxqZQLT_SaVl"},"source":["## The training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":199404,"status":"ok","timestamp":1683655774918,"user":{"displayName":"Fernando Cervantes","userId":"15218495656973493127"},"user_tz":240},"id":"L-Xp1LWGEoq9","outputId":"08215672-84b2-4735-a64c-32b89983d4db"},"outputs":[],"source":["# Move the model to the GPU memory\n","model.train()\n","\n","for e in range(10):\n","  for i, (x, y) in enumerate(trn_queue):\n","    # Empty the accumulated gradients from any previous iteration\n","\n","\n","    # Get the model's output\n","\n","\n","    # Compute the error/loss function between y_hat, and y\n","\n","\n","    # Perform the backward pass to generate the gradients of the loss function with respect to the inputs\n","\n","\n","    # Update the model parameters\n","\n","\n","    # Log the progress of the model\n","    if i % 100 == 0:\n","      acc = torch.sum(y == y_hat.detach().argmax(dim=1)) / x.shape[0]\n","\n","      print(f\"Epoch {e}, step {i}: loss={loss.item()}, acc={acc}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9dmUR3xvfJzd"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPb8hFHoW0LCi+ilg82IXhL","provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
