{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPZkUlacuJT/lHwdlDIQFA8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# 4.- Define training process of a DL model as an optimization problem."],"metadata":{"id":"-Lwtrz9vZ_zo"}},{"cell_type":"markdown","source":["Fitting the parameters of our DL model to achieve high performance is achieved through an optimization process called *training*."],"metadata":{"id":"n4-jh2pLCnHv"}},{"cell_type":"markdown","source":["1.   Select the model's architecture for the task\n","2.   Define the error function that will evaluate the performance of our DL model\n","3.   Choose an optimization algorithm to fit the model's parameters\n","4.   Run the training loop until convergence\n"],"metadata":{"id":"a-lyYhcmDHYR"}},{"cell_type":"markdown","source":["## The DL model\n","\n","The implementation of the architecture of our model $f_\\theta(x)$, that can be a LeNet5, InceptionV3, U-Net, etc."],"metadata":{"id":"UmSSsr7zHIoJ"}},{"cell_type":"markdown","source":["## The Loss/Error function\n","\n","This function depends on the task our model is being trained to perform.\n","\n","This loss/error function is a metric of the distance between the model's output $\\hat{y}$ and the expected output $y$.\n","\n","$Err(\\hat{y}, y) = L(\\hat{y}, y)$\n","\n","And the optimization problem is defined in general as\n","\n","$\\theta^* = argmin~Err(\\hat{y}=f_\\theta(x), y)$"],"metadata":{"id":"bQZD42zgQQPr"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","criterion = nn.MSELoss() # Mean squared error loss mean((y - y_hat) ** 2) For Regression-like problems\n","\n","criterion = nn.CrossEntropyLoss() # For multi-class classification sum(-y * log(y_hat))"],"metadata":{"id":"ZgKIdPKuQGp6","executionInfo":{"status":"ok","timestamp":1683654999622,"user_tz":240,"elapsed":7954,"user":{"displayName":"Fernando Cervantes","userId":"15218495656973493127"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## The Optimization algorithm\n","\n","The optimization algorithm is the method that updates the model parameters $\\theta$ during the training loop."],"metadata":{"id":"iIseg1JzQsfq"}},{"cell_type":"code","source":["#@title A simple example model \n","model = nn.Sequential(\n","    nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3),\n","    nn.Flatten(),\n","    nn.Linear(in_features=8 * 26 * 26, out_features=10)\n","    )\n"],"metadata":{"id":"zv6eGmqOQ5j8","executionInfo":{"status":"ok","timestamp":1683655119762,"user_tz":240,"elapsed":181,"user":{"displayName":"Fernando Cervantes","userId":"15218495656973493127"}},"cellView":"form"},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","\n","optimizer = optim.Adam(model.parameters())"],"metadata":{"id":"z0kOWAE_qjHD","executionInfo":{"status":"ok","timestamp":1683655230786,"user_tz":240,"elapsed":4,"user":{"displayName":"Fernando Cervantes","userId":"15218495656973493127"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## Run the training loop"],"metadata":{"id":"9TxsgCnwRYeb"}},{"cell_type":"code","source":["#@title a preprocessing pipeline (more about this on Day 2)\n","from torchvision.datasets import MNIST\n","from torchvision.transforms import Compose, ToTensor, Normalize\n","\n","# This will retrieve the images and apzply the pre-processing pipeline\n","prep_pipeline = Compose([\n","    ToTensor(),\n","    Normalize(mean=0.5, std=0.5)\n","])\n","\n","trn_data = MNIST(\"sample_data\", train=True, download=True, transform=prep_pipeline)\n","\n","from torch.utils.data import DataLoader\n","\n","trn_queue = DataLoader(trn_data, batch_size=128, shuffle=True, pin_memory=True)"],"metadata":{"id":"MG4whfdvD7Z6","executionInfo":{"status":"ok","timestamp":1683655245950,"user_tz":240,"elapsed":2474,"user":{"displayName":"Fernando Cervantes","userId":"15218495656973493127"}},"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","outputId":"f24facce-6e28-43ae-d140-6ac12090107c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to sample_data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 129484255.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting sample_data/MNIST/raw/train-images-idx3-ubyte.gz to sample_data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to sample_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 76619667.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting sample_data/MNIST/raw/train-labels-idx1-ubyte.gz to sample_data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to sample_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 41075556.20it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting sample_data/MNIST/raw/t10k-images-idx3-ubyte.gz to sample_data/MNIST/raw\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to sample_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 5333294.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting sample_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to sample_data/MNIST/raw\n","\n"]}]},{"cell_type":"markdown","source":["## The training loop"],"metadata":{"id":"gxqZQLT_SaVl"}},{"cell_type":"code","source":["# Move the model to the GPU memory\n","model.train()\n","\n","for e in range(10):\n","  for i, (x, y) in enumerate(trn_queue):\n","    # Empty the accumulated gradients from any previous iteration\n","    optimizer.zero_grad()\n","\n","    # Get the model's output\n","    y_hat = model(x)\n","\n","    # Compute the error/loss function\n","    loss = criterion(y_hat, y)\n","\n","    # Perform the backward pass to generate the gradients of the loss function with respect to the inputs\n","    loss.backward()\n","\n","    # Update the model parameters\n","    optimizer.step()\n","\n","    # Log the progress of the model\n","    if i % 100 == 0:\n","      acc = torch.sum(y == y_hat.detach().argmax(dim=1)) / x.shape[0]\n","\n","      print(f\"Epoch {e}, step {i}: loss={loss.item()}, acc={acc}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-Xp1LWGEoq9","executionInfo":{"status":"ok","timestamp":1683655774918,"user_tz":240,"elapsed":199404,"user":{"displayName":"Fernando Cervantes","userId":"15218495656973493127"}},"outputId":"08215672-84b2-4735-a64c-32b89983d4db"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, step 0: loss=2.45098614692688, acc=0.078125\n","Epoch 0, step 100: loss=0.4440862536430359, acc=0.84375\n","Epoch 0, step 200: loss=0.38024723529815674, acc=0.8828125\n","Epoch 0, step 300: loss=0.3931034207344055, acc=0.875\n","Epoch 0, step 400: loss=0.3477005660533905, acc=0.8984375\n","Epoch 1, step 0: loss=0.41891351342201233, acc=0.859375\n","Epoch 1, step 100: loss=0.3142244219779968, acc=0.9296875\n","Epoch 1, step 200: loss=0.23581382632255554, acc=0.921875\n","Epoch 1, step 300: loss=0.3355253040790558, acc=0.9140625\n","Epoch 1, step 400: loss=0.15423917770385742, acc=0.953125\n","Epoch 2, step 0: loss=0.2653961479663849, acc=0.953125\n","Epoch 2, step 100: loss=0.32926279306411743, acc=0.921875\n","Epoch 2, step 200: loss=0.3378745913505554, acc=0.8671875\n","Epoch 2, step 300: loss=0.24061369895935059, acc=0.9296875\n","Epoch 2, step 400: loss=0.33357828855514526, acc=0.921875\n","Epoch 3, step 0: loss=0.28853321075439453, acc=0.921875\n","Epoch 3, step 100: loss=0.2998126149177551, acc=0.9140625\n","Epoch 3, step 200: loss=0.26156333088874817, acc=0.9140625\n","Epoch 3, step 300: loss=0.30008748173713684, acc=0.9296875\n","Epoch 3, step 400: loss=0.2450263798236847, acc=0.921875\n","Epoch 4, step 0: loss=0.2911756932735443, acc=0.921875\n","Epoch 4, step 100: loss=0.2537153363227844, acc=0.9375\n","Epoch 4, step 200: loss=0.29380759596824646, acc=0.890625\n","Epoch 4, step 300: loss=0.1579723060131073, acc=0.9609375\n","Epoch 4, step 400: loss=0.2723715901374817, acc=0.9375\n","Epoch 5, step 0: loss=0.27892088890075684, acc=0.9375\n","Epoch 5, step 100: loss=0.21370217204093933, acc=0.921875\n","Epoch 5, step 200: loss=0.2901305556297302, acc=0.8984375\n","Epoch 5, step 300: loss=0.2107187807559967, acc=0.9375\n","Epoch 5, step 400: loss=0.2978443205356598, acc=0.9140625\n","Epoch 6, step 0: loss=0.24993601441383362, acc=0.9375\n","Epoch 6, step 100: loss=0.21421106159687042, acc=0.921875\n","Epoch 6, step 200: loss=0.38288992643356323, acc=0.875\n","Epoch 6, step 300: loss=0.33451783657073975, acc=0.90625\n","Epoch 6, step 400: loss=0.2558705806732178, acc=0.953125\n","Epoch 7, step 0: loss=0.18830113112926483, acc=0.9375\n","Epoch 7, step 100: loss=0.41894862055778503, acc=0.890625\n","Epoch 7, step 200: loss=0.2830488085746765, acc=0.9140625\n","Epoch 7, step 300: loss=0.29008904099464417, acc=0.921875\n","Epoch 7, step 400: loss=0.1793023943901062, acc=0.9375\n","Epoch 8, step 0: loss=0.2060796618461609, acc=0.921875\n","Epoch 8, step 100: loss=0.25764089822769165, acc=0.9140625\n","Epoch 8, step 200: loss=0.39843061566352844, acc=0.8828125\n","Epoch 8, step 300: loss=0.2601233422756195, acc=0.90625\n","Epoch 8, step 400: loss=0.19504201412200928, acc=0.9765625\n","Epoch 9, step 0: loss=0.2047397345304489, acc=0.9375\n","Epoch 9, step 100: loss=0.35388875007629395, acc=0.921875\n","Epoch 9, step 200: loss=0.23658253252506256, acc=0.90625\n","Epoch 9, step 300: loss=0.18957670032978058, acc=0.9609375\n","Epoch 9, step 400: loss=0.23438484966754913, acc=0.90625\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"9dmUR3xvfJzd"},"execution_count":null,"outputs":[]}]}