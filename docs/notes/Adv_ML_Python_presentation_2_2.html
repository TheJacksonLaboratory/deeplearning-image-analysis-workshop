<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.40">

  <meta name="author" content="Fernando Cervantes (fernando.cervantes@jax.org)">
  <title>Advanced Machine Learning with Python workshop ‚Äì Advanced Machine Learning with Python (Session 2 - Part 2)</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-367017fa2871343a8466c6f0f2630bc9.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.3/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-jnSuA4Ss2PkkikSOLtYs8BlYIeeIK1h99ty4YfvRPAlzr377vr3CXDb7sb7eEEBYjDtcYj+AjBH3FLv5uSJuXg==" crossorigin="anonymous">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Advanced Machine Learning with Python (Session 2 - Part 2)</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Fernando Cervantes (fernando.cervantes@jax.org) 
</div>
</div>
</div>

</section>
<section id="materials" class="slide level2">
<h2>Materials</h2>
<p><a href="https://colab.research.google.com/gist/fercer/8e5262d5df22d5b55468642855e0d1b8/advanced_machine_learning_with_python_session_2_2.ipynb" class="btn btn-outline-primary" role="button" target="‚Äù_blank‚Äù">Open notebook in Colab</a></p>
</section>
<section>
<section id="working-with-transformers" class="title-slide slide level1 center">
<h1>Working with Transformers</h1>

</section>
<section id="review-the-architecture-of-a-vision-transformer-vit" class="slide level2 scrollable">
<h2>Review the architecture of a Vision Transformer (ViT)</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="../imgs/ViT.png"></p>
<figcaption>Dosovitskiy, Alexey et al.&nbsp;‚ÄúAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.‚Äù ArXiv abs/2010.11929 (2020)</figcaption>
</figure>
</div>
<ul>
<li><p>https://docs.pytorch.org/vision/stable/models/vision_transformer.html</p></li>
<li><p><label><input type="checkbox">Review the Attention mechanism in transformer models</label></p></li>
</ul>
<p><img data-src="../imgs/Attn_layer.png"></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="../imgs/Attn_operation.png"></p>
<figcaption>Vaswani, Ashish et al.&nbsp;‚ÄúAttention is All you Need.‚Äù Neural Information Processing Systems (2017).</figcaption>
</figure>
</div>
</section>
<section id="exercise-use-a-pre-trained-vit-model-to-classify-images" class="slide level2 scrollable">
<h2>Exercise: Use a pre-trained ViT model to classify images</h2>
<ul class="task-list">
<li><label><input type="checkbox">Import the pre-trained weights of the Inception V3 model from models.inception_v3</label></li>
</ul>
<div id="4e9a0a70" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a></a><span class="im">from</span> torchvision <span class="im">import</span> models</span>
<span id="cb1-3"><a></a></span>
<span id="cb1-4"><a></a>transformer_weights <span class="op">=</span> models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1</span>
<span id="cb1-5"><a></a></span>
<span id="cb1-6"><a></a>transformer_weights.meta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>{'categories': ['tench',
  'goldfish',
  'great white shark',
  'tiger shark',
  'hammerhead',
  'electric ray',
  'stingray',
  'cock',
  'hen',
  'ostrich',
  'brambling',
  'goldfinch',
  'house finch',
  'junco',
  'indigo bunting',
  'robin',
  'bulbul',
  'jay',
  'magpie',
  'chickadee',
  'water ouzel',
  'kite',
  'bald eagle',
  'vulture',
  'great grey owl',
  'European fire salamander',
  'common newt',
  'eft',
  'spotted salamander',
  'axolotl',
  'bullfrog',
  'tree frog',
  'tailed frog',
  'loggerhead',
  'leatherback turtle',
  'mud turtle',
  'terrapin',
  'box turtle',
  'banded gecko',
  'common iguana',
  'American chameleon',
  'whiptail',
  'agama',
  'frilled lizard',
  'alligator lizard',
  'Gila monster',
  'green lizard',
  'African chameleon',
  'Komodo dragon',
  'African crocodile',
  'American alligator',
  'triceratops',
  'thunder snake',
  'ringneck snake',
  'hognose snake',
  'green snake',
  'king snake',
  'garter snake',
  'water snake',
  'vine snake',
  'night snake',
  'boa constrictor',
  'rock python',
  'Indian cobra',
  'green mamba',
  'sea snake',
  'horned viper',
  'diamondback',
  'sidewinder',
  'trilobite',
  'harvestman',
  'scorpion',
  'black and gold garden spider',
  'barn spider',
  'garden spider',
  'black widow',
  'tarantula',
  'wolf spider',
  'tick',
  'centipede',
  'black grouse',
  'ptarmigan',
  'ruffed grouse',
  'prairie chicken',
  'peacock',
  'quail',
  'partridge',
  'African grey',
  'macaw',
  'sulphur-crested cockatoo',
  'lorikeet',
  'coucal',
  'bee eater',
  'hornbill',
  'hummingbird',
  'jacamar',
  'toucan',
  'drake',
  'red-breasted merganser',
  'goose',
  'black swan',
  'tusker',
  'echidna',
  'platypus',
  'wallaby',
  'koala',
  'wombat',
  'jellyfish',
  'sea anemone',
  'brain coral',
  'flatworm',
  'nematode',
  'conch',
  'snail',
  'slug',
  'sea slug',
  'chiton',
  'chambered nautilus',
  'Dungeness crab',
  'rock crab',
  'fiddler crab',
  'king crab',
  'American lobster',
  'spiny lobster',
  'crayfish',
  'hermit crab',
  'isopod',
  'white stork',
  'black stork',
  'spoonbill',
  'flamingo',
  'little blue heron',
  'American egret',
  'bittern',
  'crane bird',
  'limpkin',
  'European gallinule',
  'American coot',
  'bustard',
  'ruddy turnstone',
  'red-backed sandpiper',
  'redshank',
  'dowitcher',
  'oystercatcher',
  'pelican',
  'king penguin',
  'albatross',
  'grey whale',
  'killer whale',
  'dugong',
  'sea lion',
  'Chihuahua',
  'Japanese spaniel',
  'Maltese dog',
  'Pekinese',
  'Shih-Tzu',
  'Blenheim spaniel',
  'papillon',
  'toy terrier',
  'Rhodesian ridgeback',
  'Afghan hound',
  'basset',
  'beagle',
  'bloodhound',
  'bluetick',
  'black-and-tan coonhound',
  'Walker hound',
  'English foxhound',
  'redbone',
  'borzoi',
  'Irish wolfhound',
  'Italian greyhound',
  'whippet',
  'Ibizan hound',
  'Norwegian elkhound',
  'otterhound',
  'Saluki',
  'Scottish deerhound',
  'Weimaraner',
  'Staffordshire bullterrier',
  'American Staffordshire terrier',
  'Bedlington terrier',
  'Border terrier',
  'Kerry blue terrier',
  'Irish terrier',
  'Norfolk terrier',
  'Norwich terrier',
  'Yorkshire terrier',
  'wire-haired fox terrier',
  'Lakeland terrier',
  'Sealyham terrier',
  'Airedale',
  'cairn',
  'Australian terrier',
  'Dandie Dinmont',
  'Boston bull',
  'miniature schnauzer',
  'giant schnauzer',
  'standard schnauzer',
  'Scotch terrier',
  'Tibetan terrier',
  'silky terrier',
  'soft-coated wheaten terrier',
  'West Highland white terrier',
  'Lhasa',
  'flat-coated retriever',
  'curly-coated retriever',
  'golden retriever',
  'Labrador retriever',
  'Chesapeake Bay retriever',
  'German short-haired pointer',
  'vizsla',
  'English setter',
  'Irish setter',
  'Gordon setter',
  'Brittany spaniel',
  'clumber',
  'English springer',
  'Welsh springer spaniel',
  'cocker spaniel',
  'Sussex spaniel',
  'Irish water spaniel',
  'kuvasz',
  'schipperke',
  'groenendael',
  'malinois',
  'briard',
  'kelpie',
  'komondor',
  'Old English sheepdog',
  'Shetland sheepdog',
  'collie',
  'Border collie',
  'Bouvier des Flandres',
  'Rottweiler',
  'German shepherd',
  'Doberman',
  'miniature pinscher',
  'Greater Swiss Mountain dog',
  'Bernese mountain dog',
  'Appenzeller',
  'EntleBucher',
  'boxer',
  'bull mastiff',
  'Tibetan mastiff',
  'French bulldog',
  'Great Dane',
  'Saint Bernard',
  'Eskimo dog',
  'malamute',
  'Siberian husky',
  'dalmatian',
  'affenpinscher',
  'basenji',
  'pug',
  'Leonberg',
  'Newfoundland',
  'Great Pyrenees',
  'Samoyed',
  'Pomeranian',
  'chow',
  'keeshond',
  'Brabancon griffon',
  'Pembroke',
  'Cardigan',
  'toy poodle',
  'miniature poodle',
  'standard poodle',
  'Mexican hairless',
  'timber wolf',
  'white wolf',
  'red wolf',
  'coyote',
  'dingo',
  'dhole',
  'African hunting dog',
  'hyena',
  'red fox',
  'kit fox',
  'Arctic fox',
  'grey fox',
  'tabby',
  'tiger cat',
  'Persian cat',
  'Siamese cat',
  'Egyptian cat',
  'cougar',
  'lynx',
  'leopard',
  'snow leopard',
  'jaguar',
  'lion',
  'tiger',
  'cheetah',
  'brown bear',
  'American black bear',
  'ice bear',
  'sloth bear',
  'mongoose',
  'meerkat',
  'tiger beetle',
  'ladybug',
  'ground beetle',
  'long-horned beetle',
  'leaf beetle',
  'dung beetle',
  'rhinoceros beetle',
  'weevil',
  'fly',
  'bee',
  'ant',
  'grasshopper',
  'cricket',
  'walking stick',
  'cockroach',
  'mantis',
  'cicada',
  'leafhopper',
  'lacewing',
  'dragonfly',
  'damselfly',
  'admiral',
  'ringlet',
  'monarch',
  'cabbage butterfly',
  'sulphur butterfly',
  'lycaenid',
  'starfish',
  'sea urchin',
  'sea cucumber',
  'wood rabbit',
  'hare',
  'Angora',
  'hamster',
  'porcupine',
  'fox squirrel',
  'marmot',
  'beaver',
  'guinea pig',
  'sorrel',
  'zebra',
  'hog',
  'wild boar',
  'warthog',
  'hippopotamus',
  'ox',
  'water buffalo',
  'bison',
  'ram',
  'bighorn',
  'ibex',
  'hartebeest',
  'impala',
  'gazelle',
  'Arabian camel',
  'llama',
  'weasel',
  'mink',
  'polecat',
  'black-footed ferret',
  'otter',
  'skunk',
  'badger',
  'armadillo',
  'three-toed sloth',
  'orangutan',
  'gorilla',
  'chimpanzee',
  'gibbon',
  'siamang',
  'guenon',
  'patas',
  'baboon',
  'macaque',
  'langur',
  'colobus',
  'proboscis monkey',
  'marmoset',
  'capuchin',
  'howler monkey',
  'titi',
  'spider monkey',
  'squirrel monkey',
  'Madagascar cat',
  'indri',
  'Indian elephant',
  'African elephant',
  'lesser panda',
  'giant panda',
  'barracouta',
  'eel',
  'coho',
  'rock beauty',
  'anemone fish',
  'sturgeon',
  'gar',
  'lionfish',
  'puffer',
  'abacus',
  'abaya',
  'academic gown',
  'accordion',
  'acoustic guitar',
  'aircraft carrier',
  'airliner',
  'airship',
  'altar',
  'ambulance',
  'amphibian',
  'analog clock',
  'apiary',
  'apron',
  'ashcan',
  'assault rifle',
  'backpack',
  'bakery',
  'balance beam',
  'balloon',
  'ballpoint',
  'Band Aid',
  'banjo',
  'bannister',
  'barbell',
  'barber chair',
  'barbershop',
  'barn',
  'barometer',
  'barrel',
  'barrow',
  'baseball',
  'basketball',
  'bassinet',
  'bassoon',
  'bathing cap',
  'bath towel',
  'bathtub',
  'beach wagon',
  'beacon',
  'beaker',
  'bearskin',
  'beer bottle',
  'beer glass',
  'bell cote',
  'bib',
  'bicycle-built-for-two',
  'bikini',
  'binder',
  'binoculars',
  'birdhouse',
  'boathouse',
  'bobsled',
  'bolo tie',
  'bonnet',
  'bookcase',
  'bookshop',
  'bottlecap',
  'bow',
  'bow tie',
  'brass',
  'brassiere',
  'breakwater',
  'breastplate',
  'broom',
  'bucket',
  'buckle',
  'bulletproof vest',
  'bullet train',
  'butcher shop',
  'cab',
  'caldron',
  'candle',
  'cannon',
  'canoe',
  'can opener',
  'cardigan',
  'car mirror',
  'carousel',
  "carpenter's kit",
  'carton',
  'car wheel',
  'cash machine',
  'cassette',
  'cassette player',
  'castle',
  'catamaran',
  'CD player',
  'cello',
  'cellular telephone',
  'chain',
  'chainlink fence',
  'chain mail',
  'chain saw',
  'chest',
  'chiffonier',
  'chime',
  'china cabinet',
  'Christmas stocking',
  'church',
  'cinema',
  'cleaver',
  'cliff dwelling',
  'cloak',
  'clog',
  'cocktail shaker',
  'coffee mug',
  'coffeepot',
  'coil',
  'combination lock',
  'computer keyboard',
  'confectionery',
  'container ship',
  'convertible',
  'corkscrew',
  'cornet',
  'cowboy boot',
  'cowboy hat',
  'cradle',
  'crane',
  'crash helmet',
  'crate',
  'crib',
  'Crock Pot',
  'croquet ball',
  'crutch',
  'cuirass',
  'dam',
  'desk',
  'desktop computer',
  'dial telephone',
  'diaper',
  'digital clock',
  'digital watch',
  'dining table',
  'dishrag',
  'dishwasher',
  'disk brake',
  'dock',
  'dogsled',
  'dome',
  'doormat',
  'drilling platform',
  'drum',
  'drumstick',
  'dumbbell',
  'Dutch oven',
  'electric fan',
  'electric guitar',
  'electric locomotive',
  'entertainment center',
  'envelope',
  'espresso maker',
  'face powder',
  'feather boa',
  'file',
  'fireboat',
  'fire engine',
  'fire screen',
  'flagpole',
  'flute',
  'folding chair',
  'football helmet',
  'forklift',
  'fountain',
  'fountain pen',
  'four-poster',
  'freight car',
  'French horn',
  'frying pan',
  'fur coat',
  'garbage truck',
  'gasmask',
  'gas pump',
  'goblet',
  'go-kart',
  'golf ball',
  'golfcart',
  'gondola',
  'gong',
  'gown',
  'grand piano',
  'greenhouse',
  'grille',
  'grocery store',
  'guillotine',
  'hair slide',
  'hair spray',
  'half track',
  'hammer',
  'hamper',
  'hand blower',
  'hand-held computer',
  'handkerchief',
  'hard disc',
  'harmonica',
  'harp',
  'harvester',
  'hatchet',
  'holster',
  'home theater',
  'honeycomb',
  'hook',
  'hoopskirt',
  'horizontal bar',
  'horse cart',
  'hourglass',
  'iPod',
  'iron',
  "jack-o'-lantern",
  'jean',
  'jeep',
  'jersey',
  'jigsaw puzzle',
  'jinrikisha',
  'joystick',
  'kimono',
  'knee pad',
  'knot',
  'lab coat',
  'ladle',
  'lampshade',
  'laptop',
  'lawn mower',
  'lens cap',
  'letter opener',
  'library',
  'lifeboat',
  'lighter',
  'limousine',
  'liner',
  'lipstick',
  'Loafer',
  'lotion',
  'loudspeaker',
  'loupe',
  'lumbermill',
  'magnetic compass',
  'mailbag',
  'mailbox',
  'maillot',
  'maillot tank suit',
  'manhole cover',
  'maraca',
  'marimba',
  'mask',
  'matchstick',
  'maypole',
  'maze',
  'measuring cup',
  'medicine chest',
  'megalith',
  'microphone',
  'microwave',
  'military uniform',
  'milk can',
  'minibus',
  'miniskirt',
  'minivan',
  'missile',
  'mitten',
  'mixing bowl',
  'mobile home',
  'Model T',
  'modem',
  'monastery',
  'monitor',
  'moped',
  'mortar',
  'mortarboard',
  'mosque',
  'mosquito net',
  'motor scooter',
  'mountain bike',
  'mountain tent',
  'mouse',
  'mousetrap',
  'moving van',
  'muzzle',
  'nail',
  'neck brace',
  'necklace',
  'nipple',
  'notebook',
  'obelisk',
  'oboe',
  'ocarina',
  'odometer',
  'oil filter',
  'organ',
  'oscilloscope',
  'overskirt',
  'oxcart',
  'oxygen mask',
  'packet',
  'paddle',
  'paddlewheel',
  'padlock',
  'paintbrush',
  'pajama',
  'palace',
  'panpipe',
  'paper towel',
  'parachute',
  'parallel bars',
  'park bench',
  'parking meter',
  'passenger car',
  'patio',
  'pay-phone',
  'pedestal',
  'pencil box',
  'pencil sharpener',
  'perfume',
  'Petri dish',
  'photocopier',
  'pick',
  'pickelhaube',
  'picket fence',
  'pickup',
  'pier',
  'piggy bank',
  'pill bottle',
  'pillow',
  'ping-pong ball',
  'pinwheel',
  'pirate',
  'pitcher',
  'plane',
  'planetarium',
  'plastic bag',
  'plate rack',
  'plow',
  'plunger',
  'Polaroid camera',
  'pole',
  'police van',
  'poncho',
  'pool table',
  'pop bottle',
  'pot',
  "potter's wheel",
  'power drill',
  'prayer rug',
  'printer',
  'prison',
  'projectile',
  'projector',
  'puck',
  'punching bag',
  'purse',
  'quill',
  'quilt',
  'racer',
  'racket',
  'radiator',
  'radio',
  'radio telescope',
  'rain barrel',
  'recreational vehicle',
  'reel',
  'reflex camera',
  'refrigerator',
  'remote control',
  'restaurant',
  'revolver',
  'rifle',
  'rocking chair',
  'rotisserie',
  'rubber eraser',
  'rugby ball',
  'rule',
  'running shoe',
  'safe',
  'safety pin',
  'saltshaker',
  'sandal',
  'sarong',
  'sax',
  'scabbard',
  'scale',
  'school bus',
  'schooner',
  'scoreboard',
  'screen',
  'screw',
  'screwdriver',
  'seat belt',
  'sewing machine',
  'shield',
  'shoe shop',
  'shoji',
  'shopping basket',
  'shopping cart',
  'shovel',
  'shower cap',
  'shower curtain',
  'ski',
  'ski mask',
  'sleeping bag',
  'slide rule',
  'sliding door',
  'slot',
  'snorkel',
  'snowmobile',
  'snowplow',
  'soap dispenser',
  'soccer ball',
  'sock',
  'solar dish',
  'sombrero',
  'soup bowl',
  'space bar',
  'space heater',
  'space shuttle',
  'spatula',
  'speedboat',
  'spider web',
  'spindle',
  'sports car',
  'spotlight',
  'stage',
  'steam locomotive',
  'steel arch bridge',
  'steel drum',
  'stethoscope',
  'stole',
  'stone wall',
  'stopwatch',
  'stove',
  'strainer',
  'streetcar',
  'stretcher',
  'studio couch',
  'stupa',
  'submarine',
  'suit',
  'sundial',
  'sunglass',
  'sunglasses',
  'sunscreen',
  'suspension bridge',
  'swab',
  'sweatshirt',
  'swimming trunks',
  'swing',
  'switch',
  'syringe',
  'table lamp',
  'tank',
  'tape player',
  'teapot',
  'teddy',
  'television',
  'tennis ball',
  'thatch',
  'theater curtain',
  'thimble',
  'thresher',
  'throne',
  'tile roof',
  'toaster',
  'tobacco shop',
  'toilet seat',
  'torch',
  'totem pole',
  'tow truck',
  'toyshop',
  'tractor',
  'trailer truck',
  'tray',
  'trench coat',
  'tricycle',
  'trimaran',
  'tripod',
  'triumphal arch',
  'trolleybus',
  'trombone',
  'tub',
  'turnstile',
  'typewriter keyboard',
  'umbrella',
  'unicycle',
  'upright',
  'vacuum',
  'vase',
  'vault',
  'velvet',
  'vending machine',
  'vestment',
  'viaduct',
  'violin',
  'volleyball',
  'waffle iron',
  'wall clock',
  'wallet',
  'wardrobe',
  'warplane',
  'washbasin',
  'washer',
  'water bottle',
  'water jug',
  'water tower',
  'whiskey jug',
  'whistle',
  'wig',
  'window screen',
  'window shade',
  'Windsor tie',
  'wine bottle',
  'wing',
  'wok',
  'wooden spoon',
  'wool',
  'worm fence',
  'wreck',
  'yawl',
  'yurt',
  'web site',
  'comic book',
  'crossword puzzle',
  'street sign',
  'traffic light',
  'book jacket',
  'menu',
  'plate',
  'guacamole',
  'consomme',
  'hot pot',
  'trifle',
  'ice cream',
  'ice lolly',
  'French loaf',
  'bagel',
  'pretzel',
  'cheeseburger',
  'hotdog',
  'mashed potato',
  'head cabbage',
  'broccoli',
  'cauliflower',
  'zucchini',
  'spaghetti squash',
  'acorn squash',
  'butternut squash',
  'cucumber',
  'artichoke',
  'bell pepper',
  'cardoon',
  'mushroom',
  'Granny Smith',
  'strawberry',
  'orange',
  'lemon',
  'fig',
  'pineapple',
  'banana',
  'jackfruit',
  'custard apple',
  'pomegranate',
  'hay',
  'carbonara',
  'chocolate sauce',
  'dough',
  'meat loaf',
  'pizza',
  'potpie',
  'burrito',
  'red wine',
  'espresso',
  'cup',
  'eggnog',
  'alp',
  'bubble',
  'cliff',
  'coral reef',
  'geyser',
  'lakeside',
  'promontory',
  'sandbar',
  'seashore',
  'valley',
  'volcano',
  'ballplayer',
  'groom',
  'scuba diver',
  'rapeseed',
  'daisy',
  "yellow lady's slipper",
  'corn',
  'acorn',
  'hip',
  'buckeye',
  'coral fungus',
  'agaric',
  'gyromitra',
  'stinkhorn',
  'earthstar',
  'hen-of-the-woods',
  'bolete',
  'ear',
  'toilet tissue'],
 'recipe': 'https://github.com/facebookresearch/SWAG',
 'license': 'https://github.com/facebookresearch/SWAG/blob/main/LICENSE',
 'num_params': 86859496,
 'min_size': (384, 384),
 '_metrics': {'ImageNet-1K': {'acc@1': 85.304, 'acc@5': 97.65}},
 '_ops': 55.484,
 '_file_size': 331.398,
 '_docs': '\n                These weights are learnt via transfer learning by end-to-end fine-tuning the original\n                `SWAG &lt;https://arxiv.org/abs/2201.08371&gt;`_ weights on ImageNet-1K data.\n            '}</code></pre>
</div>
</div>
<ul class="task-list">
<li><label><input type="checkbox">Store the categories in a variable to use them later</label></li>
</ul>
<div id="8dbe6e2a" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a>categories <span class="op">=</span> transformer_weights.meta[<span class="st">"categories"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>More info about Vision Transformer implementation in <code>torchvision</code> <a href="https://docs.pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html">here</a></p>
</div>
</div>
</div>
</section>
<section class="slide level2">

<ul class="task-list">
<li><label><input type="checkbox">Load the Inception V3 model using the pre-trained weights <code>inception_weights</code></label></li>
</ul>
<div id="af45d02d" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a>dl_model <span class="op">=</span> models.vit_b_16(transformer_weights, progress<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-2"><a></a></span>
<span id="cb4-3"><a></a>dl_model.<span class="bu">eval</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>VisionTransformer(
  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
  (encoder): Encoder(
    (dropout): Dropout(p=0.0, inplace=False)
    (layers): Sequential(
      (encoder_layer_0): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_1): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_2): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_3): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_4): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_5): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_6): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_7): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_8): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_9): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_10): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_11): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (heads): Sequential(
    (head): Linear(in_features=768, out_features=1000, bias=True)
  )
)</code></pre>
</div>
</div>
</section>
<section class="slide level2">

<ul class="task-list">
<li><label><input type="checkbox">Load a sample image to predict its category</label></li>
</ul>
<div id="0cb08424" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a><span class="im">import</span> skimage</span>
<span id="cb6-2"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-3"><a></a></span>
<span id="cb6-4"><a></a>sample_im <span class="op">=</span> skimage.data.rocket()</span>
<span id="cb6-5"><a></a>sample_im.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>(427, 640, 3)</code></pre>
</div>
</div>
<ul class="task-list">
<li><label><input type="checkbox">Visualize the sample image</label></li>
</ul>
<div id="238fad12" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a></a>plt.imshow(sample_im)</span>
<span id="cb8-2"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="DL_image_analysis_2_2_files/figure-revealjs/cell-6-output-1.png" width="608" height="416"></p>
</figure>
</div>
</div>
</div>
</section>
<section class="slide level2">

<ul class="task-list">
<li><label><input type="checkbox">Inspect what transforms are required by the pre-trained ViT model to work properly</label></li>
</ul>
<div id="e85125c7" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a></a>transformer_weights.transforms</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>functools.partial(&lt;class 'torchvision.transforms._presets.ImageClassification'&gt;, crop_size=384, resize_size=384, interpolation=&lt;InterpolationMode.BICUBIC: 'bicubic'&gt;)</code></pre>
</div>
</div>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p><code>functools.partial</code> is a function to define functions with static arguments. So üëÜ returns a function when it is called!</p>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>The transforms used by the Inception V3 are</p>
<ol type="1">
<li><p>resize the image to 384x384 pixels, and</p></li>
<li><p>normalize the values of the RGB channels.</p></li>
</ol>
</div>
</div>
</div>
</section>
<section class="slide level2">

<ul class="task-list">
<li><label><input type="checkbox">Define a preprocessing pipeline using the inception_weights.transforms() method. Add also a transformation from <code>numpy</code> arrays into torch tensors.</label></li>
</ul>
<div id="3ffdcd62" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a></a><span class="im">from</span> torchvision.transforms.v2 <span class="im">import</span> Compose, ToTensor</span>
<span id="cb11-2"><a></a></span>
<span id="cb11-3"><a></a>pipeline <span class="op">=</span> Compose([</span>
<span id="cb11-4"><a></a>  ToTensor(),</span>
<span id="cb11-5"><a></a>  transformer_weights.transforms()</span>
<span id="cb11-6"><a></a>])</span>
<span id="cb11-7"><a></a></span>
<span id="cb11-8"><a></a>pipeline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>Compose(
      ToTensor()
      ImageClassification(
      crop_size=[384]
      resize_size=[384]
      mean=[0.485, 0.456, 0.406]
      std=[0.229, 0.224, 0.225]
      interpolation=InterpolationMode.BICUBIC
  )
)</code></pre>
</div>
</div>
</section>
<section class="slide level2">

<ul class="task-list">
<li><label><input type="checkbox">Pre-process the sample image using our pipeline</label></li>
</ul>
<div id="7c095f0a" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a></a>sample_x <span class="op">=</span> pipeline(sample_im)</span>
<span id="cb13-2"><a></a><span class="bu">type</span>(sample_x), sample_x.shape, sample_x.<span class="bu">min</span>(), sample_x.<span class="bu">max</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>(torch.Tensor, torch.Size([3, 384, 384]), tensor(-1.9139), tensor(2.8376))</code></pre>
</div>
</div>
</section>
<section class="slide level2">

<ul class="task-list">
<li><label><input type="checkbox">Use the pre-trained model to predict the class of our sample image</label></li>
</ul>
<div class="callout callout-caution callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Caution</strong></p>
</div>
<div class="callout-content">
<p>Apply the model on sample_x[None, ‚Ä¶], so it is treated as a one-sample batch</p>
</div>
</div>
</div>
<div id="64d1cc97" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a></a>sample_y <span class="op">=</span> dl_model(sample_x[<span class="va">None</span>, ...])</span>
<span id="cb15-2"><a></a></span>
<span id="cb15-3"><a></a>sample_y.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>torch.Size([1, 1000])</code></pre>
</div>
</div>
</section>
<section class="slide level2">

<ul class="task-list">
<li><label><input type="checkbox">Show the categories with the highest <em>log-</em>probabilities.</label></li>
</ul>
<div id="01f56eba" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a></a>sample_y.argsort(dim<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>tensor([[641, 646, 398, 926, 340, 429, 990, 645, 924, 469, 346, 885, 351, 116,
         199,  62, 134, 661, 263, 678, 972, 594, 825, 976,  39, 397, 295, 880,
         793, 396, 853, 973, 817, 347, 433, 227, 975,  84,  32, 160, 841, 993,
         521, 462, 463, 544, 724, 518, 899,  25,  29, 832, 738, 982, 858, 290,
         939, 238, 500, 334, 659, 306, 329, 869, 488,  48,  93, 639, 834, 624,
         941, 843, 529, 966, 999, 343,  68, 801, 121, 809, 567, 373, 870,   0,
         197, 323, 522, 652, 725, 479, 297,  50, 573, 390, 381, 350, 823,  45,
         115, 344, 925, 515, 224, 865, 268, 235, 314, 706, 439, 173, 772, 453,
         214, 177, 883, 301, 419, 406, 554,  82, 150,  30,  85,   7, 655, 308,
         226, 849, 712, 369, 868, 828, 612, 460, 552, 109, 935, 959, 266,  88,
         244, 225, 400, 703, 259, 997, 964, 808, 302, 236, 709, 223,   5, 157,
          86, 649, 371, 179, 576, 928, 987, 960, 187,  36,  49, 417, 949, 291,
         762,  42, 328, 723, 305, 281, 673, 275,  98, 205, 349, 601, 854,  27,
           4, 449, 905, 392, 278, 904, 988, 366, 294, 245, 276, 292, 472, 216,
         933, 144, 989, 971, 963, 670,  72, 643, 714, 955, 401,  76, 108, 937,
         456, 512, 154, 304,  99, 289, 336, 775, 118, 133, 931, 836, 342, 962,
         385, 983,  41, 608, 260, 961, 474, 246, 330, 752, 690, 310, 611, 717,
         944, 459, 156, 345, 665, 602, 147,  75,  16, 208, 148, 539, 892, 207,
         280, 286,  92, 996, 648, 654, 981, 124, 787, 715, 663, 241,  23, 543,
         923, 909, 130, 277, 230, 288, 153, 948, 119, 879, 495, 943, 986, 211,
         233, 578, 790, 198, 331,  83, 175,  15, 575, 676, 234, 875, 741, 496,
         172, 627, 679, 353,  12, 421, 797, 658, 947, 167, 932, 531, 721, 969,
         307, 824, 237,  28, 897, 102, 504, 778, 719,  61, 919, 756, 248, 911,
         368,  17, 906, 584, 991, 729, 861, 430, 671,  14, 243, 887, 555, 874,
         322, 376, 100, 293, 958,  37, 616, 922, 501, 901,  38, 774, 170, 229,
         274, 339, 151,  67, 620, 326, 122, 965, 272,  51, 138, 193, 860, 936,
         434, 386, 873, 418, 432, 794, 666, 127, 445, 623, 713, 750, 212, 375,
         113, 549, 852, 638, 161, 614, 642,  20, 951, 270, 577, 635, 327, 364,
         940, 766, 640, 231, 796, 475, 917,  47, 128, 332, 783,  69, 186, 204,
         367, 748, 312,  31,  26, 215, 174,  71, 443, 180, 913, 866, 910, 468,
          60, 159, 104, 210, 416, 607, 300, 194, 945, 736, 415,  97, 884,  73,
         968, 391, 710, 771, 533, 192, 572,  53, 815, 651, 303, 735, 393,  64,
         566, 387, 746,  34, 165, 731,  66,  18, 399, 355, 155,  33, 129,  10,
         930, 956, 209, 110, 389, 934,  95, 499, 760, 363, 610, 598, 256, 335,
         213, 502, 196, 271, 444, 705,  55, 548, 927, 374, 980, 618, 316, 219,
         698, 979,  57, 361, 785,  21, 603, 337, 220, 921, 546, 876, 506, 158,
         169, 647, 842, 242, 262, 162,  91, 838, 770, 454, 452,  80, 324, 938,
         888, 136, 542, 672, 112, 617, 534, 768, 283, 864, 143, 953, 348, 633,
         183, 526, 101, 886, 178, 605, 560, 221, 253,  90, 734, 896,  94, 889,
         903, 946, 630, 749, 106, 568,  43, 918, 464,  54, 878, 287, 476, 510,
         751,  77, 315, 789, 593, 486, 321, 379, 667, 282, 569, 513, 985,  79,
         788, 636, 716, 826, 942, 370, 269, 821, 163, 240, 107, 320, 200, 509,
         492, 952, 780, 586, 455, 114, 228, 103, 265, 831, 325, 377, 564, 791,
         684,  58,  70, 388, 247, 420, 722, 457, 970, 126,  59,  24, 541, 252,
         779, 239, 915,  44, 805, 111, 296, 422, 466, 490, 362, 311, 436, 309,
         264, 957, 195, 358, 168,   9, 137, 588, 890, 333, 835, 257, 258, 284,
         669, 249,  74, 201, 483, 950, 992, 580, 123,  46, 395, 357, 820, 916,
         813,   3,  78, 769,  22, 806, 814, 802, 255, 285, 508, 382, 261, 697,
         535, 203, 467,  52, 338, 810, 319,  81,  96, 352, 458, 767, 545, 739,
         764, 446, 582, 596, 587, 726, 893, 800,  35, 441, 273, 675, 570, 176,
         125, 480, 747, 435, 538, 581,   6,  13, 105, 354, 171, 859, 728, 407,
         149,   1, 613, 622, 317, 978,  40, 424, 528, 699, 360, 411, 146, 135,
         152, 365, 839, 359, 621, 696, 798, 732, 254, 743,  87, 730, 190, 579,
         609, 591, 117, 777, 120, 977, 685, 489, 461, 615, 758, 318, 737, 140,
         556, 425, 589, 998, 447, 442,  65, 691, 600, 428, 995, 855, 299, 164,
         218, 487, 776, 759, 384, 394, 511, 142, 188, 631, 356,  89, 465, 491,
         881, 202, 984, 380, 520, 590, 830, 827, 550, 505, 250, 232, 383, 372,
         131, 891,   2, 378, 524, 403, 819, 423, 857, 850, 745, 574, 493, 954,
         516, 547, 485, 478, 829, 902, 139, 626, 681, 840, 689, 592, 438,  19,
         413, 537,  11, 298, 145, 251, 217, 822, 781, 816, 660, 597, 222, 530,
         862,  63, 664, 967,   8, 604, 166, 532, 184, 206, 686, 481, 182, 440,
         410, 628, 562, 551, 700, 599, 451, 848, 687, 804, 606, 753, 693, 267,
         313,  56, 414, 677, 701, 448, 482, 402, 692, 527, 141, 727, 629, 662,
         863, 929, 837, 803, 795, 784, 132, 656, 341, 702, 898, 683, 912, 695,
         470, 914, 625, 426, 974, 559, 740, 497, 742, 525, 191, 846, 711, 786,
         792, 409, 507, 757, 498, 181, 619, 718, 523, 704, 856, 427, 765, 484,
         707, 553, 847, 185, 694, 920, 907, 650, 680, 595, 450, 477, 563, 473,
         408, 189, 585, 514, 844, 994, 763, 634, 412, 708, 894, 536, 845, 851,
         773, 761, 471, 279, 871, 688, 431, 644, 674, 682, 867, 782, 811, 799,
         720, 558, 494, 877, 668, 833, 583, 637, 503, 557, 882, 632, 561, 895,
         653, 519, 908, 404, 571, 405, 565, 818, 733, 872, 517, 807, 437, 755,
         900, 540, 754, 744, 657, 812]])</code></pre>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>The model‚Äôs output are the log-probabilities of <code>sample_x</code> belonging to each of the 1000 classes.</p>
</div>
</div>
</div>
</section>
<section class="slide level2">

<ul class="task-list">
<li><label><input type="checkbox">Use the list of categories to translate the predicted class index into its category.</label></li>
</ul>
<div id="91a87dc7" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a></a>sorted_predicted_classes <span class="op">=</span> sample_y.argsort(dim<span class="op">=</span><span class="dv">1</span>, descending<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>, :<span class="dv">10</span>]</span>
<span id="cb19-2"><a></a>sorted_probs <span class="op">=</span> torch.softmax(sample_y, dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">0</span>, sorted_predicted_classes]</span>
<span id="cb19-3"><a></a></span>
<span id="cb19-4"><a></a><span class="cf">for</span> idx, prob <span class="kw">in</span> <span class="bu">zip</span>(sorted_predicted_classes, sorted_probs):</span>
<span id="cb19-5"><a></a>    <span class="bu">print</span>(categories[idx], <span class="st">"</span><span class="sc">%3.2f</span><span class="st"> </span><span class="sc">%%</span><span class="st">"</span> <span class="op">%</span> (prob <span class="op">*</span> <span class="dv">100</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>space shuttle 79.91 %
missile 12.59 %
projectile 6.04 %
radio 0.11 %
drilling platform 0.08 %
water tower 0.05 %
radio telescope 0.05 %
beacon 0.04 %
solar dish 0.02 %
crane 0.02 %</code></pre>
</div>
</div>
</section>
<section id="inspect-the-self-attention-operations-of-the-vit" class="slide level2">
<h2>Inspect the self-attention operations of the ViT</h2>
<div id="5df8f13d" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a></a>dl_model.encoder.layers[<span class="op">-</span><span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>EncoderBlock(
  (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=768, out_features=3072, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=3072, out_features=768, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)</code></pre>
</div>
</div>
</section></section>
<section>
<section id="integrate-a-mechanism-to-review-the-attention-maps-of-the-model" class="title-slide slide level1 center">
<h1>Integrate a mechanism to review the attention maps of the model</h1>

</section>
<section id="redefine-some-of-transformer-operations-to-enable-storing-the-attention-weights" class="slide level2 scrollable">
<h2>Redefine some of transformer operations to enable storing the attention weights</h2>
<div id="5d7f8131" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb23-2"><a></a><span class="im">from</span> typing <span class="im">import</span> Callable</span>
<span id="cb23-3"><a></a><span class="im">from</span> timm.models.vision_transformer <span class="im">import</span> Attention</span>
<span id="cb23-4"><a></a></span>
<span id="cb23-5"><a></a><span class="kw">class</span> EncoderBlockAttnMap(models.vision_transformer.EncoderBlock):</span>
<span id="cb23-6"><a></a>    <span class="co">"""Transformer encoder block."""</span></span>
<span id="cb23-7"><a></a></span>
<span id="cb23-8"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb23-9"><a></a>                 num_heads: <span class="bu">int</span>,</span>
<span id="cb23-10"><a></a>                 hidden_dim: <span class="bu">int</span>,</span>
<span id="cb23-11"><a></a>                 mlp_dim: <span class="bu">int</span>,</span>
<span id="cb23-12"><a></a>                 dropout: <span class="bu">float</span>,</span>
<span id="cb23-13"><a></a>                 attention_dropout: <span class="bu">float</span>,</span>
<span id="cb23-14"><a></a>                 norm_layer: Callable[..., torch.nn.Module] <span class="op">=</span> partial(torch.nn.LayerNorm, eps<span class="op">=</span><span class="fl">1e-6</span>)):</span>
<span id="cb23-15"><a></a>        <span class="co"># The definition is the same, only the forward function changes &lt;------------------------------------</span></span>
<span id="cb23-16"><a></a>        <span class="bu">super</span>(EncoderBlockAttnMap, <span class="va">self</span>).<span class="fu">__init__</span>(num_heads, hidden_dim, mlp_dim, dropout, attention_dropout, norm_layer)</span>
<span id="cb23-17"><a></a>        <span class="va">self</span>.self_attention <span class="op">=</span> Attention(hidden_dim, num_heads, attn_drop<span class="op">=</span>attention_dropout, proj_drop<span class="op">=</span><span class="fl">0.0</span>, norm_layer<span class="op">=</span>norm_layer, qkv_bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-18"><a></a></span>
<span id="cb23-19"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>: torch.Tensor):</span>
<span id="cb23-20"><a></a>        <span class="co"># with torch.autograd.graph.save_on_cpu(pin_memory=True):</span></span>
<span id="cb23-21"><a></a>        torch._assert(<span class="bu">input</span>.dim() <span class="op">==</span> <span class="dv">3</span>, <span class="ss">f"Expected (batch_size, seq_length, hidden_dim) got </span><span class="sc">{</span><span class="bu">input</span><span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-22"><a></a>        x <span class="op">=</span> <span class="va">self</span>.ln_1(<span class="bu">input</span>)</span>
<span id="cb23-23"><a></a></span>
<span id="cb23-24"><a></a>        <span class="co"># Modify this line, so we get the attention map from the self attention modules &lt;--------------------</span></span>
<span id="cb23-25"><a></a>        x <span class="op">=</span> <span class="va">self</span>.self_attention(x)</span>
<span id="cb23-26"><a></a></span>
<span id="cb23-27"><a></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb23-28"><a></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="bu">input</span></span>
<span id="cb23-29"><a></a></span>
<span id="cb23-30"><a></a>        y <span class="op">=</span> <span class="va">self</span>.ln_2(x)</span>
<span id="cb23-31"><a></a>        y <span class="op">=</span> <span class="va">self</span>.mlp(y)</span>
<span id="cb23-32"><a></a></span>
<span id="cb23-33"><a></a>        <span class="co"># Return the attention map along with the encoder output &lt;-------------------------------------------</span></span>
<span id="cb23-34"><a></a>        <span class="cf">return</span> x <span class="op">+</span> y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="0cfb652e" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a></a><span class="im">from</span> collections <span class="im">import</span> OrderedDict</span>
<span id="cb24-2"><a></a></span>
<span id="cb24-3"><a></a><span class="kw">class</span> EncoderAttnMap(models.vision_transformer.Encoder):</span>
<span id="cb24-4"><a></a>    <span class="co">"""Transformer Model Encoder for sequence to sequence translation."""</span></span>
<span id="cb24-5"><a></a></span>
<span id="cb24-6"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb24-7"><a></a>        <span class="va">self</span>,</span>
<span id="cb24-8"><a></a>        seq_length: <span class="bu">int</span>,</span>
<span id="cb24-9"><a></a>        num_layers: <span class="bu">int</span>,</span>
<span id="cb24-10"><a></a>        num_heads: <span class="bu">int</span>,</span>
<span id="cb24-11"><a></a>        hidden_dim: <span class="bu">int</span>,</span>
<span id="cb24-12"><a></a>        mlp_dim: <span class="bu">int</span>,</span>
<span id="cb24-13"><a></a>        dropout: <span class="bu">float</span>,</span>
<span id="cb24-14"><a></a>        attention_dropout: <span class="bu">float</span>,</span>
<span id="cb24-15"><a></a>        norm_layer: Callable[..., torch.nn.Module] <span class="op">=</span> partial(torch.nn.LayerNorm, eps<span class="op">=</span><span class="fl">1e-6</span>),</span>
<span id="cb24-16"><a></a>    ):</span>
<span id="cb24-17"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(seq_length, num_layers, num_heads, hidden_dim, mlp_dim, dropout, attention_dropout, norm_layer)</span>
<span id="cb24-18"><a></a></span>
<span id="cb24-19"><a></a>        layers: OrderedDict[<span class="bu">str</span>, nn.Module] <span class="op">=</span> OrderedDict()</span>
<span id="cb24-20"><a></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_layers):</span>
<span id="cb24-21"><a></a>            <span class="co"># Use the modified encoder block &lt;---------------------------------------------------------------</span></span>
<span id="cb24-22"><a></a>            layers[<span class="ss">f"encoder_layer_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>] <span class="op">=</span> EncoderBlockAttnMap(</span>
<span id="cb24-23"><a></a>                num_heads,</span>
<span id="cb24-24"><a></a>                hidden_dim,</span>
<span id="cb24-25"><a></a>                mlp_dim,</span>
<span id="cb24-26"><a></a>                dropout,</span>
<span id="cb24-27"><a></a>                attention_dropout,</span>
<span id="cb24-28"><a></a>                norm_layer,</span>
<span id="cb24-29"><a></a>            )</span>
<span id="cb24-30"><a></a></span>
<span id="cb24-31"><a></a>        <span class="va">self</span>.layers <span class="op">=</span> torch.nn.Sequential(layers)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="e7638947" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a></a><span class="co"># Redefine the classifier head to have access to the attention maps</span></span>
<span id="cb25-2"><a></a><span class="kw">class</span> ViTAttnEnabled(models.vision_transformer.VisionTransformer):</span>
<span id="cb25-3"><a></a>    <span class="co">"""Implementation of the classifier head from the ViT-B-16 architecture.</span></span>
<span id="cb25-4"><a></a><span class="co">    """</span></span>
<span id="cb25-5"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, image_size, patch_size<span class="op">=</span><span class="dv">14</span>, num_layers<span class="op">=</span><span class="dv">32</span>, num_heads<span class="op">=</span><span class="dv">16</span>, hidden_dim<span class="op">=</span><span class="dv">1280</span>, mlp_dim<span class="op">=</span><span class="dv">5120</span>, <span class="op">**</span>kwargs):      </span>
<span id="cb25-6"><a></a>        <span class="bu">super</span>(ViTAttnEnabled, <span class="va">self</span>).<span class="fu">__init__</span>(</span>
<span id="cb25-7"><a></a>            image_size,</span>
<span id="cb25-8"><a></a>            patch_size<span class="op">=</span>patch_size,</span>
<span id="cb25-9"><a></a>            num_layers<span class="op">=</span>num_layers,</span>
<span id="cb25-10"><a></a>            num_heads<span class="op">=</span>num_heads,</span>
<span id="cb25-11"><a></a>            hidden_dim<span class="op">=</span>hidden_dim,</span>
<span id="cb25-12"><a></a>            mlp_dim<span class="op">=</span>mlp_dim,</span>
<span id="cb25-13"><a></a>            <span class="op">**</span>kwargs)</span>
<span id="cb25-14"><a></a></span>
<span id="cb25-15"><a></a>        <span class="co"># Change the encoder to the modified ekwargsoder that returns the attention maps &lt;-----------</span></span>
<span id="cb25-16"><a></a>        <span class="va">self</span>.encoder <span class="op">=</span> EncoderAttnMap(</span>
<span id="cb25-17"><a></a>            <span class="va">self</span>.seq_length,</span>
<span id="cb25-18"><a></a>            num_layers<span class="op">=</span>num_layers,</span>
<span id="cb25-19"><a></a>            num_heads<span class="op">=</span>num_heads,</span>
<span id="cb25-20"><a></a>            hidden_dim<span class="op">=</span>hidden_dim,</span>
<span id="cb25-21"><a></a>            mlp_dim<span class="op">=</span>mlp_dim,</span>
<span id="cb25-22"><a></a>            dropout<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb25-23"><a></a>            attention_dropout<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb25-24"><a></a>            norm_layer<span class="op">=</span>partial(torch.nn.LayerNorm, eps<span class="op">=</span><span class="fl">1e-6</span>)</span>
<span id="cb25-25"><a></a>        )</span>
<span id="cb25-26"><a></a></span>
<span id="cb25-27"><a></a>        <span class="va">self</span>.attentions <span class="op">=</span> []</span>
<span id="cb25-28"><a></a>        <span class="va">self</span>.attentions_gradients <span class="op">=</span> []</span>
<span id="cb25-29"><a></a></span>
<span id="cb25-30"><a></a>    <span class="kw">def</span> get_attention(<span class="va">self</span>, module, <span class="bu">input</span>, output):</span>
<span id="cb25-31"><a></a>        <span class="va">self</span>.attentions.append(output.detach())</span>
<span id="cb25-32"><a></a></span>
<span id="cb25-33"><a></a>    <span class="kw">def</span> get_attention_gradients(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb25-34"><a></a>        <span class="va">self</span>.attentions_gradients.append(grad_input[<span class="dv">0</span>].detach())</span>
<span id="cb25-35"><a></a></span>
<span id="cb25-36"><a></a>    <span class="kw">def</span> register_attn_grad_hooks(<span class="va">self</span>):</span>
<span id="cb25-37"><a></a>        <span class="cf">for</span> name, module <span class="kw">in</span> <span class="va">self</span>.named_modules():</span>
<span id="cb25-38"><a></a>            <span class="cf">if</span> <span class="st">"self_attention.norm"</span> <span class="kw">in</span> name:</span>
<span id="cb25-39"><a></a>                module.register_forward_hook(<span class="va">self</span>.get_attention)</span>
<span id="cb25-40"><a></a>                module.register_full_backward_hook(<span class="va">self</span>.get_attention_gradients)</span>
<span id="cb25-41"><a></a></span>
<span id="cb25-42"><a></a>    <span class="kw">def</span> clear_attentions(<span class="va">self</span>):</span>
<span id="cb25-43"><a></a>        <span class="va">self</span>.attentions.clear()</span>
<span id="cb25-44"><a></a>        <span class="va">self</span>.attentions_gradients.clear()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="use-a-modified-vit-model-that-enables-tracking-its-attention-weights" class="slide level2">
<h2>Use a modified ViT model that enables tracking its attention weights</h2>
<ul>
<li>[] Initialize a ViT with the <code>Vit-B-16</code> architecture</li>
</ul>
<div id="ca59c829" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a></a>vit_model_self_attn <span class="op">=</span> ViTAttnEnabled(</span>
<span id="cb26-2"><a></a>        image_size<span class="op">=</span><span class="dv">384</span>,</span>
<span id="cb26-3"><a></a>        patch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb26-4"><a></a>        num_heads<span class="op">=</span><span class="dv">12</span>,</span>
<span id="cb26-5"><a></a>        num_layers<span class="op">=</span><span class="dv">12</span>,</span>
<span id="cb26-6"><a></a>        hidden_dim<span class="op">=</span><span class="dv">768</span>,</span>
<span id="cb26-7"><a></a>        mlp_dim<span class="op">=</span><span class="dv">3072</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section class="slide level2">

<ul class="task-list">
<li><label><input type="checkbox">Map the names of the modules in the orginal <code>torchvision</code> Attention layer, to the names of the <code>timm</code> Attention layer</label></li>
</ul>
<div id="cad0d14b" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a></a>name_map <span class="op">=</span> {</span>
<span id="cb27-2"><a></a>    <span class="st">"in_proj_weight"</span>: <span class="st">"qkv.weight"</span>,</span>
<span id="cb27-3"><a></a>    <span class="st">"in_proj_bias"</span>: <span class="st">"qkv.bias"</span>,</span>
<span id="cb27-4"><a></a>    <span class="st">"out_proj.weight"</span>: <span class="st">"proj.weight"</span>,</span>
<span id="cb27-5"><a></a>    <span class="st">"out_proj.bias"</span>: <span class="st">"proj.bias"</span></span>
<span id="cb27-6"><a></a>}</span>
<span id="cb27-7"><a></a></span>
<span id="cb27-8"><a></a>transformer_weights_dict <span class="op">=</span> transformer_weights.get_state_dict(progress<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb27-9"><a></a></span>
<span id="cb27-10"><a></a>vit_weights <span class="op">=</span> {}</span>
<span id="cb27-11"><a></a><span class="cf">for</span> k, v <span class="kw">in</span> transformer_weights_dict.items():</span>
<span id="cb27-12"><a></a>    k_old <span class="op">=</span> <span class="bu">list</span>(<span class="bu">filter</span>(<span class="kw">lambda</span> n: n <span class="kw">in</span> k, name_map.keys()))</span>
<span id="cb27-13"><a></a>    <span class="cf">if</span> <span class="bu">len</span>(k_old):</span>
<span id="cb27-14"><a></a>        k_old <span class="op">=</span> k_old[<span class="dv">0</span>]</span>
<span id="cb27-15"><a></a>        old_name <span class="op">=</span> k.split(k_old)[<span class="dv">0</span>]</span>
<span id="cb27-16"><a></a>        new_name <span class="op">=</span> old_name <span class="op">+</span> name_map[k_old]</span>
<span id="cb27-17"><a></a>    <span class="cf">else</span>:</span>
<span id="cb27-18"><a></a>        new_name <span class="op">=</span> k</span>
<span id="cb27-19"><a></a></span>
<span id="cb27-20"><a></a>    vit_weights[new_name] <span class="op">=</span> v</span>
<span id="cb27-21"><a></a></span>
<span id="cb27-22"><a></a>vit_model_self_attn.load_state_dict(vit_weights)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>&lt;All keys matched successfully&gt;</code></pre>
</div>
</div>
</section>
<section class="slide level2">

<ul class="task-list">
<li><label><input type="checkbox">Enable the tracking of the attention weights</label></li>
</ul>
<div id="9e6e1ecf" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a></a>vit_model_self_attn.register_attn_grad_hooks()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="apply-the-vit-class-prediction-on-an-image-and-compute-the-corresponding-attention-map" class="slide level2">
<h2>Apply the ViT class prediction on an image and compute the corresponding attention map</h2>
<ul class="task-list">
<li><label><input type="checkbox">Run the <code>forward</code> operation of the ViT model and the respective <code>backward</code> opreation to compute and store the attention weights</label></li>
</ul>
<div id="bce9041b" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a></a>sample_im <span class="op">=</span> skimage.io.imread(<span class="st">"https://r0k.us/graphics/kodak/kodak/kodim20.png"</span>)</span>
<span id="cb30-2"><a></a></span>
<span id="cb30-3"><a></a>vit_model_self_attn.clear_attentions()</span>
<span id="cb30-4"><a></a></span>
<span id="cb30-5"><a></a>sample_x <span class="op">=</span> pipeline(sample_im)</span>
<span id="cb30-6"><a></a></span>
<span id="cb30-7"><a></a><span class="cf">with</span> torch.autograd.graph.save_on_cpu(pin_memory<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb30-8"><a></a>    sample_y <span class="op">=</span> vit_model_self_attn(sample_x[<span class="va">None</span>, ...])</span>
<span id="cb30-9"><a></a></span>
<span id="cb30-10"><a></a>attn_class <span class="op">=</span> torch.argmax(sample_y, dim<span class="op">=</span><span class="dv">1</span>).item()</span>
<span id="cb30-11"><a></a>attn_class <span class="op">=</span> torch.LongTensor([attn_class])</span>
<span id="cb30-12"><a></a>attn_class <span class="op">=</span> torch.nn.functional.one_hot(attn_class, num_classes<span class="op">=</span>sample_y.shape[<span class="dv">1</span>])</span>
<span id="cb30-13"><a></a></span>
<span id="cb30-14"><a></a>attn_class <span class="op">=</span> torch.<span class="bu">sum</span>(attn_class <span class="op">*</span> sample_y)</span>
<span id="cb30-15"><a></a></span>
<span id="cb30-16"><a></a>vit_model_self_attn.zero_grad()</span>
<span id="cb30-17"><a></a>attn_class.backward()</span>
<span id="cb30-18"><a></a></span>
<span id="cb30-19"><a></a>attn_out <span class="op">=</span> [attn_tensor.clone() <span class="cf">for</span> attn_tensor <span class="kw">in</span> vit_model_self_attn.attentions]</span>
<span id="cb30-20"><a></a>grad_attn_out <span class="op">=</span> [attn_tensor.clone() <span class="cf">for</span> attn_tensor <span class="kw">in</span> vit_model_self_attn.attentions_gradients]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="roll-out-the-attention-maps" class="slide level2">
<h2>Roll out the attention maps</h2>
<ul class="task-list">
<li><label><input type="checkbox">The attention map can be computed as the accumulation of the attention weigths of each <em>Encoder</em> layer of the Transformer</label></li>
</ul>
<div id="223aaf61" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a></a>attn_rollout <span class="op">=</span> torch.eye(attn_out[<span class="dv">0</span>].size(<span class="dv">1</span>))[<span class="va">None</span>, ...]</span>
<span id="cb31-2"><a></a></span>
<span id="cb31-3"><a></a><span class="cf">for</span> attn_map, attn_grad <span class="kw">in</span> <span class="bu">zip</span>(attn_out, grad_attn_out):</span>
<span id="cb31-4"><a></a>    <span class="cf">if</span> attn_grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb31-5"><a></a>        attn_map <span class="op">=</span> attn_map <span class="op">*</span> attn_grad</span>
<span id="cb31-6"><a></a>        attn_map[attn_map <span class="op">&lt;</span> <span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb31-7"><a></a></span>
<span id="cb31-8"><a></a>    attn_map, _ <span class="op">=</span> torch.topk(attn_map, <span class="dv">10</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb31-9"><a></a>    attn_map <span class="op">=</span> attn_map.mean(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb31-10"><a></a></span>
<span id="cb31-11"><a></a>    <span class="co"># Normalize attention map</span></span>
<span id="cb31-12"><a></a>    attn_map <span class="op">=</span> attn_map <span class="op">+</span> torch.eye(attn_map.size(<span class="dv">1</span>), device<span class="op">=</span>attn_map.device)[<span class="va">None</span>, ...]</span>
<span id="cb31-13"><a></a>    attn_map <span class="op">=</span> attn_map <span class="op">/</span> attn_map.<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb31-14"><a></a></span>
<span id="cb31-15"><a></a>    attn_rollout <span class="op">=</span> torch.matmul(attn_map, attn_rollout)</span>
<span id="cb31-16"><a></a></span>
<span id="cb31-17"><a></a>attn_rollout.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>torch.Size([1, 577, 577])</code></pre>
</div>
</div>
</section>
<section class="slide level2">

<ul class="task-list">
<li><label><input type="checkbox">Keep only the attention weights of the class token with respect to the spatial tokens</label></li>
</ul>
<div id="68b264e8" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a></a>attn_rollout <span class="op">=</span> attn_rollout[:, :<span class="dv">1</span>, <span class="dv">1</span>:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul class="task-list">
<li><label><input type="checkbox">Reshape the attention weights into the original patch coordinates</label></li>
</ul>
<div id="24a946f9" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a></a>attn_rollout <span class="op">=</span> attn_rollout.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">24</span> <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb34-2"><a></a>attn_rollout.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>torch.Size([1, 1, 576])</code></pre>
</div>
</div>
<div id="e762e2e4" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a></a>attn_rollout <span class="op">=</span> torch.nn.functional.fold(attn_rollout.transpose(<span class="dv">1</span>, <span class="dv">2</span>),</span>
<span id="cb36-2"><a></a>                      (<span class="dv">24</span>, <span class="dv">24</span>),</span>
<span id="cb36-3"><a></a>                      kernel_size<span class="op">=</span>(<span class="dv">24</span>, <span class="dv">24</span>),</span>
<span id="cb36-4"><a></a>                      stride<span class="op">=</span>(<span class="dv">24</span>, <span class="dv">24</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="00a4c983" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a></a>attn_rollout <span class="op">=</span> attn_rollout.squeeze()</span>
<span id="cb37-2"><a></a></span>
<span id="cb37-3"><a></a>attn_rollout <span class="op">=</span> attn_rollout <span class="op">/</span> torch.<span class="bu">max</span>(attn_rollout)</span>
<span id="cb37-4"><a></a></span>
<span id="cb37-5"><a></a>attn_rollout.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>torch.Size([24, 24])</code></pre>
</div>
</div>
</section>
<section id="visualize-the-attention-map-computed-from-the-attention-weights" class="slide level2">
<h2>Visualize the attention map computed from the attention weights</h2>
<ul>
<li>[] Show an overlay of the attention map over the original image</li>
</ul>
<div id="2da4a589" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a></a>plt.imshow(sample_im)</span>
<span id="cb39-2"><a></a>plt.imshow(attn_rollout, cmap<span class="op">=</span><span class="st">"magma"</span>, extent<span class="op">=</span>(<span class="dv">0</span>, sample_im.shape[<span class="dv">1</span>], sample_im.shape[<span class="dv">0</span>], <span class="dv">0</span>), alpha<span class="op">=</span><span class="fl">0.75</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="DL_image_analysis_2_2_files/figure-revealjs/cell-26-output-1.png" width="608" height="416"></p>
</figure>
</div>
</div>
</div>


</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>