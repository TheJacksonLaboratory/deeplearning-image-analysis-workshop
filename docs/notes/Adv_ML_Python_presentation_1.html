<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.5.56">

  <meta name="author" content="Fernando Cervantes (fernando.cervantes@jax.org)">
  <title>Advanced Machine Learning with Python workshop – Advanced Machine Learning with Python (Session 1)</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.3/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-jnSuA4Ss2PkkikSOLtYs8BlYIeeIK1h99ty4YfvRPAlzr377vr3CXDb7sb7eEEBYjDtcYj+AjBH3FLv5uSJuXg==" crossorigin="anonymous">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Advanced Machine Learning with Python (Session 1)</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Fernando Cervantes (fernando.cervantes@jax.org) 
</div>
</div>
</div>

</section>
<section id="workshop-outcomes" class="slide level2">
<h2>Workshop outcomes</h2>
<ul>
<li>Understand the process of training ML models.</li>
<li>Load pre-trained ML models and fine-tune them with new data.</li>
<li>Evaluate the performance of ML models.</li>
<li>Adapt ML models for different tasks from pre-trained models.</li>
</ul>
<h3 id="materials">Materials</h3>
<p><a href="https://colab.research.google.com/drive/1c0Qw8ICTIl2yE2ChJZ4Fi6QNIoVUeWCx" class="btn btn-outline-primary" role="button" target="”_blank”">Open notebook in Colab</a> <a href="https://colab.research.google.com/drive/1QyVxdnwNrP1uGnsosClYsQ1hf7pF5ARF" class="btn btn-outline-primary" role="button" target="”_blank”">View solutions</a></p>
</section>
<section>
<section id="setup-environment" class="title-slide slide level1 center">
<h1>0. Setup environment</h1>

</section>
<section id="select-runtime-and-connect" class="slide level2">
<h2>Select runtime and connect</h2>
<p>On the top right corner of the page, click the drop-down arrow to the right of the <code>Connect</code> button and select <code>Change runtime type</code>.</p>

<img data-src="../imgs/connect_runtime.png" style="width:50.0%" class="r-stretch"></section>
<section class="slide level2">

<p>Make sure <code>Python 3</code> runtime is selected. For this part of the workshop <code>CPU</code> acceleration is enough.</p>

<img data-src="../imgs/select_runtime.png" style="width:50.0%" class="r-stretch"></section>
<section class="slide level2">

<p>Now we can connect to the runtime by clicking <code>Connect</code>. This will create a <strong>V</strong>irtual <strong>M</strong>achine (<strong>VM</strong>) with compute resources we can use for a limited amount of time.</p>
<p><img data-src="../imgs/connect.png" style="height:25.0%"></p>
<div class="callout callout-caution callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Caution</strong></p>
</div>
<div class="callout-content">
<p>In free Colab accounts these resources are not guaranteed and can be taken away without notice (preemptible machines).</p>
<p>Data stored in this runtime will be lost if not moved into other storage when the runtime is deleted.</p>
</div>
</div>
</div>
</section></section>
<section>
<section id="what-is-machine-learning-ml" class="title-slide slide level1 center">
<h1>1. What is <strong>M</strong>achine <strong>L</strong>earning (<strong>ML</strong>)?</h1>

</section>
<section id="machine-learning-ml" class="slide level2">
<h2><strong>M</strong>achine <strong>L</strong>earning (<strong>ML</strong>)</h2>
<p>Sub-field of <strong>Artificial Intelligence</strong> that develops methods to address tasks that require human intelligence</p>

<img data-src="../imgs/Diagram_AI.png" style="width:50.0%" class="r-stretch"></section>
<section id="artificial-intelligence-tasks" class="slide level2">
<h2>Artificial intelligence tasks</h2>
</section>
<section id="common-tasks" class="slide level2">
<h2>Common tasks</h2>
<div class="columns">
<div class="column" style="width:30%;">
<p><strong>Classification</strong></p>
<p><img data-src="../imgs/Object.png"></p>
<p>what is this?</p>
</div><div class="column" style="width:30%;">
<p><strong>Detection</strong></p>
<p><img data-src="../imgs/Detect.png"></p>
<p>where is something?</p>
</div><div class="column" style="width:30%;">
<p><strong>Segmentation</strong></p>
<p><img data-src="../imgs/Segment.png"></p>
<p>where <em>specifically</em> is something?</p>
</div></div>
</section>
<section class="slide level2">

<h3 id="more-tasks-addressed-in-recent-years">More tasks addressed in recent years</h3>
<ul>
<li><p>Style transference</p></li>
<li><p>Compression of image/video/etc…</p></li>
<li><p>Generation of content</p></li>
<li><p>Language processing</p></li>
</ul>
</section></section>
<section>
<section id="types-of-machine-learning" class="title-slide slide level1 center">
<h1>Types of machine learning</h1>

</section>
<section id="types-of-machine-learning-1" class="slide level2">
<h2>Types of machine learning</h2>
<h3 id="depending-on-how-the-model-is-trained">Depending on how the model is trained</h3>
<ul>
<li><p>Supervised</p></li>
<li><p>Unsupervised</p></li>
<li><p>Weakly supervised</p></li>
<li><p>Reinforced</p></li>
<li><p>…</p></li>
</ul>
<aside class="notes">
<p>Supervised learning: teach the machine to perform a task with a set of inputs and their respective expected outcome (<span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>).</p>
<p>Unsupervised learning: let the machine learn to perform a task on its own (<span class="math inline">\(X\)</span>) without any specific expected outcome.</p>
<p>Weakly supervised: teach the machine to perform a task using a limited set of expected outcomes.</p>
<p>Reinforced learning: let the machine learn to perform a task on its own, then give it a reward relative to its performance.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="inputs-and-outputs" class="title-slide slide level1 center">
<h1>Inputs and outputs</h1>

</section>
<section id="inputs-and-outputs-1" class="slide level2">
<h2>Inputs and outputs</h2>
<p>For a task, we want to <em>model</em> the <strong>outcome/output</strong> (<span class="math inline">\(y\)</span>) obtained by a given <strong>input</strong> (<span class="math inline">\(x\)</span>)</p>
<p><span class="math inline">\(f(x) \approx y\)</span></p>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>The complete set of (<span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>) pairs is known as dataset (<span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>).</p>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Inputs can be virtually anything, including images, texts, video, audio, electrical signals, etc.</p>
<p>While outputs are expected to be some meaningful piece of information, such as a category, position, value, etc.</p>
</div>
</div>
</div>
</section>
<section id="use-case-image-classification-with-the-cifar-100-dataset" class="slide level2">
<h2>Use case: Image classification with the CIFAR-100 dataset</h2>
<ul class="task-list">
<li><label><input type="checkbox">Load the CIFAR-100 dataset from torchvision.datasets</label></li>
</ul>
<div id="da840dd9" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a></a><span class="im">import</span> torchvision</span>
<span id="cb1-3"><a></a></span>
<span id="cb1-4"><a></a>cifar_ds <span class="op">=</span> torchvision.datasets.CIFAR100(root<span class="op">=</span><span class="st">"/tmp"</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Files already downloaded and verified</code></pre>
</div>
</div>
<ul class="task-list">
<li><label><input type="checkbox">Explore the CIFAR-100 dataset</label></li>
</ul>
<div id="6d212c36" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a>x_im, y <span class="op">=</span> cifar_ds[<span class="dv">0</span>]</span>
<span id="cb3-2"><a></a></span>
<span id="cb3-3"><a></a><span class="bu">len</span>(cifar_ds), <span class="bu">type</span>(x_im), <span class="bu">type</span>(y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>(50000, PIL.Image.Image, int)</code></pre>
</div>
</div>
<div id="d8892205" class="cell" data-execution_count="5">
<div class="cell-output cell-output-stdout">
<pre><code>y = 19 (cattle)</code></pre>
</div>

</div>
<img data-src="DL_image_analysis_1_1_files/figure-revealjs/cell-5-output-2.png" class="r-stretch"></section></section>
<section>
<section id="introduction-to-pytorch" class="title-slide slide level1 center">
<h1>Introduction to PyTorch</h1>

</section>
<section id="what-is-a-tensor-pytorch" class="slide level2">
<h2>What is a tensor (PyTorch)?</h2>
<p>A tensor is a multi-dimensional array. In PyTorch, this comes from a generalization of the notation of variables that exists on more than two dimensions.</p>
<ul>
<li>zero-dimensional variables are points,</li>
<li>one-dimensional variables are vectors,</li>
<li>two-dimensional variables are matrices,</li>
<li>and three or more dimensional variables, are tensors.</li>
</ul>
<div id="9cfb3025" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a><span class="im">import</span> torch</span>
<span id="cb6-2"><a></a></span>
<span id="cb6-3"><a></a>x0 <span class="op">=</span> torch.Tensor([<span class="dv">7</span>]) <span class="co"># This is a point</span></span>
<span id="cb6-4"><a></a></span>
<span id="cb6-5"><a></a>x1 <span class="op">=</span> torch.Tensor([<span class="dv">15</span>, <span class="dv">64</span>, <span class="dv">123</span>]) <span class="co"># This is a vector</span></span>
<span id="cb6-6"><a></a></span>
<span id="cb6-7"><a></a>x2 <span class="op">=</span> torch.Tensor([[<span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">5</span>],</span>
<span id="cb6-8"><a></a>                   [<span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">12</span>],</span>
<span id="cb6-9"><a></a>                   [<span class="dv">10</span>, <span class="dv">33</span>, <span class="dv">1</span>]]) <span class="co"># This is a matrix</span></span>
<span id="cb6-10"><a></a></span>
<span id="cb6-11"><a></a>x3 <span class="op">=</span> torch.Tensor([[[[<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb6-12"><a></a>                     [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb6-13"><a></a>                     [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]],</span>
<span id="cb6-14"><a></a>                    [[<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb6-15"><a></a>                     [<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>],</span>
<span id="cb6-16"><a></a>                     [<span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">5</span>]]]]) <span class="co"># This is a tensor</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section class="slide level2">

<ul class="task-list">
<li><label><input type="checkbox">Convert the example image <code>x_im</code> to a PyTorch tensor, and cast it to floating point data type</label></li>
</ul>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>We can use the utilities in <code>torchvision</code> to convert an image from PIL to tensor</p>
</div>
</div>
</div>
<div id="2e6a9878" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a></a><span class="im">from</span> torchvision.transforms.v2 <span class="im">import</span> PILToTensor</span>
<span id="cb7-2"><a></a></span>
<span id="cb7-3"><a></a>pre_process <span class="op">=</span> PILToTensor()</span>
<span id="cb7-4"><a></a></span>
<span id="cb7-5"><a></a>x <span class="op">=</span> pre_process(x_im)</span>
<span id="cb7-6"><a></a></span>
<span id="cb7-7"><a></a>x <span class="op">=</span> x.<span class="bu">float</span>()</span>
<span id="cb7-8"><a></a></span>
<span id="cb7-9"><a></a><span class="bu">type</span>(x), x.shape, x.dtype, x.<span class="bu">min</span>(), x.<span class="bu">max</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>(torch.Tensor,
 torch.Size([3, 32, 32]),
 torch.float32,
 tensor(1.),
 tensor(255.))</code></pre>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>For convenience, PyTorch’s tensors have their channels axis before the spatial axes.</p>
</div>
</div>
</div>
</section>
<section class="slide level2">

<ul class="task-list">
<li><label><input type="checkbox">Create a composed transformation to carry out the conversion, casting to float, and rescaling to <span class="math inline">\([0, 1]\)</span> range in the same function.</label></li>
</ul>
<div id="46a616d6" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a></a><span class="im">from</span> torchvision.transforms.v2 <span class="im">import</span> Compose, PILToTensor, ToDtype</span>
<span id="cb9-2"><a></a></span>
<span id="cb9-3"><a></a>pre_process <span class="op">=</span> Compose([</span>
<span id="cb9-4"><a></a>  PILToTensor(),</span>
<span id="cb9-5"><a></a>  ToDtype(torch.float32, scale<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-6"><a></a>])</span>
<span id="cb9-7"><a></a></span>
<span id="cb9-8"><a></a>x <span class="op">=</span> pre_process(x_im)</span>
<span id="cb9-9"><a></a></span>
<span id="cb9-10"><a></a><span class="bu">type</span>(x), x.shape, x.dtype, x.<span class="bu">min</span>(), x.<span class="bu">max</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>(torch.Tensor,
 torch.Size([3, 32, 32]),
 torch.float32,
 tensor(0.0039),
 tensor(1.))</code></pre>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>For convenience, PyTorch’s tensors have their channels axis before the spatial axes.</p>
</div>
</div>
</div>
</section>
<section id="exercise-add-the-preprocessing-pipeline-to-the-cifar-100-dataset" class="slide level2">
<h2>Exercise: Add the preprocessing pipeline to the CIFAR-100 dataset</h2>
<ul class="task-list">
<li><label><input type="checkbox">Re-load the CIFAR-100 dataset, this time passing the <code>pre_process</code> function as argument.</label></li>
</ul>
<div id="5d3ce07c" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a></a>cifar_ds <span class="op">=</span> torchvision.datasets.CIFAR100(root<span class="op">=</span><span class="st">"/tmp"</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>pre_process)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Files already downloaded and verified</code></pre>
</div>
</div>
</section></section>
<section>
<section id="training-validation-and-test-data" class="title-slide slide level1 center">
<h1>Training, Validation, and Test data</h1>

</section>
<section id="training-set" class="slide level2">
<h2>Training set</h2>
<p>The examples (<span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>) used to teach a machine/model to perform a task</p>
</section>
<section id="validation-set" class="slide level2">
<h2>Validation set</h2>
<p>Used to measure the performance of a model during training</p>
<p>This subset is not used for training the model, so it is <em>unseen</em> data.</p>
<aside class="notes">
<p>This is a subset from the <em>training set</em> and can be used to test the generalization capacity of the model or to select the best configuration of a model.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="test-set" class="slide level2">
<h2>Test set</h2>
<p>This set of samples is <strong>not</strong> used when training</p>
<p>Its purpose is to measure the <em>generalization</em> capacity of the model</p>
</section>
<section id="exercise-load-the-test-set-and-split-the-train-set-into-train-and-validation-subsets" class="slide level2">
<h2>Exercise: Load the test set and split the train set into train and validation subsets</h2>
<ul class="task-list">
<li><label><input type="checkbox">Load the CIFAR-100 test set</label></li>
</ul>
<div id="cf05ddb2" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a></a>cifar_test_ds <span class="op">=</span> torchvision.datasets.CIFAR100(root<span class="op">=</span><span class="st">"/tmp"</span>, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>pre_process)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Files already downloaded and verified</code></pre>
</div>
</div>
<ul class="task-list">
<li><label><input type="checkbox">Split the training set into train and validation subsets</label></li>
</ul>
<div id="50bf1c4a" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a></a><span class="im">from</span> torch.utils.data <span class="im">import</span> random_split</span>
<span id="cb15-2"><a></a></span>
<span id="cb15-3"><a></a>cifar_train_ds, cifar_val_ds <span class="op">=</span> random_split(cifar_ds, (<span class="dv">40_000</span>, <span class="dv">10_000</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section></section>
<section>
<section id="deep-learning-dl-models" class="title-slide slide level1 center">
<h1><strong>D</strong>eep <strong>L</strong>earning (<strong>DL</strong>) models</h1>

</section>
<section id="deep-learning-dl-models-1" class="slide level2">
<h2>Deep Learning (DL) models</h2>
<p>Models that construct knowledge in a hierarchical manner are considered <strong>deep models</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://www.mdpi.com/applsci/applsci-09-05507/article_deploy/html/images/applsci-09-05507-g003-550.jpg" style="height:50.0%"></p>
<figcaption><a href="https://www.mdpi.com/595982">From Cervantes-Sanchez et al.</a></figcaption>
</figure>
</div>
</section>
<section id="exercise-create-a-logisic-regression-model-with-pytorch" class="slide level2 scrollable">
<h2>Exercise: Create a Logisic Regression model with PyTorch</h2>
<ul class="task-list">
<li><label><input type="checkbox">Use the <code>nn</code> (Neural Networks) module from <code>pytorch</code> to create a Logistic Regression model</label></li>
</ul>
<div id="33f1c1b1" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb16-2"><a></a></span>
<span id="cb16-3"><a></a>lr_clf_1 <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">3</span> <span class="op">*</span> <span class="dv">32</span> <span class="op">*</span> <span class="dv">32</span>, out_features<span class="op">=</span><span class="dv">100</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-4"><a></a>lr_clf_2 <span class="op">=</span> nn.Softmax()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul class="task-list">
<li><label><input type="checkbox"><em>Feed</em> the model with a sample <code>x</code></label></li>
</ul>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>We have to <em>reshape</em> <code>x</code> before feeding it to the model because <code>x</code> is an image with axes: Channels, Height, Width (CHW), but the Logistic Regression input should be a vector.</p>
</div>
</div>
</div>
<div id="a439b930" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a></a>y_hat <span class="op">=</span> lr_clf_2( lr_clf_1( x.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>) ))</span>
<span id="cb17-2"><a></a></span>
<span id="cb17-3"><a></a><span class="bu">type</span>(y_hat), y_hat.shape, y_hat.dtype</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>(torch.Tensor, torch.Size([1, 100]), torch.float32)</code></pre>
</div>
</div>
</section>
<section id="exercise-create-a-multilayer-perceptron-mlp-model-with-pytorch" class="slide level2 scrollable">
<h2>Exercise: Create a MultiLayer Perceptron (MLP) model with PyTorch</h2>
<ul class="task-list">
<li><label><input type="checkbox">Use the <code>nn.Sequential</code> module to build sequential models</label></li>
</ul>
<div id="aa4e0300" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a></a>mlp_clf <span class="op">=</span> nn.Sequential(</span>
<span id="cb19-2"><a></a>  nn.Linear(in_features<span class="op">=</span><span class="dv">3</span> <span class="op">*</span> <span class="dv">32</span> <span class="op">*</span> <span class="dv">32</span>, out_features<span class="op">=</span><span class="dv">1024</span>, bias<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb19-3"><a></a>  nn.Tanh(),</span>
<span id="cb19-4"><a></a>  nn.Linear(in_features<span class="op">=</span><span class="dv">1024</span>, out_features<span class="op">=</span><span class="dv">100</span>, bias<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb19-5"><a></a>  nn.Softmax()</span>
<span id="cb19-6"><a></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul class="task-list">
<li><label><input type="checkbox"><em>Feed</em> the model with a sample <code>x</code></label></li>
</ul>
<div id="9da7f2bd" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a></a>y_hat <span class="op">=</span> mlp_clf(x.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb20-2"><a></a></span>
<span id="cb20-3"><a></a><span class="bu">type</span>(y_hat), y_hat.shape, y_hat.dtype</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>(torch.Tensor, torch.Size([1, 100]), torch.float32)</code></pre>
</div>
</div>
</section></section>
<section>
<section id="model-optimization" class="title-slide slide level1 center">
<h1>Model optimization</h1>

</section>
<section id="model-fittingtraining" class="slide level2">
<h2>Model fitting/training</h2>
<p>Models behavior depends directly on the value of their set of parameters <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li><span class="math inline">\(f(x) \approx y\)</span></li>
<li><span class="math inline">\(f_\theta(x) = y + \epsilon = \hat{y}\)</span></li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>As models increase their number of parameters, they become more <em>complex</em></p>
</div>
</div>
</div>
<p><strong>Training</strong> is the process of optimizing the values of <span class="math inline">\(\theta\)</span></p>
<aside class="notes">
<p>Training is often an expensive process in terms of computational resources and time.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="loss-function" class="title-slide slide level1 center">
<h1>Loss function</h1>

</section>
<section id="loss-function-1" class="slide level2">
<h2>Loss function</h2>
<p>This is measure of the difference between the expected outputs and the predictions made by a model <span class="math inline">\(L(Y, \hat{Y})\)</span>.</p>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>We look for <em>smooth</em> loss functions for which we can compute their gradient</p>
</div>
</div>
</div>
</section>
<section id="loss-function-for-regression" class="slide level2">
<h2>11.1 Loss function for regression</h2>
<p>In the case of regression tasks we generally use the Mean Squared Error (MSE).</p>
<p><span class="math inline">\(MSE=\frac{1}{N}\sum \left(Y - \hat{Y}\right)^2\)</span></p>
</section>
<section id="loss-function-for-classification" class="slide level2">
<h2>Loss function for classification</h2>
<p>And for classification tasks we use the <strong>C</strong>ross <strong>E</strong>ntropy (<strong>CE</strong>) function.</p>
<p><span class="math inline">\(CE = -\frac{1}{N}\sum\limits_i^N\sum\limits_k^C y_{i,k} log(\hat{y_{i,k}})\)</span></p>
<p>where <span class="math inline">\(C\)</span> is the number of classes.</p>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>For the binary classification case:</p>
<p><span class="math inline">\(BCE = -\frac{1}{N}\sum\limits_i^N \left(y_i log(\hat{y_i}) + (1 - y_i) log(1 - \hat{y_i})\right)\)</span></p>
</div>
</div>
</div>
</section>
<section id="exercise-define-the-loss-function-for-the-cifar-100-classification-problem" class="slide level2">
<h2>Exercise: Define the loss function for the CIFAR-100 classification problem</h2>
<ul class="task-list">
<li><label><input type="checkbox">Define a Cross Entropy loss function with <code>nn.CrossEntropyLoss</code></label></li>
</ul>
<div id="84c34c8c" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a></a>loss_fun <span class="op">=</span> nn.CrossEntropyLoss()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul class="task-list">
<li><label><input type="checkbox">Remove the <code>nn.Softmax</code> layer from the MLP model.</label></li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>According to the PyTorch documentation, the CrossEntropyLoss function takes as inputs the <em>logits</em> of the probabilities and not the probabilities themselves. So, we don’t need to <em>squash</em> the output of the MLP model.</p>
</div>
</div>
</div>
<div id="964186ad" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a></a>mlp_clf <span class="op">=</span> nn.Sequential(</span>
<span id="cb23-2"><a></a>  nn.Linear(in_features<span class="op">=</span><span class="dv">3</span> <span class="op">*</span> <span class="dv">32</span> <span class="op">*</span> <span class="dv">32</span>, out_features<span class="op">=</span><span class="dv">1024</span>, bias<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb23-3"><a></a>  nn.Tanh(),</span>
<span id="cb23-4"><a></a>  nn.Linear(in_features<span class="op">=</span><span class="dv">1024</span>, out_features<span class="op">=</span><span class="dv">100</span>, bias<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb23-5"><a></a>  <span class="co"># nn.Softmax() # &lt;- remove this line</span></span>
<span id="cb23-6"><a></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="exercise-define-the-loss-function-for-the-cifar-100-classification-problem-1" class="slide level2">
<h2>Exercise: Define the loss function for the CIFAR-100 classification problem</h2>
<ul class="task-list">
<li><label><input type="checkbox">Measure the prediction loss (error) of our MLP with respect to the grund-truth</label></li>
</ul>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>We are using a PyTorch loss function, and it expects PyTorch’s tensors as arguments, so we have to convert <code>y</code> to tensor before computing the loss function.</p>
</div>
</div>
</div>
<div id="4f884333" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a></a>loss <span class="op">=</span> loss_fun(y_hat, torch.LongTensor([y]))</span>
<span id="cb24-2"><a></a></span>
<span id="cb24-3"><a></a>loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>tensor(4.6085, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
</section></section>
<section>
<section id="gradient-based-optimization" class="title-slide slide level1 center">
<h1>Gradient based optimization</h1>

</section>
<section id="gradient-based-optimization-1" class="slide level2">
<h2>Gradient based optimization</h2>
<p><em>Gradient</em>-based methods are able to fit large numbers of parameters when using a <em>smooth</em> Loss function as target.</p>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>We compute the gradient of the loss function with respect to the model parameters using the chain rule from calculous. Generally, this is managed by the machine learning packages such as PyTorch and Tensorflow with a method called <em>back propagation</em>.</p>
</div>
</div>
</div>
<h3 id="gradient-descent"><strong>Gradient Descent</strong></h3>
<ul>
<li><span class="math inline">\(\theta^{t+1} = \theta^t - \eta \nabla_\theta L(Y, \hat{Y})\)</span></li>
</ul>
</section>
<section id="exercise-compute-the-gradient-of-the-loss-function-with-respect-to-the-parameters-of-the-mlp." class="slide level2">
<h2>Exercise: Compute the gradient of the loss function with respect to the parameters of the MLP.</h2>
<ul class="task-list">
<li><label><input type="checkbox">Check what are the gradients of the MLP parameters before back propagating the gradient.</label></li>
</ul>
<div id="fb162ff6" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a></a>mlp_clf[<span class="dv">0</span>].bias.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul class="task-list">
<li><label><input type="checkbox">Compute the gradient of the loss function with respect to the MLP parameters.</label></li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>To <em>back propagate</em> the gradients we use the <code>loss.backward()</code> method of the loss function.</p>
</div>
</div>
</div>
<div id="efc79a21" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a></a>loss <span class="op">=</span> loss_fun(y_hat, torch.LongTensor([y]))</span>
<span id="cb27-2"><a></a></span>
<span id="cb27-3"><a></a>loss.backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul class="task-list">
<li><label><input type="checkbox">Verify that the gradients have been propagated to the model parameters.</label></li>
</ul>
<div id="738c6c0b" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a></a>mlp_clf[<span class="dv">0</span>].bias.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="stochastic-methods" class="slide level2">
<h2>Stochastic methods</h2>
<div class="callout callout-caution callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Caution</strong></p>
</div>
<div class="callout-content">
<p>The Gradient descent method require to obtain the Loss function for the whole training set before doing a single update.</p>
<p>This can be inefficient when large volumes of data are used for training the model.</p>
</div>
</div>
</div>
<ul>
<li><p>These methods use a relative small sample from the training data called <em>mini-batch</em> at a time.</p></li>
<li><p>This reduces the amount of memory used for computing intermediate operations carried out during optimization process.</p></li>
</ul>
</section>
<section id="stochastic-gradient-descent-sgd" class="slide level2">
<h2><strong>S</strong>tochastic <strong>G</strong>radient <strong>D</strong>escent (<strong>SGD</strong>)</h2>
<aside class="notes">
<p>This strategy defines <span class="math inline">\(\theta\)</span>’s’ update rule for iteration <span class="math inline">\(t+1\)</span> using a <em>mini-batch</em> sampled at random from the training set as follows.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<ul>
<li><p><span class="math inline">\(\theta^{t+1} = \theta^t - \eta \nabla_\theta L(Y_{b}, \hat{Y_{b}})\)</span></p></li>
<li><p><span class="math inline">\(\eta\)</span> controls the update we perform on the current parameter’s values</p></li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>This parameter in Deep Learning is known as the <strong>learning rate</strong></p>
</div>
</div>
</div>
</section>
<section id="training-with-mini-batches" class="slide level2">
<h2>Training with mini-batches</h2>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>PyTorch can operate efficiently on multiple inputs at the same time. To do that, we can use a <code>DataLoader</code> to serve mini-batches of inputs.</p>
</div>
</div>
</div>
</section>
<section id="exercise-train-the-mlp-classifier" class="slide level2">
<h2>Exercise: Train the MLP classifier</h2>
<ul class="task-list">
<li><label><input type="checkbox">Use a <code>DataLoader</code> to serve mini-batches of images to train our MLP.</label></li>
</ul>
<div id="7ac55684" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb29-2"><a></a></span>
<span id="cb29-3"><a></a>cifar_train_dl <span class="op">=</span> DataLoader(cifar_train_ds, batch_size<span class="op">=</span><span class="dv">128</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-4"><a></a>cifar_val_dl <span class="op">=</span> DataLoader(cifar_val_ds, batch_size<span class="op">=</span><span class="dv">256</span>)</span>
<span id="cb29-5"><a></a>cifar_test_dl <span class="op">=</span> DataLoader(cifar_test_ds, batch_size<span class="op">=</span><span class="dv">256</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul class="task-list">
<li><label><input type="checkbox">Create a Stochastic Gradient Descent optimizer for our MLP classifier.</label></li>
</ul>
<div id="af972269" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb30-2"><a></a></span>
<span id="cb30-3"><a></a>optimizer <span class="op">=</span> optim.SGD(mlp_clf.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>, )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="exercise-train-the-mlp-classifier-1" class="slide level2 scrollable">
<h2>Exercise: Train the MLP classifier</h2>
<ul class="task-list">
<li><label><input type="checkbox">Implement the <em>training-loop</em> to fit the parameters of our MLP classifier.</label></li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Gradients are accumulated on every iteration, so we need to reset the accumulator with <code>optimizer.zero_grad()</code> for every new batch.</p>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>To perform get the new iteration’s parameter values <span class="math inline">\(\theta^{t+1}\)</span> we use <code>optimizer.step()</code> to compute the update step.</p>
</div>
</div>
</div>
<div class="sourceCode" id="cb31" data-code-line-numbers="|3|9|11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a></a>mlp_clf.train()</span>
<span id="cb31-2"><a></a><span class="cf">for</span> x, y <span class="kw">in</span> cifar_train_dl:</span>
<span id="cb31-3"><a></a>  optimizer.zero_grad()</span>
<span id="cb31-4"><a></a></span>
<span id="cb31-5"><a></a>  y_hat <span class="op">=</span> mlp_clf( x.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span> <span class="op">*</span> <span class="dv">32</span> <span class="op">*</span> <span class="dv">32</span>) ) <span class="co"># Reshape it into a batch of vectors</span></span>
<span id="cb31-6"><a></a></span>
<span id="cb31-7"><a></a>  loss <span class="op">=</span> loss_fun(y_hat, y)</span>
<span id="cb31-8"><a></a></span>
<span id="cb31-9"><a></a>  loss.backward()</span>
<span id="cb31-10"><a></a></span>
<span id="cb31-11"><a></a>  optimizer.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="exercise-train-the-mlp-classifier-and-track-the-training-and-validation-loss" class="slide level2">
<h2>Exercise: Train the MLP classifier and track the training and validation loss</h2>
<ul class="task-list">
<li><label><input type="checkbox">Save the loss function of each batch and the overall average loss during training.</label></li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>To extract the loss function’s value without anything else attached use <code>loss.item()</code>.</p>
</div>
</div>
</div>
<div class="sourceCode" id="cb32" data-code-line-numbers="1-3,13-15,21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a></a>train_loss <span class="op">=</span> []</span>
<span id="cb32-2"><a></a>train_loss_avg <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-3"><a></a>total_train_samples <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-4"><a></a></span>
<span id="cb32-5"><a></a>mlp_clf.train()</span>
<span id="cb32-6"><a></a><span class="cf">for</span> x, y <span class="kw">in</span> cifar_train_dl:</span>
<span id="cb32-7"><a></a>  optimizer.zero_grad()</span>
<span id="cb32-8"><a></a></span>
<span id="cb32-9"><a></a>  y_hat <span class="op">=</span> mlp_clf( x.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span> <span class="op">*</span> <span class="dv">32</span> <span class="op">*</span> <span class="dv">32</span>) ) <span class="co"># Reshape it into a batch of vectors</span></span>
<span id="cb32-10"><a></a></span>
<span id="cb32-11"><a></a>  loss <span class="op">=</span> loss_fun(y_hat, y)</span>
<span id="cb32-12"><a></a></span>
<span id="cb32-13"><a></a>  train_loss.append(loss.item())</span>
<span id="cb32-14"><a></a>  train_loss_avg <span class="op">+=</span> loss.item() <span class="op">*</span> <span class="bu">len</span>(x)</span>
<span id="cb32-15"><a></a>  total_train_samples <span class="op">+=</span> <span class="bu">len</span>(x)</span>
<span id="cb32-16"><a></a></span>
<span id="cb32-17"><a></a>  loss.backward()</span>
<span id="cb32-18"><a></a></span>
<span id="cb32-19"><a></a>  optimizer.step()</span>
<span id="cb32-20"><a></a></span>
<span id="cb32-21"><a></a>train_loss_avg <span class="op">/=</span> total_train_samples</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="exercise-train-the-mlp-classifier-and-track-the-training-and-validation-loss-1" class="slide level2">
<h2>Exercise: Train the MLP classifier and track the training and validation loss</h2>
<ul class="task-list">
<li><label><input type="checkbox">Compute the average loss function for the validation set.</label></li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Because we don’t train the model with the validation set, back-propagation and optimization steps are not needed.</p>
<p>Additionally, we wrap the loop <code>with torch.no_grad()</code> to prevent the generation of gradients that could fill the memory unnecessarily.</p>
</div>
</div>
</div>
<div class="sourceCode" id="cb33" data-code-line-numbers="|5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a></a>val_loss_avg <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb33-2"><a></a>total_val_samples <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb33-3"><a></a></span>
<span id="cb33-4"><a></a>mlp_clf.<span class="bu">eval</span>()</span>
<span id="cb33-5"><a></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb33-6"><a></a>  <span class="cf">for</span> x, y <span class="kw">in</span> cifar_val_dl:</span>
<span id="cb33-7"><a></a>    y_hat <span class="op">=</span> mlp_clf( x.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span> <span class="op">*</span> <span class="dv">32</span> <span class="op">*</span> <span class="dv">32</span>) ) <span class="co"># Reshape it into a batch of vectors</span></span>
<span id="cb33-8"><a></a>    loss <span class="op">=</span> loss_fun(y_hat, y)</span>
<span id="cb33-9"><a></a></span>
<span id="cb33-10"><a></a>    val_loss_avg <span class="op">+=</span> loss.item() <span class="op">*</span> <span class="bu">len</span>(x)</span>
<span id="cb33-11"><a></a>    total_val_samples <span class="op">+=</span> <span class="bu">len</span>(x)</span>
<span id="cb33-12"><a></a></span>
<span id="cb33-13"><a></a>val_loss_avg <span class="op">/=</span> total_val_samples</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="exercise-train-the-mlp-classifier-and-track-the-training-and-validation-loss-2" class="slide level2">
<h2>Exercise: Train the MLP classifier and track the training and validation loss</h2>
<ul class="task-list">
<li><label><input type="checkbox">Plot the training loss for this <em>epoch</em>.</label></li>
</ul>
<div id="fb87e26a" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb34-2"><a></a></span>
<span id="cb34-3"><a></a>plt.plot(train_loss, <span class="st">"b-"</span>, label<span class="op">=</span><span class="st">"Training loss"</span>)</span>
<span id="cb34-4"><a></a>plt.plot([<span class="dv">0</span>, <span class="bu">len</span>(train_loss)], [train_loss_avg, train_loss_avg], <span class="st">"r:"</span>, label<span class="op">=</span><span class="st">"Average training loss"</span>)</span>
<span id="cb34-5"><a></a>plt.plot([<span class="dv">0</span>, <span class="bu">len</span>(train_loss)], [val_loss_avg, val_loss_avg], <span class="st">"b:"</span>, label<span class="op">=</span><span class="st">"Average validation loss"</span>)</span>
<span id="cb34-6"><a></a>plt.legend()</span>
<span id="cb34-7"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="DL_image_analysis_1_1_files/figure-revealjs/cell-27-output-1.png" width="802" height="411"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="exercise-train-the-mlp-classifier-and-track-the-training-and-validation-loss-through-several-epochs" class="slide level2 scrollable">
<h2>Exercise: Train the MLP classifier and track the training and validation loss through several <em>epochs</em></h2>
<div class="sourceCode" id="cb35" data-code-line-numbers="1-5|"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a></a>num_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb35-2"><a></a>train_loss <span class="op">=</span> []</span>
<span id="cb35-3"><a></a>val_loss <span class="op">=</span> []</span>
<span id="cb35-4"><a></a></span>
<span id="cb35-5"><a></a><span class="cf">for</span> e <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb35-6"><a></a>  train_loss_avg <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb35-7"><a></a>  total_train_samples <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb35-8"><a></a></span>
<span id="cb35-9"><a></a>  mlp_clf.train()</span>
<span id="cb35-10"><a></a>  <span class="cf">for</span> x, y <span class="kw">in</span> cifar_train_dl:</span>
<span id="cb35-11"><a></a>    optimizer.zero_grad()</span>
<span id="cb35-12"><a></a></span>
<span id="cb35-13"><a></a>    y_hat <span class="op">=</span> mlp_clf( x.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span> <span class="op">*</span> <span class="dv">32</span> <span class="op">*</span> <span class="dv">32</span>) ) <span class="co"># Reshape it into a batch of vectors</span></span>
<span id="cb35-14"><a></a></span>
<span id="cb35-15"><a></a>    loss <span class="op">=</span> loss_fun(y_hat, y)</span>
<span id="cb35-16"><a></a></span>
<span id="cb35-17"><a></a>    train_loss_avg <span class="op">+=</span> loss.item() <span class="op">*</span> <span class="bu">len</span>(x)</span>
<span id="cb35-18"><a></a>    total_train_samples <span class="op">+=</span> <span class="bu">len</span>(x)</span>
<span id="cb35-19"><a></a></span>
<span id="cb35-20"><a></a>    loss.backward()</span>
<span id="cb35-21"><a></a></span>
<span id="cb35-22"><a></a>    optimizer.step()</span>
<span id="cb35-23"><a></a></span>
<span id="cb35-24"><a></a>  train_loss_avg <span class="op">/=</span> total_train_samples</span>
<span id="cb35-25"><a></a>  train_loss.append(train_loss_avg)</span>
<span id="cb35-26"><a></a></span>
<span id="cb35-27"><a></a>  val_loss_avg <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb35-28"><a></a>  total_val_samples <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb35-29"><a></a></span>
<span id="cb35-30"><a></a>  mlp_clf.<span class="bu">eval</span>()</span>
<span id="cb35-31"><a></a>  <span class="cf">with</span> torch.no_grad():</span>
<span id="cb35-32"><a></a>    <span class="cf">for</span> x, y <span class="kw">in</span> cifar_val_dl:</span>
<span id="cb35-33"><a></a>      y_hat <span class="op">=</span> mlp_clf( x.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span> <span class="op">*</span> <span class="dv">32</span> <span class="op">*</span> <span class="dv">32</span>) ) <span class="co"># Reshape it into a batch of vectors</span></span>
<span id="cb35-34"><a></a>      loss <span class="op">=</span> loss_fun(y_hat, y)</span>
<span id="cb35-35"><a></a></span>
<span id="cb35-36"><a></a>      val_loss_avg <span class="op">+=</span> loss.item() <span class="op">*</span> <span class="bu">len</span>(x)</span>
<span id="cb35-37"><a></a>      total_val_samples <span class="op">+=</span> <span class="bu">len</span>(x)</span>
<span id="cb35-38"><a></a></span>
<span id="cb35-39"><a></a>  val_loss_avg <span class="op">/=</span> total_val_samples</span>
<span id="cb35-40"><a></a>  val_loss.append(val_loss_avg)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="exercise-show-the-progress-of-the-training-throughout-the-epochs" class="slide level2">
<h2>Exercise: Show the progress of the training throughout the epochs</h2>
<ul class="task-list">
<li><label><input type="checkbox">Plot the average train and validation losses</label></li>
</ul>
<div id="08f15458" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb36-2"><a></a></span>
<span id="cb36-3"><a></a>plt.plot(train_loss, <span class="st">"b-"</span>, label<span class="op">=</span><span class="st">"Average training loss"</span>)</span>
<span id="cb36-4"><a></a>plt.plot(val_loss, <span class="st">"r-"</span>, label<span class="op">=</span><span class="st">"Average validation loss"</span>)</span>
<span id="cb36-5"><a></a>plt.legend()</span>
<span id="cb36-6"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="DL_image_analysis_1_1_files/figure-revealjs/cell-29-output-1.png" width="802" height="411"></p>
</figure>
</div>
</div>
</div>
</section></section>
<section>
<section id="performance-metrics" class="title-slide slide level1 center">
<h1>Performance metrics</h1>

</section>
<section id="performance-metrics-1" class="slide level2">
<h2>Performance metrics</h2>
<p>Used to measure how good or bad a model carries out a task</p>
<ul>
<li><p><span class="math inline">\(f(x) \approx y\)</span></p></li>
<li><p><span class="math inline">\(f(x) = y + \epsilon = \hat{y}\)</span></p></li>
</ul>
<aside class="notes">
<p>Given the set of parameters, as well as other factors, the output of a model can deviate from the expected outcome. So, the actual output of a model is <span class="math inline">\(f(x) = \hat{y}\)</span>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>The output <span class="math inline">\(\hat{y}\)</span> is called <strong>prediction </strong> given the context taken from statistical regression analysis.</p>
</div>
</div>
</div>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>Selecting the correct performance metrics depends on the training type, task, and even the distribution of the data.</p>
</div>
</div>
</div>
</section>
<section id="exercise-measure-the-accuracy-of-the-mlp-trained-to-classify-images-from-cifar-100" class="slide level2 scrollable">
<h2>Exercise: Measure the accuracy of the MLP trained to classify images from CIFAR-100</h2>
<ul class="task-list">
<li><label><input type="checkbox">Install the <code>torchmetrics</code> package.</label></li>
</ul>
<div class="sourceCode" id="cb37"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a></a><span class="op">!</span>pip install torchmetrics</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul class="task-list">
<li><label><input type="checkbox">Compute the average accuracy for the Train set.</label></li>
</ul>
<div id="97a5daf4" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a></a><span class="im">from</span> torchmetrics.classification <span class="im">import</span> Accuracy</span>
<span id="cb38-2"><a></a></span>
<span id="cb38-3"><a></a>mlp_clf.<span class="bu">eval</span>()</span>
<span id="cb38-4"><a></a></span>
<span id="cb38-5"><a></a>train_acc_metric <span class="op">=</span> Accuracy(task<span class="op">=</span><span class="st">"multiclass"</span>, num_classes<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb38-6"><a></a></span>
<span id="cb38-7"><a></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb38-8"><a></a>  <span class="cf">for</span> x, y <span class="kw">in</span> cifar_train_dl:</span>
<span id="cb38-9"><a></a>    y_hat <span class="op">=</span> mlp_clf( x.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span> <span class="op">*</span> <span class="dv">32</span> <span class="op">*</span> <span class="dv">32</span>) )</span>
<span id="cb38-10"><a></a>    train_acc_metric(y_hat.softmax(dim<span class="op">=</span><span class="dv">1</span>), y)</span>
<span id="cb38-11"><a></a></span>
<span id="cb38-12"><a></a>  train_acc <span class="op">=</span> train_acc_metric.compute()</span>
<span id="cb38-13"><a></a></span>
<span id="cb38-14"><a></a><span class="bu">print</span>(<span class="ss">f"Training acc=</span><span class="sc">{</span>train_acc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-15"><a></a>train_acc_metric.reset()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training acc=0.12927499413490295</code></pre>
</div>
</div>
</section>
<section id="exercise-measure-the-accuracy-of-the-mlp-trained-to-classify-images-from-cifar-100-1" class="slide level2 scrollable">
<h2>Exercise: Measure the accuracy of the MLP trained to classify images from CIFAR-100</h2>
<ul class="task-list">
<li><label><input type="checkbox">Compute the average accuracy for the Validation and Test sets.</label></li>
</ul>
<div id="d03fff04" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a></a>val_acc_metric <span class="op">=</span> Accuracy(task<span class="op">=</span><span class="st">"multiclass"</span>, num_classes<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb40-2"><a></a>test_acc_metric <span class="op">=</span> Accuracy(task<span class="op">=</span><span class="st">"multiclass"</span>, num_classes<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb40-3"><a></a></span>
<span id="cb40-4"><a></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb40-5"><a></a>  <span class="cf">for</span> x, y <span class="kw">in</span> cifar_val_dl:</span>
<span id="cb40-6"><a></a>    y_hat <span class="op">=</span> mlp_clf( x.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span> <span class="op">*</span> <span class="dv">32</span> <span class="op">*</span> <span class="dv">32</span>) )</span>
<span id="cb40-7"><a></a>    val_acc_metric(y_hat.softmax(dim<span class="op">=</span><span class="dv">1</span>), y)</span>
<span id="cb40-8"><a></a></span>
<span id="cb40-9"><a></a>  val_acc <span class="op">=</span> val_acc_metric.compute()</span>
<span id="cb40-10"><a></a></span>
<span id="cb40-11"><a></a>  <span class="cf">for</span> x, y <span class="kw">in</span> cifar_test_dl:</span>
<span id="cb40-12"><a></a>    y_hat <span class="op">=</span> mlp_clf( x.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span> <span class="op">*</span> <span class="dv">32</span> <span class="op">*</span> <span class="dv">32</span>) )</span>
<span id="cb40-13"><a></a>    test_acc_metric(y_hat.softmax(dim<span class="op">=</span><span class="dv">1</span>), y)</span>
<span id="cb40-14"><a></a></span>
<span id="cb40-15"><a></a>  test_acc <span class="op">=</span> test_acc_metric.compute()</span>
<span id="cb40-16"><a></a></span>
<span id="cb40-17"><a></a><span class="bu">print</span>(<span class="ss">f"Validation acc=</span><span class="sc">{</span>val_acc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb40-18"><a></a><span class="bu">print</span>(<span class="ss">f"Test acc=</span><span class="sc">{</span>test_acc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb40-19"><a></a></span>
<span id="cb40-20"><a></a>val_acc_metric.reset()</span>
<span id="cb40-21"><a></a>test_acc_metric.reset()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Validation acc=0.125
Test acc=0.12290000170469284</code></pre>
</div>
</div>
</section></section>
<section>
<section id="convolutional-neural-network-cnn-or-convnet" class="title-slide slide level1 center">
<h1><strong>C</strong>onvolutional <strong>N</strong>eural <strong>N</strong>etwork (<strong>CNN</strong> or <strong>ConvNet</strong>)</h1>

</section>
<section id="convolution-layers" class="slide level2">
<h2>Convolution layers</h2>
<p>The most common operation in DL models for image processing are Convolution operations.</p>

<img data-src="https://upload.wikimedia.org/wikipedia/commons/8/85/Convolution_arithmetic_-_Full_padding_no_strides_transposed.gif" class="r-stretch quarto-figure-center"><p class="caption">2D Convolution</p><p>The animation shows the convolution of a 7x7 pixels input image (bottom) with a 3x3 pixels kernel (moving window), that results in a 5x5 pixels output (top).</p>
</section>
<section id="exercise-visualize-the-effect-of-the-convolution-operation" class="slide level2">
<h2>Exercise: Visualize the effect of the convolution operation</h2>
<ul class="task-list">
<li><label><input type="checkbox">Create a convolution layer with <code>nn.Conv2D</code> using 3 channels as input, and a single one for output.</label></li>
</ul>
<div id="373d671d" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a></a>conv_1 <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span><span class="dv">3</span>, out_channels<span class="op">=</span><span class="dv">1</span>, kernel_size<span class="op">=</span><span class="dv">7</span>, padding<span class="op">=</span><span class="dv">0</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-2"><a></a></span>
<span id="cb42-3"><a></a>x, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(cifar_train_dl))</span>
<span id="cb42-4"><a></a></span>
<span id="cb42-5"><a></a>fx <span class="op">=</span> conv_1(x)</span>
<span id="cb42-6"><a></a></span>
<span id="cb42-7"><a></a><span class="bu">type</span>(fx), fx.dtype, fx.shape, fx.<span class="bu">min</span>(), fx.<span class="bu">max</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>(torch.Tensor,
 torch.float32,
 torch.Size([128, 1, 26, 26]),
 tensor(-0.1479, grad_fn=&lt;MinBackward1&gt;),
 tensor(1.0583, grad_fn=&lt;MaxBackward1&gt;))</code></pre>
</div>
</div>
<div class="callout callout-warning callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Warning</strong></p>
</div>
<div class="callout-content">
<p>The convolution layer is initialized with random values, so the results will vary.</p>
</div>
</div>
</div>
</section>
<section id="exercise-visualize-the-effect-of-the-convolution-operation-1" class="slide level2">
<h2>Exercise: Visualize the effect of the convolution operation</h2>
<ul class="task-list">
<li><label><input type="checkbox">Create a convolution layer with <code>nn.Conv2D</code> using 3 channels as input, and a single one for output.</label></li>
</ul>
<div id="482bec97" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a></a>plt.rcParams[<span class="st">'figure.figsize'</span>] <span class="op">=</span> [<span class="dv">5</span>, <span class="dv">5</span>]</span>
<span id="cb44-2"><a></a></span>
<span id="cb44-3"><a></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb44-4"><a></a>ax[<span class="dv">0</span>].imshow(x[<span class="dv">0</span>].permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="cb44-5"><a></a>ax[<span class="dv">1</span>].imshow(fx.detach()[<span class="dv">0</span>, <span class="dv">0</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb44-6"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="DL_image_analysis_1_1_files/figure-revealjs/cell-33-output-1.png" width="418" height="213"></p>
</figure>
</div>
</div>
</div>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>By default, outputs from PyTorch modules are tracked for back-propagation.</p>
<p>To visualize it with <code>matplotlib</code> we have to <code>.detach()</code> the tensor first.</p>
</div>
</div>
</div>
</section>
<section id="exercise-visualize-the-effect-of-the-convolution-operation-2" class="slide level2">
<h2>Exercise: Visualize the effect of the convolution operation</h2>
<ul class="task-list">
<li><label><input type="checkbox">Visualize the weights of the convolution layer.</label></li>
</ul>
<div id="6f50a2cf" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a></a>conv_1.weight.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>torch.Size([1, 3, 7, 7])</code></pre>
</div>
</div>
<div id="5b85c327" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb47-2"><a></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].imshow(conv_1.weight.detach()[<span class="dv">0</span>, <span class="dv">0</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb47-3"><a></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].imshow(conv_1.weight.detach()[<span class="dv">0</span>, <span class="dv">1</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb47-4"><a></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].imshow(conv_1.weight.detach()[<span class="dv">0</span>, <span class="dv">2</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb47-5"><a></a>ax[<span class="dv">1</span>, <span class="dv">1</span>].set_axis_off()</span>
<span id="cb47-6"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="DL_image_analysis_1_1_files/figure-revealjs/cell-35-output-1.png" width="408" height="411"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="exercise-visualize-the-effect-of-the-convolution-operation-3" class="slide level2">
<h2>Exercise: Visualize the effect of the convolution operation</h2>
<ul class="task-list">
<li><label><input type="checkbox">Modify the weights of the convolution layer.</label></li>
</ul>
<div id="846889e5" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a></a>conv_1 <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span><span class="dv">3</span>, out_channels<span class="op">=</span><span class="dv">1</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">0</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb48-2"><a></a></span>
<span id="cb48-3"><a></a>conv_1.weight.data[:] <span class="op">=</span> torch.FloatTensor([</span>
<span id="cb48-4"><a></a>  [</span>
<span id="cb48-5"><a></a>    [</span>
<span id="cb48-6"><a></a>      [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb48-7"><a></a>      [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb48-8"><a></a>      [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb48-9"><a></a>    ],</span>
<span id="cb48-10"><a></a>    [</span>
<span id="cb48-11"><a></a>      [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb48-12"><a></a>      [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb48-13"><a></a>      [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb48-14"><a></a>    ],</span>
<span id="cb48-15"><a></a>    [</span>
<span id="cb48-16"><a></a>      [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb48-17"><a></a>      [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb48-18"><a></a>      [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb48-19"><a></a>    ],</span>
<span id="cb48-20"><a></a>  ]</span>
<span id="cb48-21"><a></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="exercise-visualize-the-effect-of-the-convolution-operation-4" class="slide level2">
<h2>Exercise: Visualize the effect of the convolution operation</h2>
<ul class="task-list">
<li><label><input type="checkbox">Visualize the effects after changing the values.</label></li>
</ul>
<div id="7cc57205" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a></a>fx <span class="op">=</span> conv_1(x)</span>
<span id="cb49-2"><a></a></span>
<span id="cb49-3"><a></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb49-4"><a></a>ax[<span class="dv">0</span>].imshow(x[<span class="dv">0</span>].permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="cb49-5"><a></a>ax[<span class="dv">1</span>].imshow(fx.detach()[<span class="dv">0</span>].permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="cb49-6"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="DL_image_analysis_1_1_files/figure-revealjs/cell-37-output-1.png" width="418" height="213"></p>
</figure>
</div>
</div>
</div>
<p>Experiment with different values and shapes of the kernel https://en.wikipedia.org/wiki/Kernel_(image_processing)</p>
</section>
<section id="exercise-visualize-the-effect-of-the-convolution-operation-5" class="slide level2">
<h2>Exercise: Visualize the effect of the convolution operation</h2>
<ul class="task-list">
<li><label><input type="checkbox">Modify the weights of the convolution layer.</label></li>
</ul>
<div id="f1fe3b71" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a></a>conv_1 <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span><span class="dv">3</span>, out_channels<span class="op">=</span><span class="dv">1</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">0</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb50-2"><a></a></span>
<span id="cb50-3"><a></a>conv_1.weight.data[:] <span class="op">=</span> torch.FloatTensor([</span>
<span id="cb50-4"><a></a>  [[[<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>], [<span class="op">-</span><span class="dv">1</span>, <span class="dv">5</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>]],</span>
<span id="cb50-5"><a></a>   [[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]],</span>
<span id="cb50-6"><a></a>   [[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]]]</span>
<span id="cb50-7"><a></a>])</span>
<span id="cb50-8"><a></a></span>
<span id="cb50-9"><a></a>fx <span class="op">=</span> conv_1(x)</span>
<span id="cb50-10"><a></a></span>
<span id="cb50-11"><a></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb50-12"><a></a>ax[<span class="dv">0</span>].imshow(x[<span class="dv">0</span>].permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="cb50-13"><a></a>ax[<span class="dv">1</span>].imshow(fx.detach()[<span class="dv">0</span>, <span class="dv">0</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb50-14"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="DL_image_analysis_1_1_files/figure-revealjs/cell-38-output-1.png" width="418" height="213"></p>
</figure>
</div>
</div>
</div>
<p>Experiment with different values and shapes of the kernel https://en.wikipedia.org/wiki/Kernel_(image_processing)</p>
</section>
<section id="exercise-visualize-the-effect-of-the-convolution-operation-6" class="slide level2">
<h2>Exercise: Visualize the effect of the convolution operation</h2>
<ul class="task-list">
<li><label><input type="checkbox">Modify the weights of the convolution layer.</label></li>
</ul>
<div id="717b878c" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a></a>conv_1 <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span><span class="dv">3</span>, out_channels<span class="op">=</span><span class="dv">1</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">0</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb51-2"><a></a></span>
<span id="cb51-3"><a></a>conv_1.weight.data[:] <span class="op">=</span> torch.FloatTensor([</span>
<span id="cb51-4"><a></a>  [[[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]],</span>
<span id="cb51-5"><a></a>   [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]],</span>
<span id="cb51-6"><a></a>   [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]]]</span>
<span id="cb51-7"><a></a>])</span>
<span id="cb51-8"><a></a></span>
<span id="cb51-9"><a></a>fx <span class="op">=</span> conv_1(x)</span>
<span id="cb51-10"><a></a></span>
<span id="cb51-11"><a></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb51-12"><a></a>ax[<span class="dv">0</span>].imshow(x[<span class="dv">0</span>].permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="cb51-13"><a></a>ax[<span class="dv">1</span>].imshow(fx.detach()[<span class="dv">0</span>, <span class="dv">0</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb51-14"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="DL_image_analysis_1_1_files/figure-revealjs/cell-39-output-1.png" width="418" height="213"></p>
</figure>
</div>
</div>
</div>
<p>Experiment with different values and shapes of the kernel https://en.wikipedia.org/wiki/Kernel_(image_processing)</p>
</section>
<section id="examples-of-popular-deep-learning-models-in-computer-vision" class="slide level2">
<h2>Examples of popular Deep Learning models in computer vision</h2>
<ul>
<li>Inception v3 for image classification</li>
</ul>

<img data-src="https://cloud.google.com/static/tpu/docs/images/inceptionv3onc--oview.png" class="r-stretch quarto-figure-center"><p class="caption">InceptionV3</p></section>
<section id="examples-of-popular-deep-learning-models-in-computer-vision-1" class="slide level2">
<h2>Examples of popular Deep Learning models in computer vision</h2>
<ul>
<li>U-Net for cell segmentation</li>
</ul>

<img data-src="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png" class="r-stretch quarto-figure-center"><p class="caption">U-Net</p></section>
<section id="examples-of-popular-deep-learning-models-in-computer-vision-2" class="slide level2">
<h2>Examples of popular Deep Learning models in computer vision</h2>
<ul>
<li>LeNet-5 for handwritten digits classification (<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">LeCun et al.</a>)</li>
</ul>
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/6/61/LeNet_architecture.png" alt="LeNet-5"> By Daniel Voigt Godoy - <a rel="nofollow" class="external free" href="https://github.com/dvgodoy/dl-visuals/">https://github.com/dvgodoy/dl-visuals/</a>, <a href="https://creativecommons.org/licenses/by/4.0" title="Creative Commons Attribution 4.0">CC BY 4.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=150820922">Link</a></p>
</section>
<section id="exercise-implement-and-train-the-letnet-5-model-with-pytorch" class="slide level2">
<h2>Exercise: Implement and train the LetNet-5 model with PyTorch</h2>
<ul class="task-list">
<li><label><input type="checkbox">Build the convolutional neural network using <code>nn.Sequential</code>, and the <code>nn.ReLU()</code> activation function.</label></li>
</ul>
<div id="fdee84a5" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a></a>lenet_clf <span class="op">=</span> nn.Sequential(</span>
<span id="cb52-2"><a></a>    nn.Conv2d(in_channels<span class="op">=</span><span class="dv">3</span>, out_channels<span class="op">=</span><span class="dv">6</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, bias<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb52-3"><a></a>    nn.ReLU(),</span>
<span id="cb52-4"><a></a>    nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb52-5"><a></a>    nn.Conv2d(in_channels<span class="op">=</span><span class="dv">6</span>, out_channels<span class="op">=</span><span class="dv">16</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, bias<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb52-6"><a></a>    nn.ReLU(),</span>
<span id="cb52-7"><a></a>    nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb52-8"><a></a>    nn.Flatten(),</span>
<span id="cb52-9"><a></a>    nn.Linear(in_features<span class="op">=</span><span class="dv">16</span><span class="op">*</span><span class="dv">5</span><span class="op">*</span><span class="dv">5</span>, out_features<span class="op">=</span><span class="dv">120</span>, bias<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb52-10"><a></a>    nn.ReLU(),</span>
<span id="cb52-11"><a></a>    nn.Linear(in_features<span class="op">=</span><span class="dv">120</span>, out_features<span class="op">=</span><span class="dv">84</span>, bias<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb52-12"><a></a>    nn.ReLU(),</span>
<span id="cb52-13"><a></a>    nn.Linear(in_features<span class="op">=</span><span class="dv">84</span>, out_features<span class="op">=</span><span class="dv">100</span>, bias<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb52-14"><a></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Pooling layers are used to downsample feature maps to summarize information from large regions.</p>
</div>
</div>
</div>
</section>
<section id="exercise-implement-and-train-the-letnet-5-model-with-pytorch-1" class="slide level2">
<h2>Exercise: Implement and train the LetNet-5 model with PyTorch</h2>
<ul class="task-list">
<li><label><input type="checkbox">Test our implementation.</label></li>
</ul>
<div id="424d75d7" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a></a>y_hat <span class="op">=</span> lenet_clf(x)</span>
<span id="cb53-2"><a></a></span>
<span id="cb53-3"><a></a><span class="bu">type</span>(y_hat), y_hat.dtype, y_hat.shape, y_hat.<span class="bu">min</span>(), y_hat.<span class="bu">max</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>(torch.Tensor,
 torch.float32,
 torch.Size([128, 100]),
 tensor(-0.1779, grad_fn=&lt;MinBackward1&gt;),
 tensor(0.1641, grad_fn=&lt;MaxBackward1&gt;))</code></pre>
</div>
</div>
</section>
<section id="exercise-implement-and-train-the-letnet-5-model-with-pytorch-2" class="slide level2 scrollable">
<h2>Exercise: Implement and train the LetNet-5 model with PyTorch</h2>
<ul class="task-list">
<li><label><input type="checkbox">Train the model to classify images from CIFAR-100.</label></li>
</ul>
<div id="4530663f" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a></a>num_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb55-2"><a></a>train_loss <span class="op">=</span> []</span>
<span id="cb55-3"><a></a>val_loss <span class="op">=</span> []</span>
<span id="cb55-4"><a></a></span>
<span id="cb55-5"><a></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb55-6"><a></a>  lenet_clf.cuda()</span>
<span id="cb55-7"><a></a></span>
<span id="cb55-8"><a></a>optimizer <span class="op">=</span> optim.SGD(lenet_clf.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb55-9"><a></a></span>
<span id="cb55-10"><a></a><span class="cf">for</span> e <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb55-11"><a></a>  train_loss_avg <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb55-12"><a></a>  total_train_samples <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb55-13"><a></a></span>
<span id="cb55-14"><a></a>  lenet_clf.train()</span>
<span id="cb55-15"><a></a>  <span class="cf">for</span> x, y <span class="kw">in</span> cifar_train_dl:</span>
<span id="cb55-16"><a></a>    optimizer.zero_grad()</span>
<span id="cb55-17"><a></a></span>
<span id="cb55-18"><a></a>    <span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb55-19"><a></a>      x <span class="op">=</span> x.cuda()</span>
<span id="cb55-20"><a></a>    </span>
<span id="cb55-21"><a></a>    y_hat <span class="op">=</span> lenet_clf( x ).cpu()</span>
<span id="cb55-22"><a></a></span>
<span id="cb55-23"><a></a>    loss <span class="op">=</span> loss_fun(y_hat, y)</span>
<span id="cb55-24"><a></a></span>
<span id="cb55-25"><a></a>    train_loss_avg <span class="op">+=</span> loss.item() <span class="op">*</span> <span class="bu">len</span>(x)</span>
<span id="cb55-26"><a></a>    total_train_samples <span class="op">+=</span> <span class="bu">len</span>(x)</span>
<span id="cb55-27"><a></a></span>
<span id="cb55-28"><a></a>    loss.backward()</span>
<span id="cb55-29"><a></a></span>
<span id="cb55-30"><a></a>    optimizer.step()</span>
<span id="cb55-31"><a></a></span>
<span id="cb55-32"><a></a>  train_loss_avg <span class="op">/=</span> total_train_samples</span>
<span id="cb55-33"><a></a>  train_loss.append(train_loss_avg)</span>
<span id="cb55-34"><a></a></span>
<span id="cb55-35"><a></a>  val_loss_avg <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb55-36"><a></a>  total_val_samples <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb55-37"><a></a></span>
<span id="cb55-38"><a></a>  lenet_clf.<span class="bu">eval</span>()</span>
<span id="cb55-39"><a></a>  <span class="cf">with</span> torch.no_grad():</span>
<span id="cb55-40"><a></a>    <span class="cf">for</span> x, y <span class="kw">in</span> cifar_val_dl:</span>
<span id="cb55-41"><a></a>      <span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb55-42"><a></a>        x <span class="op">=</span> x.cuda()</span>
<span id="cb55-43"><a></a>      </span>
<span id="cb55-44"><a></a>      y_hat <span class="op">=</span> lenet_clf( x ).cpu()</span>
<span id="cb55-45"><a></a>      loss <span class="op">=</span> loss_fun(y_hat, y)</span>
<span id="cb55-46"><a></a></span>
<span id="cb55-47"><a></a>      val_loss_avg <span class="op">+=</span> loss.item() <span class="op">*</span> <span class="bu">len</span>(x)</span>
<span id="cb55-48"><a></a>      total_val_samples <span class="op">+=</span> <span class="bu">len</span>(x)</span>
<span id="cb55-49"><a></a></span>
<span id="cb55-50"><a></a>  val_loss_avg <span class="op">/=</span> total_val_samples</span>
<span id="cb55-51"><a></a>  val_loss.append(val_loss_avg)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="exercise-implement-and-train-the-letnet-5-model-with-pytorch-3" class="slide level2">
<h2>Exercise: Implement and train the LetNet-5 model with PyTorch</h2>
<ul class="task-list">
<li><label><input type="checkbox">Plot the average train and validation losses</label></li>
</ul>
<div id="e36ac04b" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a></a>plt.plot(train_loss, <span class="st">"b-"</span>, label<span class="op">=</span><span class="st">"Average training loss"</span>)</span>
<span id="cb56-2"><a></a>plt.plot(val_loss, <span class="st">"r-"</span>, label<span class="op">=</span><span class="st">"Average validation loss"</span>)</span>
<span id="cb56-3"><a></a>plt.legend()</span>
<span id="cb56-4"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="DL_image_analysis_1_1_files/figure-revealjs/cell-43-output-1.png" width="439" height="415"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="exercise-implement-and-train-the-letnet-5-model-with-pytorch-4" class="slide level2 scrollable">
<h2>Exercise: Implement and train the LetNet-5 model with PyTorch</h2>
<ul class="task-list">
<li><label><input type="checkbox">Compute the average accuracy for the Validation and Test sets.</label></li>
</ul>
<div id="b7e0de03" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a></a>lenet_clf.<span class="bu">eval</span>()</span>
<span id="cb57-2"><a></a></span>
<span id="cb57-3"><a></a>val_acc_metric <span class="op">=</span> Accuracy(task<span class="op">=</span><span class="st">"multiclass"</span>, num_classes<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb57-4"><a></a>test_acc_metric <span class="op">=</span> Accuracy(task<span class="op">=</span><span class="st">"multiclass"</span>, num_classes<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb57-5"><a></a>train_acc_metric <span class="op">=</span> Accuracy(task<span class="op">=</span><span class="st">"multiclass"</span>, num_classes<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb57-6"><a></a></span>
<span id="cb57-7"><a></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb57-8"><a></a>  <span class="cf">for</span> x, y <span class="kw">in</span> cifar_train_dl:</span>
<span id="cb57-9"><a></a>    <span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb57-10"><a></a>      x <span class="op">=</span> x.cuda()</span>
<span id="cb57-11"><a></a>    y_hat <span class="op">=</span> lenet_clf( x ).cpu()</span>
<span id="cb57-12"><a></a>    train_acc_metric(y_hat.softmax(dim<span class="op">=</span><span class="dv">1</span>), y)</span>
<span id="cb57-13"><a></a></span>
<span id="cb57-14"><a></a>  train_acc <span class="op">=</span> train_acc_metric.compute()</span>
<span id="cb57-15"><a></a></span>
<span id="cb57-16"><a></a>  <span class="cf">for</span> x, y <span class="kw">in</span> cifar_val_dl:</span>
<span id="cb57-17"><a></a>    <span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb57-18"><a></a>      x <span class="op">=</span> x.cuda()</span>
<span id="cb57-19"><a></a>    y_hat <span class="op">=</span> lenet_clf( x ).cpu()</span>
<span id="cb57-20"><a></a>    val_acc_metric(y_hat.softmax(dim<span class="op">=</span><span class="dv">1</span>), y)</span>
<span id="cb57-21"><a></a></span>
<span id="cb57-22"><a></a>  val_acc <span class="op">=</span> val_acc_metric.compute()</span>
<span id="cb57-23"><a></a></span>
<span id="cb57-24"><a></a>  <span class="cf">for</span> x, y <span class="kw">in</span> cifar_test_dl:</span>
<span id="cb57-25"><a></a>    <span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb57-26"><a></a>      x <span class="op">=</span> x.cuda()</span>
<span id="cb57-27"><a></a>    y_hat <span class="op">=</span> lenet_clf( x ).cpu()</span>
<span id="cb57-28"><a></a>    test_acc_metric(y_hat.softmax(dim<span class="op">=</span><span class="dv">1</span>), y)</span>
<span id="cb57-29"><a></a></span>
<span id="cb57-30"><a></a>  test_acc <span class="op">=</span> test_acc_metric.compute()</span>
<span id="cb57-31"><a></a></span>
<span id="cb57-32"><a></a><span class="bu">print</span>(<span class="ss">f"Training acc=</span><span class="sc">{</span>train_acc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb57-33"><a></a><span class="bu">print</span>(<span class="ss">f"Validation acc=</span><span class="sc">{</span>val_acc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb57-34"><a></a><span class="bu">print</span>(<span class="ss">f"Test acc=</span><span class="sc">{</span>test_acc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb57-35"><a></a></span>
<span id="cb57-36"><a></a>train_acc_metric.reset()</span>
<span id="cb57-37"><a></a>val_acc_metric.reset()</span>
<span id="cb57-38"><a></a>test_acc_metric.reset()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training acc=0.02437499910593033
Validation acc=0.020899999886751175
Test acc=0.02250000089406967</code></pre>
</div>
</div>

<div class="quarto-auto-generated-content">
<div class="footer footer-default">

</div>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        // For code content inside modals, clipBoardJS needs to be initialized with a container option
        // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>