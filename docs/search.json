[
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_1.html#select-runtime-and-connect",
    "href": "notes/Adv_ML_Python_presentation_2_1.html#select-runtime-and-connect",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 1)",
    "section": "Select runtime and connect",
    "text": "Select runtime and connect\nOn the top right corner of the page, click the drop-down arrow to the right of the Connect button and select Change runtime type."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_1.html#load-pre-trained-models-1",
    "href": "notes/Adv_ML_Python_presentation_2_1.html#load-pre-trained-models-1",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 1)",
    "section": "Load pre-trained models",
    "text": "Load pre-trained models\n\nLets use one from the PyTorchâ€™s torchvision module for computer vision\nTry first with the InceptionV3 model."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_1.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images",
    "href": "notes/Adv_ML_Python_presentation_2_1.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 1)",
    "section": "Exercise: Use a pre-trained deep learning model to classify images",
    "text": "Exercise: Use a pre-trained deep learning model to classify images\n\nImport the pre-trained weights of the Inception V3 model from models.inception_v3\n\n\nimport torch\nfrom torchvision import models\n\ninception_weights = models.inception.Inception_V3_Weights.IMAGENET1K_V1\n\ninception_weights.meta\n\n{'num_params': 27161264,\n 'min_size': (75, 75),\n 'categories': ['tench',\n  'goldfish',\n  'great white shark',\n  'tiger shark',\n  'hammerhead',\n  'electric ray',\n  'stingray',\n  'cock',\n  'hen',\n  'ostrich',\n  'brambling',\n  'goldfinch',\n  'house finch',\n  'junco',\n  'indigo bunting',\n  'robin',\n  'bulbul',\n  'jay',\n  'magpie',\n  'chickadee',\n  'water ouzel',\n  'kite',\n  'bald eagle',\n  'vulture',\n  'great grey owl',\n  'European fire salamander',\n  'common newt',\n  'eft',\n  'spotted salamander',\n  'axolotl',\n  'bullfrog',\n  'tree frog',\n  'tailed frog',\n  'loggerhead',\n  'leatherback turtle',\n  'mud turtle',\n  'terrapin',\n  'box turtle',\n  'banded gecko',\n  'common iguana',\n  'American chameleon',\n  'whiptail',\n  'agama',\n  'frilled lizard',\n  'alligator lizard',\n  'Gila monster',\n  'green lizard',\n  'African chameleon',\n  'Komodo dragon',\n  'African crocodile',\n  'American alligator',\n  'triceratops',\n  'thunder snake',\n  'ringneck snake',\n  'hognose snake',\n  'green snake',\n  'king snake',\n  'garter snake',\n  'water snake',\n  'vine snake',\n  'night snake',\n  'boa constrictor',\n  'rock python',\n  'Indian cobra',\n  'green mamba',\n  'sea snake',\n  'horned viper',\n  'diamondback',\n  'sidewinder',\n  'trilobite',\n  'harvestman',\n  'scorpion',\n  'black and gold garden spider',\n  'barn spider',\n  'garden spider',\n  'black widow',\n  'tarantula',\n  'wolf spider',\n  'tick',\n  'centipede',\n  'black grouse',\n  'ptarmigan',\n  'ruffed grouse',\n  'prairie chicken',\n  'peacock',\n  'quail',\n  'partridge',\n  'African grey',\n  'macaw',\n  'sulphur-crested cockatoo',\n  'lorikeet',\n  'coucal',\n  'bee eater',\n  'hornbill',\n  'hummingbird',\n  'jacamar',\n  'toucan',\n  'drake',\n  'red-breasted merganser',\n  'goose',\n  'black swan',\n  'tusker',\n  'echidna',\n  'platypus',\n  'wallaby',\n  'koala',\n  'wombat',\n  'jellyfish',\n  'sea anemone',\n  'brain coral',\n  'flatworm',\n  'nematode',\n  'conch',\n  'snail',\n  'slug',\n  'sea slug',\n  'chiton',\n  'chambered nautilus',\n  'Dungeness crab',\n  'rock crab',\n  'fiddler crab',\n  'king crab',\n  'American lobster',\n  'spiny lobster',\n  'crayfish',\n  'hermit crab',\n  'isopod',\n  'white stork',\n  'black stork',\n  'spoonbill',\n  'flamingo',\n  'little blue heron',\n  'American egret',\n  'bittern',\n  'crane bird',\n  'limpkin',\n  'European gallinule',\n  'American coot',\n  'bustard',\n  'ruddy turnstone',\n  'red-backed sandpiper',\n  'redshank',\n  'dowitcher',\n  'oystercatcher',\n  'pelican',\n  'king penguin',\n  'albatross',\n  'grey whale',\n  'killer whale',\n  'dugong',\n  'sea lion',\n  'Chihuahua',\n  'Japanese spaniel',\n  'Maltese dog',\n  'Pekinese',\n  'Shih-Tzu',\n  'Blenheim spaniel',\n  'papillon',\n  'toy terrier',\n  'Rhodesian ridgeback',\n  'Afghan hound',\n  'basset',\n  'beagle',\n  'bloodhound',\n  'bluetick',\n  'black-and-tan coonhound',\n  'Walker hound',\n  'English foxhound',\n  'redbone',\n  'borzoi',\n  'Irish wolfhound',\n  'Italian greyhound',\n  'whippet',\n  'Ibizan hound',\n  'Norwegian elkhound',\n  'otterhound',\n  'Saluki',\n  'Scottish deerhound',\n  'Weimaraner',\n  'Staffordshire bullterrier',\n  'American Staffordshire terrier',\n  'Bedlington terrier',\n  'Border terrier',\n  'Kerry blue terrier',\n  'Irish terrier',\n  'Norfolk terrier',\n  'Norwich terrier',\n  'Yorkshire terrier',\n  'wire-haired fox terrier',\n  'Lakeland terrier',\n  'Sealyham terrier',\n  'Airedale',\n  'cairn',\n  'Australian terrier',\n  'Dandie Dinmont',\n  'Boston bull',\n  'miniature schnauzer',\n  'giant schnauzer',\n  'standard schnauzer',\n  'Scotch terrier',\n  'Tibetan terrier',\n  'silky terrier',\n  'soft-coated wheaten terrier',\n  'West Highland white terrier',\n  'Lhasa',\n  'flat-coated retriever',\n  'curly-coated retriever',\n  'golden retriever',\n  'Labrador retriever',\n  'Chesapeake Bay retriever',\n  'German short-haired pointer',\n  'vizsla',\n  'English setter',\n  'Irish setter',\n  'Gordon setter',\n  'Brittany spaniel',\n  'clumber',\n  'English springer',\n  'Welsh springer spaniel',\n  'cocker spaniel',\n  'Sussex spaniel',\n  'Irish water spaniel',\n  'kuvasz',\n  'schipperke',\n  'groenendael',\n  'malinois',\n  'briard',\n  'kelpie',\n  'komondor',\n  'Old English sheepdog',\n  'Shetland sheepdog',\n  'collie',\n  'Border collie',\n  'Bouvier des Flandres',\n  'Rottweiler',\n  'German shepherd',\n  'Doberman',\n  'miniature pinscher',\n  'Greater Swiss Mountain dog',\n  'Bernese mountain dog',\n  'Appenzeller',\n  'EntleBucher',\n  'boxer',\n  'bull mastiff',\n  'Tibetan mastiff',\n  'French bulldog',\n  'Great Dane',\n  'Saint Bernard',\n  'Eskimo dog',\n  'malamute',\n  'Siberian husky',\n  'dalmatian',\n  'affenpinscher',\n  'basenji',\n  'pug',\n  'Leonberg',\n  'Newfoundland',\n  'Great Pyrenees',\n  'Samoyed',\n  'Pomeranian',\n  'chow',\n  'keeshond',\n  'Brabancon griffon',\n  'Pembroke',\n  'Cardigan',\n  'toy poodle',\n  'miniature poodle',\n  'standard poodle',\n  'Mexican hairless',\n  'timber wolf',\n  'white wolf',\n  'red wolf',\n  'coyote',\n  'dingo',\n  'dhole',\n  'African hunting dog',\n  'hyena',\n  'red fox',\n  'kit fox',\n  'Arctic fox',\n  'grey fox',\n  'tabby',\n  'tiger cat',\n  'Persian cat',\n  'Siamese cat',\n  'Egyptian cat',\n  'cougar',\n  'lynx',\n  'leopard',\n  'snow leopard',\n  'jaguar',\n  'lion',\n  'tiger',\n  'cheetah',\n  'brown bear',\n  'American black bear',\n  'ice bear',\n  'sloth bear',\n  'mongoose',\n  'meerkat',\n  'tiger beetle',\n  'ladybug',\n  'ground beetle',\n  'long-horned beetle',\n  'leaf beetle',\n  'dung beetle',\n  'rhinoceros beetle',\n  'weevil',\n  'fly',\n  'bee',\n  'ant',\n  'grasshopper',\n  'cricket',\n  'walking stick',\n  'cockroach',\n  'mantis',\n  'cicada',\n  'leafhopper',\n  'lacewing',\n  'dragonfly',\n  'damselfly',\n  'admiral',\n  'ringlet',\n  'monarch',\n  'cabbage butterfly',\n  'sulphur butterfly',\n  'lycaenid',\n  'starfish',\n  'sea urchin',\n  'sea cucumber',\n  'wood rabbit',\n  'hare',\n  'Angora',\n  'hamster',\n  'porcupine',\n  'fox squirrel',\n  'marmot',\n  'beaver',\n  'guinea pig',\n  'sorrel',\n  'zebra',\n  'hog',\n  'wild boar',\n  'warthog',\n  'hippopotamus',\n  'ox',\n  'water buffalo',\n  'bison',\n  'ram',\n  'bighorn',\n  'ibex',\n  'hartebeest',\n  'impala',\n  'gazelle',\n  'Arabian camel',\n  'llama',\n  'weasel',\n  'mink',\n  'polecat',\n  'black-footed ferret',\n  'otter',\n  'skunk',\n  'badger',\n  'armadillo',\n  'three-toed sloth',\n  'orangutan',\n  'gorilla',\n  'chimpanzee',\n  'gibbon',\n  'siamang',\n  'guenon',\n  'patas',\n  'baboon',\n  'macaque',\n  'langur',\n  'colobus',\n  'proboscis monkey',\n  'marmoset',\n  'capuchin',\n  'howler monkey',\n  'titi',\n  'spider monkey',\n  'squirrel monkey',\n  'Madagascar cat',\n  'indri',\n  'Indian elephant',\n  'African elephant',\n  'lesser panda',\n  'giant panda',\n  'barracouta',\n  'eel',\n  'coho',\n  'rock beauty',\n  'anemone fish',\n  'sturgeon',\n  'gar',\n  'lionfish',\n  'puffer',\n  'abacus',\n  'abaya',\n  'academic gown',\n  'accordion',\n  'acoustic guitar',\n  'aircraft carrier',\n  'airliner',\n  'airship',\n  'altar',\n  'ambulance',\n  'amphibian',\n  'analog clock',\n  'apiary',\n  'apron',\n  'ashcan',\n  'assault rifle',\n  'backpack',\n  'bakery',\n  'balance beam',\n  'balloon',\n  'ballpoint',\n  'Band Aid',\n  'banjo',\n  'bannister',\n  'barbell',\n  'barber chair',\n  'barbershop',\n  'barn',\n  'barometer',\n  'barrel',\n  'barrow',\n  'baseball',\n  'basketball',\n  'bassinet',\n  'bassoon',\n  'bathing cap',\n  'bath towel',\n  'bathtub',\n  'beach wagon',\n  'beacon',\n  'beaker',\n  'bearskin',\n  'beer bottle',\n  'beer glass',\n  'bell cote',\n  'bib',\n  'bicycle-built-for-two',\n  'bikini',\n  'binder',\n  'binoculars',\n  'birdhouse',\n  'boathouse',\n  'bobsled',\n  'bolo tie',\n  'bonnet',\n  'bookcase',\n  'bookshop',\n  'bottlecap',\n  'bow',\n  'bow tie',\n  'brass',\n  'brassiere',\n  'breakwater',\n  'breastplate',\n  'broom',\n  'bucket',\n  'buckle',\n  'bulletproof vest',\n  'bullet train',\n  'butcher shop',\n  'cab',\n  'caldron',\n  'candle',\n  'cannon',\n  'canoe',\n  'can opener',\n  'cardigan',\n  'car mirror',\n  'carousel',\n  \"carpenter's kit\",\n  'carton',\n  'car wheel',\n  'cash machine',\n  'cassette',\n  'cassette player',\n  'castle',\n  'catamaran',\n  'CD player',\n  'cello',\n  'cellular telephone',\n  'chain',\n  'chainlink fence',\n  'chain mail',\n  'chain saw',\n  'chest',\n  'chiffonier',\n  'chime',\n  'china cabinet',\n  'Christmas stocking',\n  'church',\n  'cinema',\n  'cleaver',\n  'cliff dwelling',\n  'cloak',\n  'clog',\n  'cocktail shaker',\n  'coffee mug',\n  'coffeepot',\n  'coil',\n  'combination lock',\n  'computer keyboard',\n  'confectionery',\n  'container ship',\n  'convertible',\n  'corkscrew',\n  'cornet',\n  'cowboy boot',\n  'cowboy hat',\n  'cradle',\n  'crane',\n  'crash helmet',\n  'crate',\n  'crib',\n  'Crock Pot',\n  'croquet ball',\n  'crutch',\n  'cuirass',\n  'dam',\n  'desk',\n  'desktop computer',\n  'dial telephone',\n  'diaper',\n  'digital clock',\n  'digital watch',\n  'dining table',\n  'dishrag',\n  'dishwasher',\n  'disk brake',\n  'dock',\n  'dogsled',\n  'dome',\n  'doormat',\n  'drilling platform',\n  'drum',\n  'drumstick',\n  'dumbbell',\n  'Dutch oven',\n  'electric fan',\n  'electric guitar',\n  'electric locomotive',\n  'entertainment center',\n  'envelope',\n  'espresso maker',\n  'face powder',\n  'feather boa',\n  'file',\n  'fireboat',\n  'fire engine',\n  'fire screen',\n  'flagpole',\n  'flute',\n  'folding chair',\n  'football helmet',\n  'forklift',\n  'fountain',\n  'fountain pen',\n  'four-poster',\n  'freight car',\n  'French horn',\n  'frying pan',\n  'fur coat',\n  'garbage truck',\n  'gasmask',\n  'gas pump',\n  'goblet',\n  'go-kart',\n  'golf ball',\n  'golfcart',\n  'gondola',\n  'gong',\n  'gown',\n  'grand piano',\n  'greenhouse',\n  'grille',\n  'grocery store',\n  'guillotine',\n  'hair slide',\n  'hair spray',\n  'half track',\n  'hammer',\n  'hamper',\n  'hand blower',\n  'hand-held computer',\n  'handkerchief',\n  'hard disc',\n  'harmonica',\n  'harp',\n  'harvester',\n  'hatchet',\n  'holster',\n  'home theater',\n  'honeycomb',\n  'hook',\n  'hoopskirt',\n  'horizontal bar',\n  'horse cart',\n  'hourglass',\n  'iPod',\n  'iron',\n  \"jack-o'-lantern\",\n  'jean',\n  'jeep',\n  'jersey',\n  'jigsaw puzzle',\n  'jinrikisha',\n  'joystick',\n  'kimono',\n  'knee pad',\n  'knot',\n  'lab coat',\n  'ladle',\n  'lampshade',\n  'laptop',\n  'lawn mower',\n  'lens cap',\n  'letter opener',\n  'library',\n  'lifeboat',\n  'lighter',\n  'limousine',\n  'liner',\n  'lipstick',\n  'Loafer',\n  'lotion',\n  'loudspeaker',\n  'loupe',\n  'lumbermill',\n  'magnetic compass',\n  'mailbag',\n  'mailbox',\n  'maillot',\n  'maillot tank suit',\n  'manhole cover',\n  'maraca',\n  'marimba',\n  'mask',\n  'matchstick',\n  'maypole',\n  'maze',\n  'measuring cup',\n  'medicine chest',\n  'megalith',\n  'microphone',\n  'microwave',\n  'military uniform',\n  'milk can',\n  'minibus',\n  'miniskirt',\n  'minivan',\n  'missile',\n  'mitten',\n  'mixing bowl',\n  'mobile home',\n  'Model T',\n  'modem',\n  'monastery',\n  'monitor',\n  'moped',\n  'mortar',\n  'mortarboard',\n  'mosque',\n  'mosquito net',\n  'motor scooter',\n  'mountain bike',\n  'mountain tent',\n  'mouse',\n  'mousetrap',\n  'moving van',\n  'muzzle',\n  'nail',\n  'neck brace',\n  'necklace',\n  'nipple',\n  'notebook',\n  'obelisk',\n  'oboe',\n  'ocarina',\n  'odometer',\n  'oil filter',\n  'organ',\n  'oscilloscope',\n  'overskirt',\n  'oxcart',\n  'oxygen mask',\n  'packet',\n  'paddle',\n  'paddlewheel',\n  'padlock',\n  'paintbrush',\n  'pajama',\n  'palace',\n  'panpipe',\n  'paper towel',\n  'parachute',\n  'parallel bars',\n  'park bench',\n  'parking meter',\n  'passenger car',\n  'patio',\n  'pay-phone',\n  'pedestal',\n  'pencil box',\n  'pencil sharpener',\n  'perfume',\n  'Petri dish',\n  'photocopier',\n  'pick',\n  'pickelhaube',\n  'picket fence',\n  'pickup',\n  'pier',\n  'piggy bank',\n  'pill bottle',\n  'pillow',\n  'ping-pong ball',\n  'pinwheel',\n  'pirate',\n  'pitcher',\n  'plane',\n  'planetarium',\n  'plastic bag',\n  'plate rack',\n  'plow',\n  'plunger',\n  'Polaroid camera',\n  'pole',\n  'police van',\n  'poncho',\n  'pool table',\n  'pop bottle',\n  'pot',\n  \"potter's wheel\",\n  'power drill',\n  'prayer rug',\n  'printer',\n  'prison',\n  'projectile',\n  'projector',\n  'puck',\n  'punching bag',\n  'purse',\n  'quill',\n  'quilt',\n  'racer',\n  'racket',\n  'radiator',\n  'radio',\n  'radio telescope',\n  'rain barrel',\n  'recreational vehicle',\n  'reel',\n  'reflex camera',\n  'refrigerator',\n  'remote control',\n  'restaurant',\n  'revolver',\n  'rifle',\n  'rocking chair',\n  'rotisserie',\n  'rubber eraser',\n  'rugby ball',\n  'rule',\n  'running shoe',\n  'safe',\n  'safety pin',\n  'saltshaker',\n  'sandal',\n  'sarong',\n  'sax',\n  'scabbard',\n  'scale',\n  'school bus',\n  'schooner',\n  'scoreboard',\n  'screen',\n  'screw',\n  'screwdriver',\n  'seat belt',\n  'sewing machine',\n  'shield',\n  'shoe shop',\n  'shoji',\n  'shopping basket',\n  'shopping cart',\n  'shovel',\n  'shower cap',\n  'shower curtain',\n  'ski',\n  'ski mask',\n  'sleeping bag',\n  'slide rule',\n  'sliding door',\n  'slot',\n  'snorkel',\n  'snowmobile',\n  'snowplow',\n  'soap dispenser',\n  'soccer ball',\n  'sock',\n  'solar dish',\n  'sombrero',\n  'soup bowl',\n  'space bar',\n  'space heater',\n  'space shuttle',\n  'spatula',\n  'speedboat',\n  'spider web',\n  'spindle',\n  'sports car',\n  'spotlight',\n  'stage',\n  'steam locomotive',\n  'steel arch bridge',\n  'steel drum',\n  'stethoscope',\n  'stole',\n  'stone wall',\n  'stopwatch',\n  'stove',\n  'strainer',\n  'streetcar',\n  'stretcher',\n  'studio couch',\n  'stupa',\n  'submarine',\n  'suit',\n  'sundial',\n  'sunglass',\n  'sunglasses',\n  'sunscreen',\n  'suspension bridge',\n  'swab',\n  'sweatshirt',\n  'swimming trunks',\n  'swing',\n  'switch',\n  'syringe',\n  'table lamp',\n  'tank',\n  'tape player',\n  'teapot',\n  'teddy',\n  'television',\n  'tennis ball',\n  'thatch',\n  'theater curtain',\n  'thimble',\n  'thresher',\n  'throne',\n  'tile roof',\n  'toaster',\n  'tobacco shop',\n  'toilet seat',\n  'torch',\n  'totem pole',\n  'tow truck',\n  'toyshop',\n  'tractor',\n  'trailer truck',\n  'tray',\n  'trench coat',\n  'tricycle',\n  'trimaran',\n  'tripod',\n  'triumphal arch',\n  'trolleybus',\n  'trombone',\n  'tub',\n  'turnstile',\n  'typewriter keyboard',\n  'umbrella',\n  'unicycle',\n  'upright',\n  'vacuum',\n  'vase',\n  'vault',\n  'velvet',\n  'vending machine',\n  'vestment',\n  'viaduct',\n  'violin',\n  'volleyball',\n  'waffle iron',\n  'wall clock',\n  'wallet',\n  'wardrobe',\n  'warplane',\n  'washbasin',\n  'washer',\n  'water bottle',\n  'water jug',\n  'water tower',\n  'whiskey jug',\n  'whistle',\n  'wig',\n  'window screen',\n  'window shade',\n  'Windsor tie',\n  'wine bottle',\n  'wing',\n  'wok',\n  'wooden spoon',\n  'wool',\n  'worm fence',\n  'wreck',\n  'yawl',\n  'yurt',\n  'web site',\n  'comic book',\n  'crossword puzzle',\n  'street sign',\n  'traffic light',\n  'book jacket',\n  'menu',\n  'plate',\n  'guacamole',\n  'consomme',\n  'hot pot',\n  'trifle',\n  'ice cream',\n  'ice lolly',\n  'French loaf',\n  'bagel',\n  'pretzel',\n  'cheeseburger',\n  'hotdog',\n  'mashed potato',\n  'head cabbage',\n  'broccoli',\n  'cauliflower',\n  'zucchini',\n  'spaghetti squash',\n  'acorn squash',\n  'butternut squash',\n  'cucumber',\n  'artichoke',\n  'bell pepper',\n  'cardoon',\n  'mushroom',\n  'Granny Smith',\n  'strawberry',\n  'orange',\n  'lemon',\n  'fig',\n  'pineapple',\n  'banana',\n  'jackfruit',\n  'custard apple',\n  'pomegranate',\n  'hay',\n  'carbonara',\n  'chocolate sauce',\n  'dough',\n  'meat loaf',\n  'pizza',\n  'potpie',\n  'burrito',\n  'red wine',\n  'espresso',\n  'cup',\n  'eggnog',\n  'alp',\n  'bubble',\n  'cliff',\n  'coral reef',\n  'geyser',\n  'lakeside',\n  'promontory',\n  'sandbar',\n  'seashore',\n  'valley',\n  'volcano',\n  'ballplayer',\n  'groom',\n  'scuba diver',\n  'rapeseed',\n  'daisy',\n  \"yellow lady's slipper\",\n  'corn',\n  'acorn',\n  'hip',\n  'buckeye',\n  'coral fungus',\n  'agaric',\n  'gyromitra',\n  'stinkhorn',\n  'earthstar',\n  'hen-of-the-woods',\n  'bolete',\n  'ear',\n  'toilet tissue'],\n 'recipe': 'https://github.com/pytorch/vision/tree/main/references/classification#inception-v3',\n '_metrics': {'ImageNet-1K': {'acc@1': 77.294, 'acc@5': 93.45}},\n '_ops': 5.713,\n '_file_size': 103.903,\n '_docs': 'These weights are ported from the original paper.'}\n\n\n\nStore the categories in a variable to use them later\n\n\ncategories = inception_weights.meta[\"categories\"]\n\n\n\n\n\n\n\nTip\n\n\nMore info about Inception V3 implementation in torchvision here"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_1.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-1",
    "href": "notes/Adv_ML_Python_presentation_2_1.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-1",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 1)",
    "section": "Exercise: Use a pre-trained deep learning model to classify images",
    "text": "Exercise: Use a pre-trained deep learning model to classify images\n\nLoad the Inception V3 model using the pre-trained weights inception_weights\n\n\ndl_model = models.inception_v3(inception_weights, progress=True)\n\ndl_model.eval()\n\nInception3(\n  (Conv2d_1a_3x3): BasicConv2d(\n    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (Conv2d_2a_3x3): BasicConv2d(\n    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (Conv2d_2b_3x3): BasicConv2d(\n    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (Conv2d_3b_1x1): BasicConv2d(\n    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (Conv2d_4a_3x3): BasicConv2d(\n    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (Mixed_5b): InceptionA(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_1): BasicConv2d(\n      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_2): BasicConv2d(\n      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_5c): InceptionA(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_1): BasicConv2d(\n      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_2): BasicConv2d(\n      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_5d): InceptionA(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_1): BasicConv2d(\n      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_2): BasicConv2d(\n      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6a): InceptionB(\n    (branch3x3): BasicConv2d(\n      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6b): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6c): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6d): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6e): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (AuxLogits): InceptionAux(\n    (conv0): BasicConv2d(\n      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (conv1): BasicConv2d(\n      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (fc): Linear(in_features=768, out_features=1000, bias=True)\n  )\n  (Mixed_7a): InceptionD(\n    (branch3x3_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2): BasicConv2d(\n      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_2): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_3): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_4): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_7b): InceptionE(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_1): BasicConv2d(\n      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_7c): InceptionE(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_1): BasicConv2d(\n      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_1.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-2",
    "href": "notes/Adv_ML_Python_presentation_2_1.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-2",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 1)",
    "section": "Exercise: Use a pre-trained deep learning model to classify images",
    "text": "Exercise: Use a pre-trained deep learning model to classify images\n\nLoad a sample image to predict its category\n\n\nimport skimage\nimport matplotlib.pyplot as plt\n\nsample_im = skimage.data.rocket()\nsample_im.shape\n\n(427, 640, 3)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_1.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-3",
    "href": "notes/Adv_ML_Python_presentation_2_1.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-3",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 1)",
    "section": "Exercise: Use a pre-trained deep learning model to classify images",
    "text": "Exercise: Use a pre-trained deep learning model to classify images\n\nVisualize the sample image\n\n\nplt.imshow(sample_im)\nplt.show()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_1.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-4",
    "href": "notes/Adv_ML_Python_presentation_2_1.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-4",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 1)",
    "section": "Exercise: Use a pre-trained deep learning model to classify images",
    "text": "Exercise: Use a pre-trained deep learning model to classify images\n\nInspect what transforms are required by the pre-trained Inception model to work properly\n\n\ninception_weights.transforms\n\nfunctools.partial(&lt;class 'torchvision.transforms._presets.ImageClassification'&gt;, crop_size=299, resize_size=342)\n\n\n\n\n\n\n\n\nImportant\n\n\nfunctools.partial is a function to define functions with static arguments. So ðŸ‘† returns a function when it is called!\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe transforms used by the Inception V3 are\n\nresize the image to 342x342 pixels,\ncrop the center 299x299 pixels window, and\nnormalize the values of the RGB channels."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_1.html#exercise-use-a-pretrained-deep-learning-model-to-classify-images",
    "href": "notes/Adv_ML_Python_presentation_2_1.html#exercise-use-a-pretrained-deep-learning-model-to-classify-images",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 1)",
    "section": "Exercise: Use a pretrained deep learning model to classify images",
    "text": "Exercise: Use a pretrained deep learning model to classify images\n\nDefine a preprocessing pipeline using the inception_weights.transforms() method. Add also a transformation from numpy arrays into torch tensors.\n\n\nfrom torchvision.transforms.v2 import Compose, ToTensor\n\npipeline = Compose([\n  ToTensor(),\n  inception_weights.transforms()\n])\n\npipeline\n\nCompose(\n      ToTensor()\n      ImageClassification(\n      crop_size=[299]\n      resize_size=[342]\n      mean=[0.485, 0.456, 0.406]\n      std=[0.229, 0.224, 0.225]\n      interpolation=InterpolationMode.BILINEAR\n  )\n)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_1.html#exercise-use-a-pretrained-deep-learning-model-to-classify-images-1",
    "href": "notes/Adv_ML_Python_presentation_2_1.html#exercise-use-a-pretrained-deep-learning-model-to-classify-images-1",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 1)",
    "section": "Exercise: Use a pretrained deep learning model to classify images",
    "text": "Exercise: Use a pretrained deep learning model to classify images\n\nPre-process the sample image using our pipeline\n\n\nsample_x = pipeline(sample_im)\ntype(sample_x), sample_x.shape, sample_x.min(), sample_x.max()\n\n(torch.Tensor, torch.Size([3, 299, 299]), tensor(-1.8792), tensor(2.6400))"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_1.html#exercise-use-a-pretrained-deep-learning-model-to-classify-images-2",
    "href": "notes/Adv_ML_Python_presentation_2_1.html#exercise-use-a-pretrained-deep-learning-model-to-classify-images-2",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 1)",
    "section": "Exercise: Use a pretrained deep learning model to classify images",
    "text": "Exercise: Use a pretrained deep learning model to classify images\n\nUse the pre-trained model to predict the class of our sample image\n\n\n\n\n\n\n\nCaution\n\n\nApply the model on sample_x[None, â€¦], so it is treated as a one-sample batch\n\n\n\n\nsample_y = dl_model(sample_x[None, ...])\n\nsample_y.shape\n\ntorch.Size([1, 1000])"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_1.html#exercise-use-a-pretrained-deep-learning-model-to-classify-images-3",
    "href": "notes/Adv_ML_Python_presentation_2_1.html#exercise-use-a-pretrained-deep-learning-model-to-classify-images-3",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 1)",
    "section": "Exercise: Use a pretrained deep learning model to classify images",
    "text": "Exercise: Use a pretrained deep learning model to classify images\n\n\n\n\n\n\nNote\n\n\nThe modelâ€™s output are the log-probabilities of sample_x belonging to each of the 1000 classes.\n\n\n\n\nShow the categories with the highest log-probabilities.\n\n\nsample_y.argsort(dim=1)\n\ntensor([[578, 253, 339, 982, 301,  40, 584, 897, 382,  12, 170, 181,  25, 161,\n         255, 162, 159,  14, 500,  72,  44, 368, 714, 211, 201, 240,  90, 277,\n          99, 519,  46,  13, 790, 160, 883, 307,  31, 467, 271,  32, 452, 997,\n          92, 414, 286,  64, 241, 661, 360, 381, 473, 999, 316,  24, 184, 213,\n         804,  26, 889, 383, 689,  11, 231,  28, 207, 520, 134,  77,  37, 177,\n         513, 539, 191, 264, 306,  19, 103, 529, 823, 171, 838, 247, 174, 400,\n         228, 665, 669, 278,  16, 870, 354, 412, 272, 377, 175, 411, 551,  75,\n         265, 390, 601, 929,  27, 543, 434, 268,  94, 200,  33, 504, 760, 614,\n         387, 875, 167, 809, 588, 189,  30, 291, 703, 944, 323, 610, 631,  57,\n          41, 881, 349, 938, 493, 825, 732, 249, 593, 299, 516, 237, 246, 195,\n         137, 850, 756,  91, 695, 336, 799, 431, 766, 761, 310, 337, 894, 622,\n         305, 129, 351, 989, 581, 991, 612, 110, 672, 436, 239, 226,  93,  76,\n         785,  21, 260,  63, 294, 176, 960, 528, 606, 826, 168, 166,  87, 531,\n         355, 154,  15, 774, 717, 322,  18,   0, 193, 435, 152, 678, 722, 269,\n          86,  47, 292, 759, 248, 317, 105, 302, 621, 376,  60, 793, 295, 283,\n         692, 720, 495, 859, 284, 393, 267, 596, 537, 656, 235, 831, 707, 444,\n          85, 130, 232, 534, 560, 204, 188, 430, 379, 853,  43,  61, 261, 684,\n         932, 172, 746, 178, 986, 876, 615, 996, 770, 700, 298, 459, 384, 273,\n         328,  52, 605, 736, 968,  95, 410, 922, 296, 259, 347, 401, 518, 256,\n         912, 937, 319,  97, 155, 463, 636, 457,  53, 671, 119, 893, 300, 281,\n         921, 752, 394, 163, 244, 340, 697, 985, 763, 553, 243, 133, 568, 210,\n         454, 933, 597, 443,  20, 308, 451, 579, 549, 164, 716, 917,  59, 592,\n         136, 114, 185, 690, 472, 878, 642, 135, 771, 639, 276, 456, 485, 238,\n         943, 582, 486, 544, 979,  42, 868, 511, 321, 156,   9, 721, 365, 113,\n         263, 910,  88, 499, 670,  17, 635, 734, 266, 984, 386, 140, 280, 750,\n         407,  10, 654, 197, 478,  36,  62, 775, 638,  55, 275, 861, 230, 421,\n         603, 646, 447, 112, 233, 788, 222, 655, 492, 423, 896, 618, 675, 993,\n         794, 616, 215, 915, 196, 402, 229, 852, 364,  22, 139,  81, 685, 713,\n         309, 314, 153, 116, 547, 217, 643, 566, 719, 874,  98, 330, 587, 254,\n         458, 623, 886, 901, 842, 541, 930, 817, 507, 946, 362, 849,  73, 786,\n         939,  35, 157, 396, 250, 326, 725, 345, 617, 580, 988, 778, 552, 251,\n         187, 865,  71, 887, 242, 651, 448,  23, 559, 359, 202, 145, 180, 857,\n         691, 182, 475, 813, 743, 131, 843, 304, 318, 353, 422, 440,  89, 234,\n         704, 706,  68, 728, 150, 395, 108, 391, 512, 425, 468, 947, 143, 293,\n         350, 526, 257, 550, 957, 970, 955, 575,   5, 101, 123, 945, 633, 104,\n          80, 502, 729, 508,  49, 225, 258, 426, 637, 183, 995, 789, 344, 906,\n         948, 179, 950, 398, 397, 664, 532, 903, 198, 626, 194, 236, 667, 676,\n         757, 465, 324, 335, 218, 303, 699, 496, 934, 158,  38, 441, 433, 992,\n         375, 357, 810, 815, 693, 192, 585, 107, 586, 289, 331, 735, 772, 125,\n         515, 416, 392, 482, 869, 608, 681, 607, 882, 640, 380, 126, 854, 924,\n         768, 312, 589, 837, 141, 570, 315, 653, 102, 572, 479, 369, 956, 503,\n         115, 481,  74, 797, 122, 535, 327, 352,  45, 953, 802,  48, 389,  96,\n         270, 723, 224, 378, 795, 424, 450, 461, 796, 100,  39, 252, 800, 648,\n         679, 121, 787, 453, 925, 613, 846, 726, 702, 173,   8, 221, 287, 926,\n         169, 341, 325, 186, 128, 356, 709, 455, 645, 983,   6,  67, 449, 285,\n         972,  65, 765, 564, 282, 663, 677, 118, 806,  29, 576, 151, 388, 748,\n         782, 311, 358, 963, 329, 824, 333, 533, 203, 905, 673, 830, 951, 480,\n         419, 445, 829, 127, 904, 858,  69, 371, 313, 208, 753, 509, 521, 641,\n         219, 214, 209,  56, 928, 462, 138, 149, 776, 987, 523, 206, 916, 332,\n          51,   1, 320, 262, 798, 524, 805, 514, 627, 274, 705, 334, 420, 227,\n         773, 474, 205, 696, 899,  84, 212, 111,   7, 483, 801, 109, 990, 710,\n         866, 488,  78, 791, 602, 851, 747, 971,  70,  82, 439, 848, 367, 873,\n         591, 958, 505, 464, 342, 290, 674, 659, 898, 288, 803, 967, 594, 370,\n         730, 385, 567, 779, 223, 374, 429, 649, 741, 836, 711,  50, 952, 165,\n         598, 739, 546, 487, 686, 658, 749, 715, 501, 609, 890, 954, 432,  34,\n         769, 428, 647, 973, 962, 742, 981,   4, 619, 892,  58, 841, 964,  83,\n         142, 891, 361, 885, 909, 927, 124, 777, 363, 220, 373, 144, 406, 660,\n         731, 297, 783, 630, 245, 577, 106, 583, 965, 911, 427, 556, 834, 132,\n         762, 469, 569, 738, 199, 491, 446, 348,   3, 346, 147, 902, 808, 740,\n         936, 908, 819, 372, 827, 624, 975, 687, 548, 879, 611,  79, 574, 662,\n         476, 522, 864, 745, 969, 880, 998, 680, 343, 632, 884, 877, 497, 835,\n         477, 701, 811, 573, 818, 120, 949, 565, 190, 767, 914, 650, 418, 931,\n         216, 844, 872, 666, 415, 527, 117, 604, 148, 438, 923, 279, 599, 888,\n         698, 961, 855, 941, 338, 978, 466, 538, 688, 724, 839, 792, 542, 652,\n         525, 814, 918, 708, 366, 590, 860, 862, 600, 784, 942, 966, 976, 867,\n           2, 146, 489, 413, 712, 595, 644, 828, 994,  54, 399, 629, 780, 727,\n         822, 959, 764, 935, 832, 755, 751, 417, 977, 561, 558, 545, 625, 490,\n          66, 470, 980, 974, 816, 571, 863, 506, 737, 494, 555, 907, 409, 840,\n         460, 563, 471, 634, 620, 820, 484, 530, 913, 536, 403, 718, 856, 758,\n         845, 919, 821, 562, 871, 920, 404, 408, 683, 781, 940, 510, 847, 498,\n         628, 694, 895, 554, 405, 900, 442, 754, 833, 437, 807, 682, 668, 557,\n         812, 733, 517, 540, 744, 657]])"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_1.html#exercise-use-a-pretrained-deep-learning-model-to-classify-images-4",
    "href": "notes/Adv_ML_Python_presentation_2_1.html#exercise-use-a-pretrained-deep-learning-model-to-classify-images-4",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 1)",
    "section": "Exercise: Use a pretrained deep learning model to classify images",
    "text": "Exercise: Use a pretrained deep learning model to classify images\n\nUse the list of categories to translate the predicted class index into its category.\n\n\nsorted_predicted_classes = sample_y.argsort(dim=1, descending=True)[0, :10]\nsorted_probs = torch.softmax(sample_y, dim=1)[0, sorted_predicted_classes]\n\nfor idx, prob in zip(sorted_predicted_classes, sorted_probs):\n    print(categories[idx], \"%3.2f %%\" % (prob * 100))\n\nmissile 37.93 %\nprojectile 14.39 %\ndrilling platform 12.16 %\ncrane 5.72 %\npole 1.06 %\nspace shuttle 0.93 %\nflagpole 0.93 %\nmosque 0.51 %\nobelisk 0.46 %\nsolar dish 0.45 %"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_1.html#try-with-other-sample-images",
    "href": "notes/Adv_ML_Python_presentation_2_1.html#try-with-other-sample-images",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 1)",
    "section": "Try with other sample images",
    "text": "Try with other sample images\n\n\n\n\n\n\nCaution\n\n\nOnly works with RGB images\n\n\n\n\nUse the pretrained model to classify images from the internet (maybe pictures of dogs).\n\n\nsample_im = skimage.io.imread(\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Black_Labrador_Retriever_-_Male_IMG_3323.jpg/1280px-Black_Labrador_Retriever_-_Male_IMG_3323.jpg\")\nsample_x = pipeline(sample_im)\nsample_y = dl_model(sample_x[None, ...])\n\nplt.imshow(sample_im)\nplt.title(categories[sample_y.argmax(dim=1)])\nplt.show()\n\nsorted_predicted_classes = sample_y.argsort(dim=1, descending=True)[0, :10]\nsorted_probs = torch.softmax(sample_y, dim=1)[0, sorted_predicted_classes]\n\nfor idx, prob in zip(sorted_predicted_classes, sorted_probs):\n    print(categories[idx], \"%3.2f %%\" % (prob * 100))\n\n\n\n\n\n\n\n\nLabrador retriever 87.15 %\nGerman short-haired pointer 3.31 %\nflat-coated retriever 0.92 %\nGreat Dane 0.83 %\nblack-and-tan coonhound 0.52 %\ngiant schnauzer 0.34 %\ncurly-coated retriever 0.22 %\nStaffordshire bullterrier 0.19 %\nDoberman 0.14 %\nRottweiler 0.07 %\n\n\n\nTry with an image from a category that is not in the labels set of the model (like giraffes: https://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Giraffe_Mikumi_National_Park.jpg/800px-Giraffe_Mikumi_National_Park.jpg)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_1.html#exercise-modify-the-classifier-layer-dl_model.fc-to-return-the-features-map-from-the-input-image-instead-of-the-category",
    "href": "notes/Adv_ML_Python_presentation_2_1.html#exercise-modify-the-classifier-layer-dl_model.fc-to-return-the-features-map-from-the-input-image-instead-of-the-category",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 1)",
    "section": "Exercise: Modify the classifier layer dl_model.fc to return the features map from the input image instead of the category",
    "text": "Exercise: Modify the classifier layer dl_model.fc to return the features map from the input image instead of the category\n\n\n\n\n\n\nTip\n\n\nThe classifier layer is commonly implemented as a MultiLayer Perceptron (Fully connected) at the end of the models. The specific name of that layer can vary between implementations.\n\n\n\n\nLoad the pre-trained Inception V3 model again to use it as feature extractor.\n\n\ndl_extractor = models.inception_v3(inception_weights, progress=True)\ndl_extractor.eval()\n\ndl_extractor.fc\n\nLinear(in_features=2048, out_features=1000, bias=True)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_1.html#exercise-modify-the-classifier-layer-dl_model.fc-to-return-the-features-map-from-the-input-image-instead-of-the-category-1",
    "href": "notes/Adv_ML_Python_presentation_2_1.html#exercise-modify-the-classifier-layer-dl_model.fc-to-return-the-features-map-from-the-input-image-instead-of-the-category-1",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 1)",
    "section": "Exercise: Modify the classifier layer dl_model.fc to return the features map from the input image instead of the category",
    "text": "Exercise: Modify the classifier layer dl_model.fc to return the features map from the input image instead of the category\n\nReplace the .fc layer with a torch.nn.Identity module.\n\n\ndl_extractor.fc = torch.nn.Identity()\n\n\nUse the model for feature extraction in the same way it is used for image classification.\n\n\nsample_fx = dl_extractor(sample_x[None, ...])\n\nsample_fx.shape\n\ntorch.Size([1, 2048])"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#select-runtime-and-connect",
    "href": "notes/Adv_ML_Python_presentation_1.html#select-runtime-and-connect",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Select runtime and connect",
    "text": "Select runtime and connect\nOn the top right corner of the page, click the drop-down arrow to the right of the Connect button and select Change runtime type."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#machine-learning-ml",
    "href": "notes/Adv_ML_Python_presentation_1.html#machine-learning-ml",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Machine Learning (ML)",
    "text": "Machine Learning (ML)\nSub-field of Artificial Intelligence that develops methods to address tasks that require human intelligence"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#artificial-intelligence-tasks",
    "href": "notes/Adv_ML_Python_presentation_1.html#artificial-intelligence-tasks",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Artificial intelligence tasks",
    "text": "Artificial intelligence tasks"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#common-tasks",
    "href": "notes/Adv_ML_Python_presentation_1.html#common-tasks",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Common tasks",
    "text": "Common tasks\n\n\nClassification\n\nwhat is this?\n\nDetection\n\nwhere is something?\n\nSegmentation\n\nwhere specifically is something?"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#types-of-machine-learning-1",
    "href": "notes/Adv_ML_Python_presentation_1.html#types-of-machine-learning-1",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Types of machine learning",
    "text": "Types of machine learning\nDepending on how the model is trained\n\nSupervised\nUnsupervised\nWeakly supervised\nReinforced\nâ€¦\n\n\nSupervised learning: teach the machine to perform a task with a set of inputs and their respective expected outcome (\\(X\\), \\(Y\\)).\nUnsupervised learning: let the machine learn to perform a task on its own (\\(X\\)) without any specific expected outcome.\nWeakly supervised: teach the machine to perform a task using a limited set of expected outcomes.\nReinforced learning: let the machine learn to perform a task on its own, then give it a reward relative to its performance."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#inputs-and-outputs-1",
    "href": "notes/Adv_ML_Python_presentation_1.html#inputs-and-outputs-1",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Inputs and outputs",
    "text": "Inputs and outputs\nFor a task, we want to model the outcome/output (\\(y\\)) obtained by a given input (\\(x\\))\n\\(f(x) \\approx y\\)\n\n\n\n\n\n\nNote\n\n\nThe complete set of (\\(x\\), \\(y\\)) pairs is known as dataset (\\(X\\), \\(Y\\)).\n\n\n\n\n\n\n\n\n\nNote\n\n\nInputs can be virtually anything, including images, texts, video, audio, electrical signals, etc.\nWhile outputs are expected to be some meaningful piece of information, such as a category, position, value, etc."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#use-case-image-classification-with-the-cifar-100-dataset",
    "href": "notes/Adv_ML_Python_presentation_1.html#use-case-image-classification-with-the-cifar-100-dataset",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Use case: Image classification with the CIFAR-100 dataset",
    "text": "Use case: Image classification with the CIFAR-100 dataset\n\nLoad the CIFAR-100 dataset from torchvision.datasets\n\n\nimport torch\nimport torchvision\n\ncifar_ds = torchvision.datasets.CIFAR100(root=\"/tmp\", train=True, download=True)\n\nFiles already downloaded and verified\n\n\n\nExplore the CIFAR-100 dataset\n\n\nx_im, y = cifar_ds[0]\n\nlen(cifar_ds), type(x_im), type(y)\n\n(50000, PIL.Image.Image, int)\n\n\n\n\ny = 19 (cattle)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#what-is-a-tensor-pytorch",
    "href": "notes/Adv_ML_Python_presentation_1.html#what-is-a-tensor-pytorch",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "What is a tensor (PyTorch)?",
    "text": "What is a tensor (PyTorch)?\nA tensor is a multi-dimensional array. In PyTorch, this comes from a generalization of the notation of variables that exists on more than two dimensions.\n\nzero-dimensional variables are points,\none-dimensional variables are vectors,\ntwo-dimensional variables are matrices,\nand three or more dimensional variables, are tensors.\n\n\nimport torch\n\nx0 = torch.Tensor([7]) # This is a point\n\nx1 = torch.Tensor([15, 64, 123]) # This is a vector\n\nx2 = torch.Tensor([[3, 6, 5],\n                   [7, 9, 12],\n                   [10, 33, 1]]) # This is a matrix\n\nx3 = torch.Tensor([[[[1, 0, 0],\n                     [0, 1, 0],\n                     [0, 0, 1]],\n                    [[2, 0, 1],\n                     [0, 2, 3],\n                     [4, 1, 5]]]]) # This is a tensor"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-add-the-preprocessing-pipeline-to-the-cifar-100-dataset",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-add-the-preprocessing-pipeline-to-the-cifar-100-dataset",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Add the preprocessing pipeline to the CIFAR-100 dataset",
    "text": "Exercise: Add the preprocessing pipeline to the CIFAR-100 dataset\n\nRe-load the CIFAR-100 dataset, this time passing the pre_process function as argument.\n\n\ncifar_ds = torchvision.datasets.CIFAR100(root=\"/tmp\", train=True, download=True, transform=pre_process)\n\nFiles already downloaded and verified"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#training-set",
    "href": "notes/Adv_ML_Python_presentation_1.html#training-set",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Training set",
    "text": "Training set\nThe examples (\\(x\\), \\(y\\)) used to teach a machine/model to perform a task"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#validation-set",
    "href": "notes/Adv_ML_Python_presentation_1.html#validation-set",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Validation set",
    "text": "Validation set\nUsed to measure the performance of a model during training\nThis subset is not used for training the model, so it is unseen data.\n\nThis is a subset from the training set and can be used to test the generalization capacity of the model or to select the best configuration of a model."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#test-set",
    "href": "notes/Adv_ML_Python_presentation_1.html#test-set",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Test set",
    "text": "Test set\nThis set of samples is not used when training\nIts purpose is to measure the generalization capacity of the model"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-load-the-test-set-and-split-the-train-set-into-train-and-validation-subsets",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-load-the-test-set-and-split-the-train-set-into-train-and-validation-subsets",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Load the test set and split the train set into train and validation subsets",
    "text": "Exercise: Load the test set and split the train set into train and validation subsets\n\nLoad the CIFAR-100 test set\n\n\ncifar_test_ds = torchvision.datasets.CIFAR100(root=\"/tmp\", train=False, download=True, transform=pre_process)\n\nFiles already downloaded and verified\n\n\n\nSplit the training set into train and validation subsets\n\n\nfrom torch.utils.data import random_split\n\ncifar_train_ds, cifar_val_ds = random_split(cifar_ds, (40_000, 10_000))"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#deep-learning-dl-models-1",
    "href": "notes/Adv_ML_Python_presentation_1.html#deep-learning-dl-models-1",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Deep Learning (DL) models",
    "text": "Deep Learning (DL) models\nModels that construct knowledge in a hierarchical manner are considered deep models.\n\n\n\nFrom Cervantes-Sanchez et al."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-create-a-logisic-regression-model-with-pytorch",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-create-a-logisic-regression-model-with-pytorch",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Create a Logisic Regression model with PyTorch",
    "text": "Exercise: Create a Logisic Regression model with PyTorch\n\nUse the nn (Neural Networks) module from pytorch to create a Logistic Regression model\n\n\nimport torch.nn as nn\n\nlr_clf_1 = nn.Linear(in_features=3 * 32 * 32, out_features=100, bias=True)\nlr_clf_2 = nn.Softmax()\n\n\nFeed the model with a sample x\n\n\n\n\n\n\n\nImportant\n\n\nWe have to reshape x before feeding it to the model because x is an image with axes: Channels, Height, Width (CHW), but the Logistic Regression input should be a vector.\n\n\n\n\ny_hat = lr_clf_2( lr_clf_1( x.reshape(1, -1) ))\n\ntype(y_hat), y_hat.shape, y_hat.dtype\n\n(torch.Tensor, torch.Size([1, 100]), torch.float32)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-create-a-multilayer-perceptron-mlp-model-with-pytorch",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-create-a-multilayer-perceptron-mlp-model-with-pytorch",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Create a MultiLayer Perceptron (MLP) model with PyTorch",
    "text": "Exercise: Create a MultiLayer Perceptron (MLP) model with PyTorch\n\nUse the nn.Sequential module to build sequential models\n\n\nmlp_clf = nn.Sequential(\n  nn.Linear(in_features=3 * 32 * 32, out_features=1024, bias=True),\n  nn.Tanh(),\n  nn.Linear(in_features=1024, out_features=100, bias=True),\n  nn.Softmax()\n)\n\n\nFeed the model with a sample x\n\n\ny_hat = mlp_clf(x.reshape(1, -1))\n\ntype(y_hat), y_hat.shape, y_hat.dtype\n\n(torch.Tensor, torch.Size([1, 100]), torch.float32)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#model-fittingtraining",
    "href": "notes/Adv_ML_Python_presentation_1.html#model-fittingtraining",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Model fitting/training",
    "text": "Model fitting/training\nModels behavior depends directly on the value of their set of parameters \\(\\theta\\).\n\n\\(f(x) \\approx y\\)\n\\(f_\\theta(x) = y + \\epsilon = \\hat{y}\\)\n\n\n\n\n\n\n\nNote\n\n\nAs models increase their number of parameters, they become more complex\n\n\n\nTraining is the process of optimizing the values of \\(\\theta\\)\n\nTraining is often an expensive process in terms of computational resources and time."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#loss-function-1",
    "href": "notes/Adv_ML_Python_presentation_1.html#loss-function-1",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Loss function",
    "text": "Loss function\nThis is measure of the difference between the expected outputs and the predictions made by a model \\(L(Y, \\hat{Y})\\).\n\n\n\n\n\n\nNote\n\n\nWe look for smooth loss functions for which we can compute their gradient"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#loss-function-for-regression",
    "href": "notes/Adv_ML_Python_presentation_1.html#loss-function-for-regression",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "11.1 Loss function for regression",
    "text": "11.1 Loss function for regression\nIn the case of regression tasks we generally use the Mean Squared Error (MSE).\n\\(MSE=\\frac{1}{N}\\sum \\left(Y - \\hat{Y}\\right)^2\\)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#loss-function-for-classification",
    "href": "notes/Adv_ML_Python_presentation_1.html#loss-function-for-classification",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Loss function for classification",
    "text": "Loss function for classification\nAnd for classification tasks we use the Cross Entropy (CE) function.\n\\(CE = -\\frac{1}{N}\\sum\\limits_i^N\\sum\\limits_k^C y_{i,k} log(\\hat{y_{i,k}})\\)\nwhere \\(C\\) is the number of classes.\n\n\n\n\n\n\nNote\n\n\nFor the binary classification case:\n\\(BCE = -\\frac{1}{N}\\sum\\limits_i^N \\left(y_i log(\\hat{y_i}) + (1 - y_i) log(1 - \\hat{y_i})\\right)\\)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-define-the-loss-function-for-the-cifar-100-classification-problem",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-define-the-loss-function-for-the-cifar-100-classification-problem",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Define the loss function for the CIFAR-100 classification problem",
    "text": "Exercise: Define the loss function for the CIFAR-100 classification problem\n\nDefine a Cross Entropy loss function with nn.CrossEntropyLoss\n\n\nloss_fun = nn.CrossEntropyLoss()\n\n\nRemove the nn.Softmax layer from the MLP model.\n\n\n\n\n\n\n\nNote\n\n\nAccording to the PyTorch documentation, the CrossEntropyLoss function takes as inputs the logits of the probabilities and not the probabilities themselves. So, we donâ€™t need to squash the output of the MLP model.\n\n\n\n\nmlp_clf = nn.Sequential(\n  nn.Linear(in_features=3 * 32 * 32, out_features=1024, bias=True),\n  nn.Tanh(),\n  nn.Linear(in_features=1024, out_features=100, bias=True),\n  # nn.Softmax() # &lt;- remove this line\n)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-define-the-loss-function-for-the-cifar-100-classification-problem-1",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-define-the-loss-function-for-the-cifar-100-classification-problem-1",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Define the loss function for the CIFAR-100 classification problem",
    "text": "Exercise: Define the loss function for the CIFAR-100 classification problem\n\nMeasure the prediction loss (error) of our MLP with respect to the grund-truth\n\n\n\n\n\n\n\nImportant\n\n\nWe are using a PyTorch loss function, and it expects PyTorchâ€™s tensors as arguments, so we have to convert y to tensor before computing the loss function.\n\n\n\n\nloss = loss_fun(y_hat, torch.LongTensor([y]))\n\nloss\n\ntensor(4.6085, grad_fn=&lt;NllLossBackward0&gt;)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#gradient-based-optimization-1",
    "href": "notes/Adv_ML_Python_presentation_1.html#gradient-based-optimization-1",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Gradient based optimization",
    "text": "Gradient based optimization\nGradient-based methods are able to fit large numbers of parameters when using a smooth Loss function as target.\n\n\n\n\n\n\nNote\n\n\nWe compute the gradient of the loss function with respect to the model parameters using the chain rule from calculous. Generally, this is managed by the machine learning packages such as PyTorch and Tensorflow with a method called back propagation.\n\n\n\nGradient Descent\n\n\\(\\theta^{t+1} = \\theta^t - \\eta \\nabla_\\theta L(Y, \\hat{Y})\\)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-compute-the-gradient-of-the-loss-function-with-respect-to-the-parameters-of-the-mlp.",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-compute-the-gradient-of-the-loss-function-with-respect-to-the-parameters-of-the-mlp.",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Compute the gradient of the loss function with respect to the parameters of the MLP.",
    "text": "Exercise: Compute the gradient of the loss function with respect to the parameters of the MLP.\n\nCheck what are the gradients of the MLP parameters before back propagating the gradient.\n\n\nmlp_clf[0].bias.grad\n\n\nCompute the gradient of the loss function with respect to the MLP parameters.\n\n\n\n\n\n\n\nNote\n\n\nTo back propagate the gradients we use the loss.backward() method of the loss function.\n\n\n\n\nloss = loss_fun(y_hat, torch.LongTensor([y]))\n\nloss.backward()\n\n\nVerify that the gradients have been propagated to the model parameters.\n\n\nmlp_clf[0].bias.grad"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#stochastic-methods",
    "href": "notes/Adv_ML_Python_presentation_1.html#stochastic-methods",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Stochastic methods",
    "text": "Stochastic methods\n\n\n\n\n\n\nCaution\n\n\nThe Gradient descent method require to obtain the Loss function for the whole training set before doing a single update.\nThis can be inefficient when large volumes of data are used for training the model.\n\n\n\n\nThese methods use a relative small sample from the training data called mini-batch at a time.\nThis reduces the amount of memory used for computing intermediate operations carried out during optimization process."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#stochastic-gradient-descent-sgd",
    "href": "notes/Adv_ML_Python_presentation_1.html#stochastic-gradient-descent-sgd",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Stochastic Gradient Descent (SGD)",
    "text": "Stochastic Gradient Descent (SGD)\n\nThis strategy defines \\(\\theta\\)â€™sâ€™ update rule for iteration \\(t+1\\) using a mini-batch sampled at random from the training set as follows.\n\n\n\\(\\theta^{t+1} = \\theta^t - \\eta \\nabla_\\theta L(Y_{b}, \\hat{Y_{b}})\\)\n\\(\\eta\\) controls the update we perform on the current parameterâ€™s values\n\n\n\n\n\n\n\nNote\n\n\nThis parameter in Deep Learning is known as the learning rate"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#training-with-mini-batches",
    "href": "notes/Adv_ML_Python_presentation_1.html#training-with-mini-batches",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Training with mini-batches",
    "text": "Training with mini-batches\n\n\n\n\n\n\nNote\n\n\nPyTorch can operate efficiently on multiple inputs at the same time. To do that, we can use a DataLoader to serve mini-batches of inputs."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-train-the-mlp-classifier",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-train-the-mlp-classifier",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Train the MLP classifier",
    "text": "Exercise: Train the MLP classifier\n\nUse a DataLoader to serve mini-batches of images to train our MLP.\n\n\nfrom torch.utils.data import DataLoader\n\ncifar_train_dl = DataLoader(cifar_train_ds, batch_size=128, shuffle=True)\ncifar_val_dl = DataLoader(cifar_val_ds, batch_size=256)\ncifar_test_dl = DataLoader(cifar_test_ds, batch_size=256)\n\n\nCreate a Stochastic Gradient Descent optimizer for our MLP classifier.\n\n\nimport torch.optim as optim\n\noptimizer = optim.SGD(mlp_clf.parameters(), lr=0.01, )"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-train-the-mlp-classifier-1",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-train-the-mlp-classifier-1",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Train the MLP classifier",
    "text": "Exercise: Train the MLP classifier\n\nImplement the training-loop to fit the parameters of our MLP classifier.\n\n\n\n\n\n\n\nNote\n\n\nGradients are accumulated on every iteration, so we need to reset the accumulator with optimizer.zero_grad() for every new batch.\n\n\n\n\n\n\n\n\n\nNote\n\n\nTo perform get the new iterationâ€™s parameter values \\(\\theta^{t+1}\\) we use optimizer.step() to compute the update step.\n\n\n\nmlp_clf.train()\nfor x, y in cifar_train_dl:\n  optimizer.zero_grad()\n\n  y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) ) # Reshape it into a batch of vectors\n\n  loss = loss_fun(y_hat, y)\n\n  loss.backward()\n\n  optimizer.step()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-train-the-mlp-classifier-and-track-the-training-and-validation-loss",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-train-the-mlp-classifier-and-track-the-training-and-validation-loss",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Train the MLP classifier and track the training and validation loss",
    "text": "Exercise: Train the MLP classifier and track the training and validation loss\n\nSave the loss function of each batch and the overall average loss during training.\n\n\n\n\n\n\n\nNote\n\n\nTo extract the loss functionâ€™s value without anything else attached use loss.item().\n\n\n\ntrain_loss = []\ntrain_loss_avg = 0\ntotal_train_samples = 0\n\nmlp_clf.train()\nfor x, y in cifar_train_dl:\n  optimizer.zero_grad()\n\n  y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) ) # Reshape it into a batch of vectors\n\n  loss = loss_fun(y_hat, y)\n\n  train_loss.append(loss.item())\n  train_loss_avg += loss.item() * len(x)\n  total_train_samples += len(x)\n\n  loss.backward()\n\n  optimizer.step()\n\ntrain_loss_avg /= total_train_samples"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-train-the-mlp-classifier-and-track-the-training-and-validation-loss-1",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-train-the-mlp-classifier-and-track-the-training-and-validation-loss-1",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Train the MLP classifier and track the training and validation loss",
    "text": "Exercise: Train the MLP classifier and track the training and validation loss\n\nCompute the average loss function for the validation set.\n\n\n\n\n\n\n\nNote\n\n\nBecause we donâ€™t train the model with the validation set, back-propagation and optimization steps are not needed.\nAdditionally, we wrap the loop with torch.no_grad() to prevent the generation of gradients that could fill the memory unnecessarily.\n\n\n\nval_loss_avg = 0\ntotal_val_samples = 0\n\nmlp_clf.eval()\nwith torch.no_grad():\n  for x, y in cifar_val_dl:\n    y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) ) # Reshape it into a batch of vectors\n    loss = loss_fun(y_hat, y)\n\n    val_loss_avg += loss.item() * len(x)\n    total_val_samples += len(x)\n\nval_loss_avg /= total_val_samples"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-train-the-mlp-classifier-and-track-the-training-and-validation-loss-2",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-train-the-mlp-classifier-and-track-the-training-and-validation-loss-2",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Train the MLP classifier and track the training and validation loss",
    "text": "Exercise: Train the MLP classifier and track the training and validation loss\n\nPlot the training loss for this epoch.\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot(train_loss, \"b-\", label=\"Training loss\")\nplt.plot([0, len(train_loss)], [train_loss_avg, train_loss_avg], \"r:\", label=\"Average training loss\")\nplt.plot([0, len(train_loss)], [val_loss_avg, val_loss_avg], \"b:\", label=\"Average validation loss\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-train-the-mlp-classifier-and-track-the-training-and-validation-loss-through-several-epochs",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-train-the-mlp-classifier-and-track-the-training-and-validation-loss-through-several-epochs",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Train the MLP classifier and track the training and validation loss through several epochs",
    "text": "Exercise: Train the MLP classifier and track the training and validation loss through several epochs\nnum_epochs = 10\ntrain_loss = []\nval_loss = []\n\nfor e in range(num_epochs):\n  train_loss_avg = 0\n  total_train_samples = 0\n\n  mlp_clf.train()\n  for x, y in cifar_train_dl:\n    optimizer.zero_grad()\n\n    y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) ) # Reshape it into a batch of vectors\n\n    loss = loss_fun(y_hat, y)\n\n    train_loss_avg += loss.item() * len(x)\n    total_train_samples += len(x)\n\n    loss.backward()\n\n    optimizer.step()\n\n  train_loss_avg /= total_train_samples\n  train_loss.append(train_loss_avg)\n\n  val_loss_avg = 0\n  total_val_samples = 0\n\n  mlp_clf.eval()\n  with torch.no_grad():\n    for x, y in cifar_val_dl:\n      y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) ) # Reshape it into a batch of vectors\n      loss = loss_fun(y_hat, y)\n\n      val_loss_avg += loss.item() * len(x)\n      total_val_samples += len(x)\n\n  val_loss_avg /= total_val_samples\n  val_loss.append(val_loss_avg)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-show-the-progress-of-the-training-throughout-the-epochs",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-show-the-progress-of-the-training-throughout-the-epochs",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Show the progress of the training throughout the epochs",
    "text": "Exercise: Show the progress of the training throughout the epochs\n\nPlot the average train and validation losses\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot(train_loss, \"b-\", label=\"Average training loss\")\nplt.plot(val_loss, \"r-\", label=\"Average validation loss\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#performance-metrics-1",
    "href": "notes/Adv_ML_Python_presentation_1.html#performance-metrics-1",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Performance metrics",
    "text": "Performance metrics\nUsed to measure how good or bad a model carries out a task\n\n\\(f(x) \\approx y\\)\n\\(f(x) = y + \\epsilon = \\hat{y}\\)\n\n\nGiven the set of parameters, as well as other factors, the output of a model can deviate from the expected outcome. So, the actual output of a model is \\(f(x) = \\hat{y}\\).\n\n\n\n\n\n\n\nNote\n\n\nThe output \\(\\hat{y}\\) is called prediction  given the context taken from statistical regression analysis.\n\n\n\n\n\n\n\n\n\nImportant\n\n\nSelecting the correct performance metrics depends on the training type, task, and even the distribution of the data."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-measure-the-accuracy-of-the-mlp-trained-to-classify-images-from-cifar-100",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-measure-the-accuracy-of-the-mlp-trained-to-classify-images-from-cifar-100",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Measure the accuracy of the MLP trained to classify images from CIFAR-100",
    "text": "Exercise: Measure the accuracy of the MLP trained to classify images from CIFAR-100\n\nInstall the torchmetrics package.\n\n!pip install torchmetrics\n\nCompute the average accuracy for the Train set.\n\n\nfrom torchmetrics.classification import Accuracy\n\nmlp_clf.eval()\n\ntrain_acc_metric = Accuracy(task=\"multiclass\", num_classes=100)\n\nwith torch.no_grad():\n  for x, y in cifar_train_dl:\n    y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) )\n    train_acc_metric(y_hat.softmax(dim=1), y)\n\n  train_acc = train_acc_metric.compute()\n\nprint(f\"Training acc={train_acc}\")\ntrain_acc_metric.reset()\n\nTraining acc=0.12927499413490295"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-measure-the-accuracy-of-the-mlp-trained-to-classify-images-from-cifar-100-1",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-measure-the-accuracy-of-the-mlp-trained-to-classify-images-from-cifar-100-1",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Measure the accuracy of the MLP trained to classify images from CIFAR-100",
    "text": "Exercise: Measure the accuracy of the MLP trained to classify images from CIFAR-100\n\nCompute the average accuracy for the Validation and Test sets.\n\n\nval_acc_metric = Accuracy(task=\"multiclass\", num_classes=100)\ntest_acc_metric = Accuracy(task=\"multiclass\", num_classes=100)\n\nwith torch.no_grad():\n  for x, y in cifar_val_dl:\n    y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) )\n    val_acc_metric(y_hat.softmax(dim=1), y)\n\n  val_acc = val_acc_metric.compute()\n\n  for x, y in cifar_test_dl:\n    y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) )\n    test_acc_metric(y_hat.softmax(dim=1), y)\n\n  test_acc = test_acc_metric.compute()\n\nprint(f\"Validation acc={val_acc}\")\nprint(f\"Test acc={test_acc}\")\n\nval_acc_metric.reset()\ntest_acc_metric.reset()\n\nValidation acc=0.125\nTest acc=0.12290000170469284"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#convolution-layers",
    "href": "notes/Adv_ML_Python_presentation_1.html#convolution-layers",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Convolution layers",
    "text": "Convolution layers\nThe most common operation in DL models for image processing are Convolution operations.\n\n2D ConvolutionThe animation shows the convolution of a 7x7 pixels input image (bottom) with a 3x3 pixels kernel (moving window), that results in a 5x5 pixels output (top)."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-visualize-the-effect-of-the-convolution-operation",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-visualize-the-effect-of-the-convolution-operation",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Visualize the effect of the convolution operation",
    "text": "Exercise: Visualize the effect of the convolution operation\n\nCreate a convolution layer with nn.Conv2D using 3 channels as input, and a single one for output.\n\n\nconv_1 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=7, padding=0, bias=True)\n\nx, _ = next(iter(cifar_train_dl))\n\nfx = conv_1(x)\n\ntype(fx), fx.dtype, fx.shape, fx.min(), fx.max()\n\n(torch.Tensor,\n torch.float32,\n torch.Size([128, 1, 26, 26]),\n tensor(-0.1479, grad_fn=&lt;MinBackward1&gt;),\n tensor(1.0583, grad_fn=&lt;MaxBackward1&gt;))\n\n\n\n\n\n\n\n\nWarning\n\n\nThe convolution layer is initialized with random values, so the results will vary."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-visualize-the-effect-of-the-convolution-operation-1",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-visualize-the-effect-of-the-convolution-operation-1",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Visualize the effect of the convolution operation",
    "text": "Exercise: Visualize the effect of the convolution operation\n\nCreate a convolution layer with nn.Conv2D using 3 channels as input, and a single one for output.\n\n\nplt.rcParams['figure.figsize'] = [5, 5]\n\nfig, ax = plt.subplots(1, 2)\nax[0].imshow(x[0].permute(1, 2, 0))\nax[1].imshow(fx.detach()[0, 0], cmap=\"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nBy default, outputs from PyTorch modules are tracked for back-propagation.\nTo visualize it with matplotlib we have to .detach() the tensor first."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-visualize-the-effect-of-the-convolution-operation-2",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-visualize-the-effect-of-the-convolution-operation-2",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Visualize the effect of the convolution operation",
    "text": "Exercise: Visualize the effect of the convolution operation\n\nVisualize the weights of the convolution layer.\n\n\nconv_1.weight.shape\n\ntorch.Size([1, 3, 7, 7])\n\n\n\nfig, ax = plt.subplots(2, 2)\nax[0, 0].imshow(conv_1.weight.detach()[0, 0], cmap=\"gray\")\nax[0, 1].imshow(conv_1.weight.detach()[0, 1], cmap=\"gray\")\nax[1, 0].imshow(conv_1.weight.detach()[0, 2], cmap=\"gray\")\nax[1, 1].set_axis_off()\nplt.show()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-visualize-the-effect-of-the-convolution-operation-3",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-visualize-the-effect-of-the-convolution-operation-3",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Visualize the effect of the convolution operation",
    "text": "Exercise: Visualize the effect of the convolution operation\n\nModify the weights of the convolution layer.\n\n\nconv_1 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, padding=0, bias=False)\n\nconv_1.weight.data[:] = torch.FloatTensor([\n  [\n    [\n      [0, 0, 0],\n      [0, 0, 0],\n      [0, 0, 0],\n    ],\n    [\n      [0, 0, 0],\n      [0, 1, 0],\n      [0, 0, 0],\n    ],\n    [\n      [0, 0, 0],\n      [0, 0, 0],\n      [0, 0, 0],\n    ],\n  ]\n])"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-visualize-the-effect-of-the-convolution-operation-4",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-visualize-the-effect-of-the-convolution-operation-4",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Visualize the effect of the convolution operation",
    "text": "Exercise: Visualize the effect of the convolution operation\n\nVisualize the effects after changing the values.\n\n\nfx = conv_1(x)\n\nfig, ax = plt.subplots(1, 2)\nax[0].imshow(x[0].permute(1, 2, 0))\nax[1].imshow(fx.detach()[0].permute(1, 2, 0))\nplt.show()\n\n\n\n\n\n\n\n\nExperiment with different values and shapes of the kernel https://en.wikipedia.org/wiki/Kernel_(image_processing)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-visualize-the-effect-of-the-convolution-operation-5",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-visualize-the-effect-of-the-convolution-operation-5",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Visualize the effect of the convolution operation",
    "text": "Exercise: Visualize the effect of the convolution operation\n\nModify the weights of the convolution layer.\n\n\nconv_1 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, padding=0, bias=False)\n\nconv_1.weight.data[:] = torch.FloatTensor([\n  [[[0, -1, 0], [-1, 5, -1], [0, -1, 0]],\n   [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n   [[0, 0, 0], [0, 0, 0], [0, 0, 0]]]\n])\n\nfx = conv_1(x)\n\nfig, ax = plt.subplots(1, 2)\nax[0].imshow(x[0].permute(1, 2, 0))\nax[1].imshow(fx.detach()[0, 0], cmap=\"gray\")\nplt.show()\n\n\n\n\n\n\n\n\nExperiment with different values and shapes of the kernel https://en.wikipedia.org/wiki/Kernel_(image_processing)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-visualize-the-effect-of-the-convolution-operation-6",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-visualize-the-effect-of-the-convolution-operation-6",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Visualize the effect of the convolution operation",
    "text": "Exercise: Visualize the effect of the convolution operation\n\nModify the weights of the convolution layer.\n\n\nconv_1 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, padding=0, bias=False)\n\nconv_1.weight.data[:] = torch.FloatTensor([\n  [[[1, 0, -1], [1, 0, -1], [1, 0, -1]],\n   [[1, 0, -1], [1, 0, -1], [1, 0, -1]],\n   [[1, 0, -1], [1, 0, -1], [1, 0, -1]]]\n])\n\nfx = conv_1(x)\n\nfig, ax = plt.subplots(1, 2)\nax[0].imshow(x[0].permute(1, 2, 0))\nax[1].imshow(fx.detach()[0, 0], cmap=\"gray\")\nplt.show()\n\n\n\n\n\n\n\n\nExperiment with different values and shapes of the kernel https://en.wikipedia.org/wiki/Kernel_(image_processing)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#examples-of-popular-deep-learning-models-in-computer-vision",
    "href": "notes/Adv_ML_Python_presentation_1.html#examples-of-popular-deep-learning-models-in-computer-vision",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Examples of popular Deep Learning models in computer vision",
    "text": "Examples of popular Deep Learning models in computer vision\n\nInception v3 for image classification\n\n\nInceptionV3"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#examples-of-popular-deep-learning-models-in-computer-vision-1",
    "href": "notes/Adv_ML_Python_presentation_1.html#examples-of-popular-deep-learning-models-in-computer-vision-1",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Examples of popular Deep Learning models in computer vision",
    "text": "Examples of popular Deep Learning models in computer vision\n\nU-Net for cell segmentation\n\n\nU-Net"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#examples-of-popular-deep-learning-models-in-computer-vision-2",
    "href": "notes/Adv_ML_Python_presentation_1.html#examples-of-popular-deep-learning-models-in-computer-vision-2",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Examples of popular Deep Learning models in computer vision",
    "text": "Examples of popular Deep Learning models in computer vision\n\nLeNet-5 for handwritten digits classification (LeCun et al.)\n\n By Daniel Voigt Godoy - https://github.com/dvgodoy/dl-visuals/, CC BY 4.0, Link"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-implement-and-train-the-letnet-5-model-with-pytorch",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-implement-and-train-the-letnet-5-model-with-pytorch",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Implement and train the LetNet-5 model with PyTorch",
    "text": "Exercise: Implement and train the LetNet-5 model with PyTorch\n\nBuild the convolutional neural network using nn.Sequential, and the nn.ReLU() activation function.\n\n\nlenet_clf = nn.Sequential(\n    nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, bias=True),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2),\n    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, bias=True),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2),\n    nn.Flatten(),\n    nn.Linear(in_features=16*5*5, out_features=120, bias=True),\n    nn.ReLU(),\n    nn.Linear(in_features=120, out_features=84, bias=True),\n    nn.ReLU(),\n    nn.Linear(in_features=84, out_features=100, bias=True),\n)\n\n\n\n\n\n\n\nNote\n\n\nPooling layers are used to downsample feature maps to summarize information from large regions."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-implement-and-train-the-letnet-5-model-with-pytorch-1",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-implement-and-train-the-letnet-5-model-with-pytorch-1",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Implement and train the LetNet-5 model with PyTorch",
    "text": "Exercise: Implement and train the LetNet-5 model with PyTorch\n\nTest our implementation.\n\n\ny_hat = lenet_clf(x)\n\ntype(y_hat), y_hat.dtype, y_hat.shape, y_hat.min(), y_hat.max()\n\n(torch.Tensor,\n torch.float32,\n torch.Size([128, 100]),\n tensor(-0.1779, grad_fn=&lt;MinBackward1&gt;),\n tensor(0.1641, grad_fn=&lt;MaxBackward1&gt;))"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-implement-and-train-the-letnet-5-model-with-pytorch-2",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-implement-and-train-the-letnet-5-model-with-pytorch-2",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Implement and train the LetNet-5 model with PyTorch",
    "text": "Exercise: Implement and train the LetNet-5 model with PyTorch\n\nTrain the model to classify images from CIFAR-100.\n\n\nnum_epochs = 10\ntrain_loss = []\nval_loss = []\n\nif torch.cuda.is_available():\n  lenet_clf.cuda()\n\noptimizer = optim.SGD(lenet_clf.parameters(), lr=0.01)\n\nfor e in range(num_epochs):\n  train_loss_avg = 0\n  total_train_samples = 0\n\n  lenet_clf.train()\n  for x, y in cifar_train_dl:\n    optimizer.zero_grad()\n\n    if torch.cuda.is_available():\n      x = x.cuda()\n    \n    y_hat = lenet_clf( x ).cpu()\n\n    loss = loss_fun(y_hat, y)\n\n    train_loss_avg += loss.item() * len(x)\n    total_train_samples += len(x)\n\n    loss.backward()\n\n    optimizer.step()\n\n  train_loss_avg /= total_train_samples\n  train_loss.append(train_loss_avg)\n\n  val_loss_avg = 0\n  total_val_samples = 0\n\n  lenet_clf.eval()\n  with torch.no_grad():\n    for x, y in cifar_val_dl:\n      if torch.cuda.is_available():\n        x = x.cuda()\n      \n      y_hat = lenet_clf( x ).cpu()\n      loss = loss_fun(y_hat, y)\n\n      val_loss_avg += loss.item() * len(x)\n      total_val_samples += len(x)\n\n  val_loss_avg /= total_val_samples\n  val_loss.append(val_loss_avg)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-implement-and-train-the-letnet-5-model-with-pytorch-3",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-implement-and-train-the-letnet-5-model-with-pytorch-3",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Implement and train the LetNet-5 model with PyTorch",
    "text": "Exercise: Implement and train the LetNet-5 model with PyTorch\n\nPlot the average train and validation losses\n\n\nplt.plot(train_loss, \"b-\", label=\"Average training loss\")\nplt.plot(val_loss, \"r-\", label=\"Average validation loss\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1.html#exercise-implement-and-train-the-letnet-5-model-with-pytorch-4",
    "href": "notes/Adv_ML_Python_presentation_1.html#exercise-implement-and-train-the-letnet-5-model-with-pytorch-4",
    "title": "Advanced Machine Learning with Python (Session 1)",
    "section": "Exercise: Implement and train the LetNet-5 model with PyTorch",
    "text": "Exercise: Implement and train the LetNet-5 model with PyTorch\n\nCompute the average accuracy for the Validation and Test sets.\n\n\nlenet_clf.eval()\n\nval_acc_metric = Accuracy(task=\"multiclass\", num_classes=100)\ntest_acc_metric = Accuracy(task=\"multiclass\", num_classes=100)\ntrain_acc_metric = Accuracy(task=\"multiclass\", num_classes=100)\n\nwith torch.no_grad():\n  for x, y in cifar_train_dl:\n    if torch.cuda.is_available():\n      x = x.cuda()\n    y_hat = lenet_clf( x ).cpu()\n    train_acc_metric(y_hat.softmax(dim=1), y)\n\n  train_acc = train_acc_metric.compute()\n\n  for x, y in cifar_val_dl:\n    if torch.cuda.is_available():\n      x = x.cuda()\n    y_hat = lenet_clf( x ).cpu()\n    val_acc_metric(y_hat.softmax(dim=1), y)\n\n  val_acc = val_acc_metric.compute()\n\n  for x, y in cifar_test_dl:\n    if torch.cuda.is_available():\n      x = x.cuda()\n    y_hat = lenet_clf( x ).cpu()\n    test_acc_metric(y_hat.softmax(dim=1), y)\n\n  test_acc = test_acc_metric.compute()\n\nprint(f\"Training acc={train_acc}\")\nprint(f\"Validation acc={val_acc}\")\nprint(f\"Test acc={test_acc}\")\n\ntrain_acc_metric.reset()\nval_acc_metric.reset()\ntest_acc_metric.reset()\n\nTraining acc=0.02437499910593033\nValidation acc=0.020899999886751175\nTest acc=0.02250000089406967"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Machine Learning with Python",
    "section": "",
    "text": "Welcome to Advanced Machine Learning with Python!\nMy name is Fernando Cervantes, a Systems Analyst II in the Imaging Applications Group of the Research IT department of The Jackson Laboratory.\nYou can ask me questions about this material at fernando.cervantes@jax.org"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-1",
    "href": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-1",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 2)",
    "section": "Tissue classification with the MoNuSAC dataset",
    "text": "Tissue classification with the MoNuSAC dataset\n\nR. Verma, et al.Â â€œMoNuSAC2020: A Multi-organ Nuclei Segmentation and Classification Challenge.â€ IEEE Transactions on Medical Imaging (2021)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_2.html#data-preparation",
    "href": "notes/Adv_ML_Python_presentation_2_2.html#data-preparation",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 2)",
    "section": "Data preparation",
    "text": "Data preparation\n\nDownload the MoNuSAC dataset from https://monusac-2020.grand-challenge.org/Data/\n\n\n\n\n\n\n\nNote\n\n\nMore information about the type of tissue of each image can be found here."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-2",
    "href": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-2",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 2)",
    "section": "Tissue classification with the MoNuSAC dataset",
    "text": "Tissue classification with the MoNuSAC dataset\n\nExplore the dataset\n\n\nfrom skimage.io import imread\nimport matplotlib.pyplot as plt\n\nimg = imread(train_images_fns[0])\nplt.imshow(img)\nplt.title(tissue_classes[train_labels[0]])\nplt.show()\n\nimg = imread(test_images_fns[0])\nplt.imshow(img)\nplt.title(tissue_classes[test_labels[0]])\nplt.show()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-3",
    "href": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-3",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 2)",
    "section": "Tissue classification with the MoNuSAC dataset",
    "text": "Tissue classification with the MoNuSAC dataset\n\nDefine the pre-processing pipeline using the transforms from the pre-trained InceptionV3 model\n\n\nimport torchvision\nfrom torchvision.transforms.v2 import Compose, ToTensor\n\ninception_weights = torchvision.models.inception.Inception_V3_Weights.IMAGENET1K_V1\n\npipeline = Compose([\n  ToTensor(),\n  inception_weights.transforms()\n])\n\npipeline"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-4",
    "href": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-4",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 2)",
    "section": "Tissue classification with the MoNuSAC dataset",
    "text": "Tissue classification with the MoNuSAC dataset\n\nCreate a dataset class to load the images from disk\n\n\nfrom torch.utils.data import Dataset\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, image_filenames, image_labels, transform):\n        self.image_filenames = image_filenames\n        self.image_labels = image_labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_labels)\n\n    def __getitem__(self, idx):\n        image = imread(self.image_filenames[idx])\n        image = self.transform(image)\n\n        label = self.image_labels[idx]\n\n        return image, label\n\n\nCreate the training and test sets using the CustomImageDataset class.\n\n\ntrain_ds = CustomImageDataset(train_images_fns, train_labels, pipeline)\ntest_ds = CustomImageDataset(test_images_fns, test_labels, pipeline)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-5",
    "href": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-5",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 2)",
    "section": "Tissue classification with the MoNuSAC dataset",
    "text": "Tissue classification with the MoNuSAC dataset\n\nSplit the training set into train and validation subsets\n\n\nfrom torch.utils.data import random_split\n\ntrain_ds, val_ds = random_split(train_ds, [0.8, 0.2])\n\nprint(f\"Training images={len(train_ds)}\")\nprint(f\"Validation images={len(val_ds)}\")\nprint(f\"Test images={len(test_ds)}\")\n\n\nUse a DataLoader to serve image batches from the datasets.\n\n\nfrom torch.utils.data import DataLoader\n\ntrain_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_dl = DataLoader(val_ds, batch_size=128)\ntest_dl = DataLoader(test_ds, batch_size=128)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-6",
    "href": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-6",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 2)",
    "section": "Tissue classification with the MoNuSAC dataset",
    "text": "Tissue classification with the MoNuSAC dataset\n\nUse the pre-trained InceptionV3 model from torchvision\n\n\nimport torch\n\ndl_model = torchvision.models.inception_v3(\n    inception_weights,\n    progress=True\n)\n\ndl_model.eval()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-7",
    "href": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-7",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 2)",
    "section": "Tissue classification with the MoNuSAC dataset",
    "text": "Tissue classification with the MoNuSAC dataset\n\nReplace the classifier for a MLP with the output number of classes of the MoNuSAS dataset\n\n\nimport torch.nn as nn\n\ndl_model.fc = nn.Sequential(\n    nn.Linear(in_features=2048, out_features=1024, bias=True),\n    nn.ReLU(),\n    nn.Linear(in_features=1024, out_features=4, bias=True)\n)\n\n\nFreeze all layers but the MLP that serves as classifier.\n\n\nfor param in dl_model.parameters():\n    param.requires_grad = False\n\ndl_model.eval()\n\nfor param in dl_model.fc.parameters():\n    param.requires_grad = True\n\ndl_model.fc.train()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-8",
    "href": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-8",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 2)",
    "section": "Tissue classification with the MoNuSAC dataset",
    "text": "Tissue classification with the MoNuSAC dataset\n\nDefine the optimization method and loss function.\n\n\nimport torch.optim as optim\n\noptimizer = optim.Adam(dl_model.fc.parameters(), lr=0.001, weight_decay=0.001)\n\nloss_fun = nn.CrossEntropyLoss()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-9",
    "href": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-9",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 2)",
    "section": "Tissue classification with the MoNuSAC dataset",
    "text": "Tissue classification with the MoNuSAC dataset\n\nImplement the training step\n\n\nif torch.cuda.is_available():\n    dl_model.cuda()\n\navg_train_loss = 0\ntotal_train_samples = 0\n\ndl_model.fc.train()\nfor x, y in train_dl:\n    optimizer.zero_grad()\n\n    if torch.cuda.is_available():\n        x = x.cuda()\n\n    y_hat = dl_model(x).cpu()\n\n    loss = loss_fun(y_hat, y)\n\n    loss.backward()\n\n    optimizer.step()\n\n    avg_train_loss += loss.item() * len(y)\n    total_train_samples += len(y)\n\navg_train_loss /= total_train_samples\nprint(f\"Train loss={avg_train_loss}\")"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-10",
    "href": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-10",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 2)",
    "section": "Tissue classification with the MoNuSAC dataset",
    "text": "Tissue classification with the MoNuSAC dataset\n\nImplement the validation step\n\n\navg_val_loss = 0\ntotal_val_samples = 0\n\ndl_model.fc.eval()\nwith torch.no_grad():\n    for x, y in val_dl:\n        if torch.cuda.is_available():\n            x = x.cuda()\n\n        y_hat = dl_model(x).cpu()\n\n        loss = loss_fun(y_hat, y)\n\n        avg_val_loss += loss.item() * len(y)\n        total_val_samples += len(y)\n\navg_val_loss /= total_val_samples\nprint(f\"Validation loss={avg_val_loss}\")"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-11",
    "href": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-11",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 2)",
    "section": "Tissue classification with the MoNuSAC dataset",
    "text": "Tissue classification with the MoNuSAC dataset\n\nPut those two steps together inside an epoch loop and measure the accuracy of the model\n\n\nfrom torchmetrics.classification import Accuracy\n\nif torch.cuda.is_available():\n    dl_model.cuda()\n\ntrain_acc_metric = Accuracy(task=\"multiclass\", num_classes=4)\nval_acc_metric = Accuracy(task=\"multiclass\", num_classes=4)\n\nnum_epochs = 20\nfor e in range(num_epochs):\n    avg_train_loss = 0\n    total_train_samples = 0\n\n    dl_model.fc.train()\n    for x, y in train_dl:\n        optimizer.zero_grad()\n\n        if torch.cuda.is_available():\n            x = x.cuda()\n\n        y_hat = dl_model(x).cpu()\n\n        loss = loss_fun(y_hat, y)\n\n        loss.backward()\n\n        optimizer.step()\n\n        avg_train_loss += loss.item() * len(y)\n        total_train_samples += len(y)\n\n        train_acc_metric(y_hat.softmax(dim=1), y)\n\n    avg_train_loss /= total_train_samples\n    train_acc = train_acc_metric.compute()\n\n    avg_val_loss = 0\n    total_val_samples = 0\n\n    dl_model.fc.eval()\n    with torch.no_grad():\n        for x, y in val_dl:\n            if torch.cuda.is_available():\n                x = x.cuda()\n\n            y_hat = dl_model(x).cpu()\n\n            loss = loss_fun(y_hat, y)\n\n            avg_val_loss += loss.item() * len(y)\n            total_val_samples += len(y)\n\n            val_acc_metric(y_hat.softmax(dim=1), y)\n\n    avg_val_loss /= total_val_samples\n    val_acc = val_acc_metric.compute()\n\n    print(f\"[{(e + 1) / num_epochs: 2.2%}] Train loss={avg_train_loss: 2.4} (Acc={train_acc: 2.2%}), Validation loss={avg_val_loss: 2.4} (Acc={val_acc: 2.2%})\")\n\n    train_acc_metric.reset()\n    val_acc_metric.reset()\n\n[ 5.00%] Train loss= 1.458 (Acc= 31.55%), Validation loss= 1.471 (Acc= 17.07%)\n[ 10.00%] Train loss= 1.291 (Acc= 38.69%), Validation loss= 1.313 (Acc= 24.39%)\n[ 15.00%] Train loss= 1.162 (Acc= 50.00%), Validation loss= 1.329 (Acc= 26.83%)\n[ 20.00%] Train loss= 1.028 (Acc= 62.50%), Validation loss= 1.305 (Acc= 39.02%)\n[ 25.00%] Train loss= 0.8927 (Acc= 76.79%), Validation loss= 1.29 (Acc= 39.02%)\n[ 30.00%] Train loss= 0.8068 (Acc= 70.24%), Validation loss= 1.255 (Acc= 51.22%)\n[ 35.00%] Train loss= 0.7616 (Acc= 72.02%), Validation loss= 1.418 (Acc= 36.59%)\n[ 40.00%] Train loss= 0.6216 (Acc= 82.14%), Validation loss= 1.256 (Acc= 48.78%)\n[ 45.00%] Train loss= 0.5317 (Acc= 87.50%), Validation loss= 1.459 (Acc= 39.02%)\n[ 50.00%] Train loss= 0.4719 (Acc= 88.10%), Validation loss= 1.322 (Acc= 48.78%)\n[ 55.00%] Train loss= 0.4233 (Acc= 89.88%), Validation loss= 1.484 (Acc= 34.15%)\n[ 60.00%] Train loss= 0.4569 (Acc= 86.31%), Validation loss= 1.779 (Acc= 46.34%)\n[ 65.00%] Train loss= 0.3873 (Acc= 89.29%), Validation loss= 1.474 (Acc= 39.02%)\n[ 70.00%] Train loss= 0.3403 (Acc= 92.26%), Validation loss= 1.646 (Acc= 51.22%)\n[ 75.00%] Train loss= 0.2852 (Acc= 94.64%), Validation loss= 1.535 (Acc= 39.02%)\n[ 80.00%] Train loss= 0.2652 (Acc= 94.05%), Validation loss= 1.518 (Acc= 53.66%)\n[ 85.00%] Train loss= 0.2109 (Acc= 96.43%), Validation loss= 1.48 (Acc= 39.02%)\n[ 90.00%] Train loss= 0.1441 (Acc= 98.81%), Validation loss= 1.476 (Acc= 43.90%)\n[ 95.00%] Train loss= 0.1203 (Acc= 99.40%), Validation loss= 1.56 (Acc= 51.22%)\n[ 100.00%] Train loss= 0.1117 (Acc= 99.40%), Validation loss= 1.479 (Acc= 48.78%)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-12",
    "href": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-12",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 2)",
    "section": "Tissue classification with the MoNuSAC dataset",
    "text": "Tissue classification with the MoNuSAC dataset\n\nCompute the performance of the model on the test set\n\n\navg_test_loss = 0\ntotal_test_samples = 0\n\ntest_acc_metric = Accuracy(task=\"multiclass\", num_classes=4)\n\ndl_model.fc.eval()\nwith torch.no_grad():\n    for x, y in test_dl:\n        if torch.cuda.is_available():\n            x = x.cuda()\n\n        y_hat = dl_model(x).cpu()\n\n        loss = loss_fun(y_hat, y)\n\n        avg_test_loss += loss.item() * len(y)\n        total_test_samples += len(y)\n\n        test_acc_metric(y_hat.softmax(dim=1), y)\n\navg_test_loss /= total_test_samples\ntest_acc = test_acc_metric.compute()\n\nprint(f\"Test loss={avg_test_loss: 2.4} (Acc={test_acc: 2.2%})\")\n\ntest_acc_metric.reset()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-13",
    "href": "notes/Adv_ML_Python_presentation_2_2.html#tissue-classification-with-the-monusac-dataset-13",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 2)",
    "section": "Tissue classification with the MoNuSAC dataset",
    "text": "Tissue classification with the MoNuSAC dataset\n\nSave the current state of the model\n\n\ncheckpoint = dl_model.state_dict()\ntorch.save(checkpoint, \"monusac_checkpoint.pt\")"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_2.html#explore-the-monusac-dataset-in-the-embedded-feature-space-1",
    "href": "notes/Adv_ML_Python_presentation_2_2.html#explore-the-monusac-dataset-in-the-embedded-feature-space-1",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 2)",
    "section": "Explore the MoNuSAC dataset in the embedded feature space",
    "text": "Explore the MoNuSAC dataset in the embedded feature space\n\nExtract the embedded features from the train set.\n\n\nimport numpy as np\n\nif torch.cuda.is_available():\n    dl_model.cuda()\n\ntrain_features = []\ntrain_labels = []\n\ndl_model.fc[1] = nn.Identity()\n\ndl_model.eval()\nwith torch.no_grad():\n    for x, y in train_dl:\n        if torch.cuda.is_available():\n            x = x.cuda()\n\n        fx = dl_model(x).cpu().detach().numpy()\n\n        train_features.append(fx)\n        train_labels.append(y.numpy())\n\ntrain_features = np.concatenate(train_features, 0)\ntrain_labels = np.concatenate(train_labels, 0)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_2.html#explore-the-monusac-dataset-in-the-embedded-feature-space-2",
    "href": "notes/Adv_ML_Python_presentation_2_2.html#explore-the-monusac-dataset-in-the-embedded-feature-space-2",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 2)",
    "section": "Explore the MoNuSAC dataset in the embedded feature space",
    "text": "Explore the MoNuSAC dataset in the embedded feature space\n\nProject the features into a two-dimensional UMap.\n\n\nimport umap\n\nreducer = umap.UMAP()\n\nembedding = reducer.fit_transform(train_features)\n\nembedding.shape"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_2.html#explore-the-monusac-dataset-in-the-embedded-feature-space-3",
    "href": "notes/Adv_ML_Python_presentation_2_2.html#explore-the-monusac-dataset-in-the-embedded-feature-space-3",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 2)",
    "section": "Explore the MoNuSAC dataset in the embedded feature space",
    "text": "Explore the MoNuSAC dataset in the embedded feature space\n\nShow the embedded space.\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nfor c in range(4):\n    plt.scatter(embedding[train_labels == c, 0], embedding[train_labels == c, 1], label=tissue_classes[c])\n\nplt.legend()\nplt.gca().set_aspect('equal', 'datalim')\nplt.title('UMAP projection of InceptionV3 features of the MoNuSAC dataset', fontsize=24)\n\n\nText(0.5, 1.0, 'UMAP projection of InceptionV3 features of the MoNuSAC dataset')"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_2.html#explore-the-monusac-dataset-in-the-embedded-feature-space-4",
    "href": "notes/Adv_ML_Python_presentation_2_2.html#explore-the-monusac-dataset-in-the-embedded-feature-space-4",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 2)",
    "section": "Explore the MoNuSAC dataset in the embedded feature space",
    "text": "Explore the MoNuSAC dataset in the embedded feature space\n\nExtract the embedded features from the validation and test sets.\n\n\nval_features = []\nval_labels = []\n\ntest_features = []\ntest_labels = []\n\nwith torch.no_grad():\n    for x, y in val_dl:\n        if torch.cuda.is_available():\n            x = x.cuda()\n\n        fx = dl_model(x).cpu().detach().numpy()\n\n        val_features.append(fx)\n        val_labels.append(y.numpy())\n\nval_features = np.concatenate(val_features, 0)\nval_labels = np.concatenate(val_labels, 0)\n\nwith torch.no_grad():\n    for x, y in test_dl:\n        if torch.cuda.is_available():\n            x = x.cuda()\n\n        fx = dl_model(x).cpu().detach().numpy()\n\n        test_features.append(fx)\n        test_labels.append(y.numpy())\n\ntest_features = np.concatenate(test_features, 0)\ntest_labels = np.concatenate(test_labels, 0)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2_2.html#explore-the-monusac-dataset-in-the-embedded-feature-space-5",
    "href": "notes/Adv_ML_Python_presentation_2_2.html#explore-the-monusac-dataset-in-the-embedded-feature-space-5",
    "title": "Advanced Machine Learning with Python (Session 2 - Part 2)",
    "section": "Explore the MoNuSAC dataset in the embedded feature space",
    "text": "Explore the MoNuSAC dataset in the embedded feature space\n\nProject the features of the test and validation sets into a two-dimensional UMap.\n\n\nembedding_val = reducer.transform(val_features)\nembedding_test = reducer.transform(test_features)\n\n\nShow the embedded space.\n\n\n\nCode\nfor c in range(4):\n    plt.scatter(embedding[train_labels == c, 0], embedding[train_labels == c, 1], label=tissue_classes[c] + \" (train)\", marker=\"o\")\n\nfor c in range(4):\n    plt.scatter(embedding_val[val_labels == c, 0], embedding_val[val_labels == c, 1], label=tissue_classes[c] + \" (validation)\", marker=\"v\")\n\nfor c in range(4):\n    plt.scatter(embedding_test[test_labels == c, 0], embedding_test[test_labels == c, 1], label=tissue_classes[c] + \" (test)\", marker=\"s\")\n\nplt.legend()\nplt.gca().set_aspect('equal', 'datalim')\nplt.title('UMAP projection of InceptionV3 features of the MoNuSAC dataset', fontsize=24)\n\n\nText(0.5, 1.0, 'UMAP projection of InceptionV3 features of the MoNuSAC dataset')"
  }
]