[
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#workshop-outcomes",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#workshop-outcomes",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Workshop outcomes",
    "text": "Workshop outcomes\n\nUnderstand the process of training ML models.\nLoad pre-trained ML models and fine-tune them with new data.\nEvaluate the performance of ML models.\nAdapt ML models for different tasks from pre-trained models."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#select-runtime-and-connect",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#select-runtime-and-connect",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Select runtime and connect",
    "text": "Select runtime and connect\nOn the top right corner of the page, click the drop-down arrow to the right of the Connect button and select Change runtime type."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#machine-learning-ml",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#machine-learning-ml",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Machine Learning (ML)",
    "text": "Machine Learning (ML)\nSub-field of Artificial Intelligence that develops methods to address tasks that require human intelligence"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#artificial-intelligence-tasks",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#artificial-intelligence-tasks",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Artificial intelligence tasks",
    "text": "Artificial intelligence tasks"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#common-tasks",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#common-tasks",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Common tasks",
    "text": "Common tasks\n\n\nClassification\n\nwhat is this?\n\nDetection\n\nwhere is something?\n\nSegmentation\n\nwhere specifically is something?"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#types-of-machine-learning-1",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#types-of-machine-learning-1",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Types of machine learning",
    "text": "Types of machine learning\nDepending on how the model is trained\n\nSupervised\nUnsupervised\nWeakly supervised\nReinforced\n…\n\n\nSupervised learning: teach the machine to perform a task with a set of inputs and their respective expected outcome (\\(X\\), \\(Y\\)).\nUnsupervised learning: let the machine learn to perform a task on its own (\\(X\\)) without any specific expected outcome.\nWeakly supervised: teach the machine to perform a task using a limited set of expected outcomes.\nReinforced learning: let the machine learn to perform a task on its own, then give it a reward relative to its performance."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#inputs-and-outputs-1",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#inputs-and-outputs-1",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Inputs and outputs",
    "text": "Inputs and outputs\nFor a task, we want to model the outcome/output (\\(y\\)) obtained by a given input (\\(x\\))\n\\(f(x) \\approx y\\)\n\n\n\n\n\n\nNote\n\n\nThe complete set of (\\(x\\), \\(y\\)) pairs is known as dataset (\\(X\\), \\(Y\\)).\n\n\n\n\n\n\n\n\n\nNote\n\n\nInputs can be virtually anything, including images, texts, video, audio, electrical signals, etc.\nWhile outputs are expected to be some meaningful piece of information, such as a category, position, value, etc."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#use-case-image-classification-with-the-cifar-100-dataset",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#use-case-image-classification-with-the-cifar-100-dataset",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Use case: Image classification with the CIFAR-100 dataset",
    "text": "Use case: Image classification with the CIFAR-100 dataset\n\nLoad the CIFAR-100 dataset from torchvision.datasets\n\n\nimport torch\nimport torchvision\n\ncifar_ds = torchvision.datasets.CIFAR100(root=\"/tmp\", train=True, download=True)\n\n\nExplore the CIFAR-100 dataset\n\n\nx_im, y = cifar_ds[0]\n\nlen(cifar_ds), type(x_im), type(y)\n\n(50000, PIL.Image.Image, int)\n\n\n\n\ny = 19 (cattle)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#what-is-a-tensor-pytorch",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#what-is-a-tensor-pytorch",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "What is a tensor (PyTorch)?",
    "text": "What is a tensor (PyTorch)?\nA tensor is a multi-dimensional array. In PyTorch, this comes from a generalization of the notation of variables that exists on more than two dimensions.\n\nzero-dimensional variables are points,\none-dimensional variables are vectors,\ntwo-dimensional variables are matrices,\nand three or more dimensional variables, are tensors.\n\n\nimport torch\n\nx0 = torch.Tensor([7]) # This is a point\n\nx1 = torch.Tensor([15, 64, 123]) # This is a vector\n\nx2 = torch.Tensor([[3, 6, 5],\n                   [7, 9, 12],\n                   [10, 33, 1]]) # This is a matrix\n\nx3 = torch.Tensor([[[[1, 0, 0],\n                     [0, 1, 0],\n                     [0, 0, 1]],\n                    [[2, 0, 1],\n                     [0, 2, 3],\n                     [4, 1, 5]]]]) # This is a tensor"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-add-the-preprocessing-pipeline-to-the-cifar-100-dataset",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-add-the-preprocessing-pipeline-to-the-cifar-100-dataset",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Add the preprocessing pipeline to the CIFAR-100 dataset",
    "text": "Exercise: Add the preprocessing pipeline to the CIFAR-100 dataset\n\nRe-load the CIFAR-100 dataset, this time passing the pre_process function as argument.\n\n\ncifar_ds = torchvision.datasets.CIFAR100(root=\"/tmp\", train=True, download=True, transform=pre_process)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#training-set",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#training-set",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Training set",
    "text": "Training set\nThe examples (\\(x\\), \\(y\\)) used to teach a machine/model to perform a task"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#validation-set",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#validation-set",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Validation set",
    "text": "Validation set\nUsed to measure the performance of a model during training\nThis subset is not used for training the model, so it is unseen data.\n\nThis is a subset from the training set and can be used to test the generalization capacity of the model or to select the best configuration of a model."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#test-set",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#test-set",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Test set",
    "text": "Test set\nThis set of samples is not used when training\nIts purpose is to measure the generalization capacity of the model"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-load-the-test-set-and-split-the-train-set-into-train-and-validation-subsets",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-load-the-test-set-and-split-the-train-set-into-train-and-validation-subsets",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Load the test set and split the train set into train and validation subsets",
    "text": "Exercise: Load the test set and split the train set into train and validation subsets\n\nLoad the CIFAR-100 test set\n\n\ncifar_test_ds = torchvision.datasets.CIFAR100(root=\"/tmp\", train=False, download=True, transform=pre_process)\n\n\nSplit the training set into train and validation subsets\n\n\nfrom torch.utils.data import random_split\n\ncifar_train_ds, cifar_val_ds = random_split(cifar_ds, (40_000, 10_000))"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#deep-learning-dl-models-1",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#deep-learning-dl-models-1",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Deep Learning (DL) models",
    "text": "Deep Learning (DL) models\nModels that construct knowledge in a hierarchical manner are considered deep models.\n\n\n\nFrom Cervantes-Sanchez et al."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-create-a-logisic-regression-model-with-pytorch",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-create-a-logisic-regression-model-with-pytorch",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Create a Logisic Regression model with PyTorch",
    "text": "Exercise: Create a Logisic Regression model with PyTorch\n\nUse the nn (Neural Networks) module from pytorch to create a Logistic Regression model\n\n\nimport torch.nn as nn\n\nlr_clf_1 = nn.Linear(in_features=3 * 32 * 32, out_features=100, bias=True)\nlr_clf_2 = nn.Softmax()\n\n\nFeed the model with a sample x\n\n\n\n\n\n\n\nImportant\n\n\nWe have to reshape x before feeding it to the model because x is an image with axes: Channels, Height, Width (CHW), but the Logistic Regression input should be a vector.\n\n\n\n\ny_hat = lr_clf_2( lr_clf_1( x.reshape(1, -1) ))\n\ntype(y_hat), y_hat.shape, y_hat.dtype\n\n(torch.Tensor, torch.Size([1, 100]), torch.float32)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-create-a-multilayer-perceptron-mlp-model-with-pytorch",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-create-a-multilayer-perceptron-mlp-model-with-pytorch",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Create a MultiLayer Perceptron (MLP) model with PyTorch",
    "text": "Exercise: Create a MultiLayer Perceptron (MLP) model with PyTorch\n\nUse the nn.Sequential module to build sequential models\n\n\nmlp_clf = nn.Sequential(\n  nn.Linear(in_features=3 * 32 * 32, out_features=1024, bias=True),\n  nn.Tanh(),\n  nn.Linear(in_features=1024, out_features=100, bias=True),\n  nn.Softmax()\n)\n\n\nFeed the model with a sample x\n\n\ny_hat = mlp_clf(x.reshape(1, -1))\n\ntype(y_hat), y_hat.shape, y_hat.dtype\n\n(torch.Tensor, torch.Size([1, 100]), torch.float32)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#model-fittingtraining",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#model-fittingtraining",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Model fitting/training",
    "text": "Model fitting/training\nModels behavior depends directly on the value of their set of parameters \\(\\theta\\).\n\n\\(f(x) \\approx y\\)\n\\(f_\\theta(x) = y + \\epsilon = \\hat{y}\\)\n\n\n\n\n\n\n\nNote\n\n\nAs models increase their number of parameters, they become more complex\n\n\n\nTraining is the process of optimizing the values of \\(\\theta\\)\n\nTraining is often an expensive process in terms of computational resources and time."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#loss-function-1",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#loss-function-1",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Loss function",
    "text": "Loss function\nThis is measure of the difference between the expected outputs and the predictions made by a model \\(L(Y, \\hat{Y})\\).\n\n\n\n\n\n\nNote\n\n\nWe look for smooth loss functions for which we can compute their gradient"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#loss-function-for-regression",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#loss-function-for-regression",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "11.1 Loss function for regression",
    "text": "11.1 Loss function for regression\nIn the case of regression tasks we generally use the Mean Squared Error (MSE).\n\\(MSE=\\frac{1}{N}\\sum \\left(Y - \\hat{Y}\\right)^2\\)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#loss-function-for-classification",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#loss-function-for-classification",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Loss function for classification",
    "text": "Loss function for classification\nAnd for classification tasks we use the Cross Entropy (CE) function.\n\\(CE = -\\frac{1}{N}\\sum\\limits_i^N\\sum\\limits_k^C y_{i,k} log(\\hat{y_{i,k}})\\)\nwhere \\(C\\) is the number of classes.\n\n\n\n\n\n\nNote\n\n\nFor the binary classification case:\n\\(BCE = -\\frac{1}{N}\\sum\\limits_i^N \\left(y_i log(\\hat{y_i}) + (1 - y_i) log(1 - \\hat{y_i})\\right)\\)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-define-the-loss-function-for-the-cifar-100-classification-problem",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-define-the-loss-function-for-the-cifar-100-classification-problem",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Define the loss function for the CIFAR-100 classification problem",
    "text": "Exercise: Define the loss function for the CIFAR-100 classification problem\n\nDefine a Cross Entropy loss function with nn.CrossEntropyLoss\n\n\nloss_fun = nn.CrossEntropyLoss()\n\n\nRemove the nn.Softmax layer from the MLP model.\n\n\n\n\n\n\n\nNote\n\n\nAccording to the PyTorch documentation, the CrossEntropyLoss function takes as inputs the logits of the probabilities and not the probabilities themselves. So, we don’t need to squash the output of the MLP model.\n\n\n\n\nmlp_clf = nn.Sequential(\n  nn.Linear(in_features=3 * 32 * 32, out_features=1024, bias=True),\n  nn.Tanh(),\n  nn.Linear(in_features=1024, out_features=100, bias=True),\n  # nn.Softmax() # &lt;- remove this line\n)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-define-the-loss-function-for-the-cifar-100-classification-problem-1",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-define-the-loss-function-for-the-cifar-100-classification-problem-1",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Define the loss function for the CIFAR-100 classification problem",
    "text": "Exercise: Define the loss function for the CIFAR-100 classification problem\n\nMeasure the prediction loss (error) of our MLP with respect to the grund-truth\n\n\n\n\n\n\n\nImportant\n\n\nWe are using a PyTorch loss function, and it expects PyTorch’s tensors as arguments, so we have to convert y to tensor before computing the loss function.\n\n\n\n\nloss = loss_fun(y_hat, torch.LongTensor([y]))\n\nloss\n\ntensor(4.6027, grad_fn=&lt;NllLossBackward0&gt;)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#gradient-based-optimization-1",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#gradient-based-optimization-1",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Gradient based optimization",
    "text": "Gradient based optimization\nGradient-based methods are able to fit large numbers of parameters when using a smooth Loss function as target.\n\n\n\n\n\n\nNote\n\n\nWe compute the gradient of the loss function with respect to the model parameters using the chain rule from calculous. Generally, this is managed by the machine learning packages such as PyTorch and Tensorflow with a method called back propagation.\n\n\n\nGradient Descent\n\n\\(\\theta^{t+1} = \\theta^t - \\eta \\nabla_\\theta L(Y, \\hat{Y})\\)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-compute-the-gradient-of-the-loss-function-with-respect-to-the-parameters-of-the-mlp.",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-compute-the-gradient-of-the-loss-function-with-respect-to-the-parameters-of-the-mlp.",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Compute the gradient of the loss function with respect to the parameters of the MLP.",
    "text": "Exercise: Compute the gradient of the loss function with respect to the parameters of the MLP.\n\nCheck what are the gradients of the MLP parameters before back propagating the gradient.\n\n\nmlp_clf[0].bias.grad\n\n\nCompute the gradient of the loss function with respect to the MLP parameters.\n\n\n\n\n\n\n\nNote\n\n\nTo back propagate the gradients we use the loss.backward() method of the loss function.\n\n\n\n\nloss = loss_fun(y_hat, torch.LongTensor([y]))\n\nloss.backward()\n\n\nVerify that the gradients have been propagated to the model parameters.\n\n\nmlp_clf[0].bias.grad"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#stochastic-methods",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#stochastic-methods",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Stochastic methods",
    "text": "Stochastic methods\n\n\n\n\n\n\nCaution\n\n\nThe Gradient descent method require to obtain the Loss function for the whole training set before doing a single update.\nThis can be inefficient when large volumes of data are used for training the model.\n\n\n\n\nThese methods use a relative small sample from the training data called mini-batch at a time.\nThis reduces the amount of memory used for computing intermediate operations carried out during optimization process."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#stochastic-gradient-descent-sgd",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#stochastic-gradient-descent-sgd",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Stochastic Gradient Descent (SGD)",
    "text": "Stochastic Gradient Descent (SGD)\n\nThis strategy defines \\(\\theta\\)’s’ update rule for iteration \\(t+1\\) using a mini-batch sampled at random from the training set as follows.\n\n\n\\(\\theta^{t+1} = \\theta^t - \\eta \\nabla_\\theta L(Y_{b}, \\hat{Y_{b}})\\)\n\\(\\eta\\) controls the update we perform on the current parameter’s values\n\n\n\n\n\n\n\nNote\n\n\nThis parameter in Deep Learning is known as the learning rate"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#training-with-mini-batches",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#training-with-mini-batches",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Training with mini-batches",
    "text": "Training with mini-batches\n\n\n\n\n\n\nNote\n\n\nPyTorch can operate efficiently on multiple inputs at the same time. To do that, we can use a DataLoader to serve mini-batches of inputs."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-train-the-mlp-classifier",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-train-the-mlp-classifier",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Train the MLP classifier",
    "text": "Exercise: Train the MLP classifier\n\nUse a DataLoader to serve mini-batches of images to train our MLP.\n\n\nfrom torch.utils.data import DataLoader\n\ncifar_train_dl = DataLoader(cifar_train_ds, batch_size=128, shuffle=True)\ncifar_val_dl = DataLoader(cifar_val_ds, batch_size=256)\ncifar_test_dl = DataLoader(cifar_test_ds, batch_size=256)\n\n\nCreate a Stochastic Gradient Descent optimizer for our MLP classifier.\n\n\nimport torch.optim as optim\n\noptimizer = optim.SGD(mlp_clf.parameters(), lr=0.01, )"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-train-the-mlp-classifier-1",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-train-the-mlp-classifier-1",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Train the MLP classifier",
    "text": "Exercise: Train the MLP classifier\n\nImplement the training-loop to fit the parameters of our MLP classifier.\n\n\n\n\n\n\n\nNote\n\n\nGradients are accumulated on every iteration, so we need to reset the accumulator with optimizer.zero_grad() for every new batch.\n\n\n\n\n\n\n\n\n\nNote\n\n\nTo perform get the new iteration’s parameter values \\(\\theta^{t+1}\\) we use optimizer.step() to compute the update step.\n\n\n\nmlp_clf.train()\nfor x, y in cifar_train_dl:\n  optimizer.zero_grad()\n\n  y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) ) # Reshape it into a batch of vectors\n\n  loss = loss_fun(y_hat, y)\n\n  loss.backward()\n\n  optimizer.step()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-train-the-mlp-classifier-and-track-the-training-and-validation-loss",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-train-the-mlp-classifier-and-track-the-training-and-validation-loss",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Train the MLP classifier and track the training and validation loss",
    "text": "Exercise: Train the MLP classifier and track the training and validation loss\n\nSave the loss function of each batch and the overall average loss during training.\n\n\n\n\n\n\n\nNote\n\n\nTo extract the loss function’s value without anything else attached use loss.item().\n\n\n\ntrain_loss = []\ntrain_loss_avg = 0\ntotal_train_samples = 0\n\nmlp_clf.train()\nfor x, y in cifar_train_dl:\n  optimizer.zero_grad()\n\n  y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) ) # Reshape it into a batch of vectors\n\n  loss = loss_fun(y_hat, y)\n\n  train_loss.append(loss.item())\n  train_loss_avg += loss.item() * len(x)\n  total_train_samples += len(x)\n\n  loss.backward()\n\n  optimizer.step()\n\ntrain_loss_avg /= total_train_samples"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-train-the-mlp-classifier-and-track-the-training-and-validation-loss-1",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-train-the-mlp-classifier-and-track-the-training-and-validation-loss-1",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Train the MLP classifier and track the training and validation loss",
    "text": "Exercise: Train the MLP classifier and track the training and validation loss\n\nCompute the average loss function for the validation set.\n\n\n\n\n\n\n\nNote\n\n\nBecause we don’t train the model with the validation set, back-propagation and optimization steps are not needed.\nAdditionally, we wrap the loop with torch.no_grad() to prevent the generation of gradients that could fill the memory unnecessarily.\n\n\n\nval_loss_avg = 0\ntotal_val_samples = 0\n\nmlp_clf.eval()\nwith torch.no_grad():\n  for x, y in cifar_val_dl:\n    y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) ) # Reshape it into a batch of vectors\n    loss = loss_fun(y_hat, y)\n\n    val_loss_avg += loss.item() * len(x)\n    total_val_samples += len(x)\n\nval_loss_avg /= total_val_samples"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-train-the-mlp-classifier-and-track-the-training-and-validation-loss-2",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-train-the-mlp-classifier-and-track-the-training-and-validation-loss-2",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Train the MLP classifier and track the training and validation loss",
    "text": "Exercise: Train the MLP classifier and track the training and validation loss\n\nPlot the training loss for this epoch.\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot(train_loss, \"b-\", label=\"Training loss\")\nplt.plot([0, len(train_loss)], [train_loss_avg, train_loss_avg], \"r:\", label=\"Average training loss\")\nplt.plot([0, len(train_loss)], [val_loss_avg, val_loss_avg], \"b:\", label=\"Average validation loss\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-train-the-mlp-classifier-and-track-the-training-and-validation-loss-through-several-epochs",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-train-the-mlp-classifier-and-track-the-training-and-validation-loss-through-several-epochs",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Train the MLP classifier and track the training and validation loss through several epochs",
    "text": "Exercise: Train the MLP classifier and track the training and validation loss through several epochs\nnum_epochs = 10\ntrain_loss = []\nval_loss = []\n\nfor e in range(num_epochs):\n  train_loss_avg = 0\n  total_train_samples = 0\n\n  mlp_clf.train()\n  for x, y in cifar_train_dl:\n    optimizer.zero_grad()\n\n    y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) ) # Reshape it into a batch of vectors\n\n    loss = loss_fun(y_hat, y)\n\n    train_loss_avg += loss.item() * len(x)\n    total_train_samples += len(x)\n\n    loss.backward()\n\n    optimizer.step()\n\n  train_loss_avg /= total_train_samples\n  train_loss.append(train_loss_avg)\n\n  val_loss_avg = 0\n  total_val_samples = 0\n\n  mlp_clf.eval()\n  with torch.no_grad():\n    for x, y in cifar_val_dl:\n      y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) ) # Reshape it into a batch of vectors\n      loss = loss_fun(y_hat, y)\n\n      val_loss_avg += loss.item() * len(x)\n      total_val_samples += len(x)\n\n  val_loss_avg /= total_val_samples\n  val_loss.append(val_loss_avg)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-show-the-progress-of-the-training-throughout-the-epochs",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-show-the-progress-of-the-training-throughout-the-epochs",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Show the progress of the training throughout the epochs",
    "text": "Exercise: Show the progress of the training throughout the epochs\n\nPlot the average train and validation losses\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot(train_loss, \"b-\", label=\"Average training loss\")\nplt.plot(val_loss, \"r-\", label=\"Average validation loss\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#performance-metrics-1",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#performance-metrics-1",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Performance metrics",
    "text": "Performance metrics\nUsed to measure how good or bad a model carries out a task\n\n\\(f(x) \\approx y\\)\n\\(f(x) = y + \\epsilon = \\hat{y}\\)\n\n\nGiven the set of parameters, as well as other factors, the output of a model can deviate from the expected outcome. So, the actual output of a model is \\(f(x) = \\hat{y}\\).\n\n\n\n\n\n\n\nNote\n\n\nThe output \\(\\hat{y}\\) is called prediction  given the context taken from statistical regression analysis.\n\n\n\n\n\n\n\n\n\nImportant\n\n\nSelecting the correct performance metrics depends on the training type, task, and even the distribution of the data."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-measure-the-accuracy-of-the-mlp-trained-to-classify-images-from-cifar-100",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-measure-the-accuracy-of-the-mlp-trained-to-classify-images-from-cifar-100",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Measure the accuracy of the MLP trained to classify images from CIFAR-100",
    "text": "Exercise: Measure the accuracy of the MLP trained to classify images from CIFAR-100\n\nInstall the torchmetrics package.\n\n!pip install torchmetrics\n\nCompute the average accuracy for the Train set.\n\n\nfrom torchmetrics.classification import Accuracy\n\nmlp_clf.eval()\n\ntrain_acc_metric = Accuracy(task=\"multiclass\", num_classes=100)\n\nwith torch.no_grad():\n  for x, y in cifar_train_dl:\n    y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) )\n    train_acc_metric(y_hat.softmax(dim=1), y)\n\n  train_acc = train_acc_metric.compute()\n\nprint(f\"Training acc={train_acc}\")\ntrain_acc_metric.reset()\n\nTraining acc=0.1317249983549118"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-measure-the-accuracy-of-the-mlp-trained-to-classify-images-from-cifar-100-1",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-measure-the-accuracy-of-the-mlp-trained-to-classify-images-from-cifar-100-1",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Measure the accuracy of the MLP trained to classify images from CIFAR-100",
    "text": "Exercise: Measure the accuracy of the MLP trained to classify images from CIFAR-100\n\nCompute the average accuracy for the Validation and Test sets.\n\n\nval_acc_metric = Accuracy(task=\"multiclass\", num_classes=100)\ntest_acc_metric = Accuracy(task=\"multiclass\", num_classes=100)\n\nwith torch.no_grad():\n  for x, y in cifar_val_dl:\n    y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) )\n    val_acc_metric(y_hat.softmax(dim=1), y)\n\n  val_acc = val_acc_metric.compute()\n\n  for x, y in cifar_test_dl:\n    y_hat = mlp_clf( x.reshape(-1, 3 * 32 * 32) )\n    test_acc_metric(y_hat.softmax(dim=1), y)\n\n  test_acc = test_acc_metric.compute()\n\nprint(f\"Validation acc={val_acc}\")\nprint(f\"Test acc={test_acc}\")\n\nval_acc_metric.reset()\ntest_acc_metric.reset()\n\nValidation acc=0.12780000269412994\nTest acc=0.12269999831914902"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#convolution-layers",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#convolution-layers",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Convolution layers",
    "text": "Convolution layers\nThe most common operation in DL models for image processing are Convolution operations.\n\n2D ConvolutionThe animation shows the convolution of a 7x7 pixels input image (bottom) with a 3x3 pixels kernel (moving window), that results in a 5x5 pixels output (top)."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-visualize-the-effect-of-the-convolution-operation",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-visualize-the-effect-of-the-convolution-operation",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Visualize the effect of the convolution operation",
    "text": "Exercise: Visualize the effect of the convolution operation\n\nCreate a convolution layer with nn.Conv2D using 3 channels as input, and a single one for output.\n\n\nconv_1 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=7, padding=0, bias=True)\n\nx, _ = next(iter(cifar_train_dl))\n\nfx = conv_1(x)\n\ntype(fx), fx.dtype, fx.shape, fx.min(), fx.max()\n\n(torch.Tensor,\n torch.float32,\n torch.Size([128, 1, 26, 26]),\n tensor(-0.5288, grad_fn=&lt;MinBackward1&gt;),\n tensor(0.7724, grad_fn=&lt;MaxBackward1&gt;))\n\n\n\n\n\n\n\n\nWarning\n\n\nThe convolution layer is initialized with random values, so the results will vary."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-visualize-the-effect-of-the-convolution-operation-1",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-visualize-the-effect-of-the-convolution-operation-1",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Visualize the effect of the convolution operation",
    "text": "Exercise: Visualize the effect of the convolution operation\n\nCreate a convolution layer with nn.Conv2D using 3 channels as input, and a single one for output.\n\n\nplt.rcParams['figure.figsize'] = [5, 5]\n\nfig, ax = plt.subplots(1, 2)\nax[0].imshow(x[0].permute(1, 2, 0))\nax[1].imshow(fx.detach()[0, 0], cmap=\"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nBy default, outputs from PyTorch modules are tracked for back-propagation.\nTo visualize it with matplotlib we have to .detach() the tensor first."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-visualize-the-effect-of-the-convolution-operation-2",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-visualize-the-effect-of-the-convolution-operation-2",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Visualize the effect of the convolution operation",
    "text": "Exercise: Visualize the effect of the convolution operation\n\nVisualize the weights of the convolution layer.\n\n\nconv_1.weight.shape\n\ntorch.Size([1, 3, 7, 7])\n\n\n\nfig, ax = plt.subplots(2, 2)\nax[0, 0].imshow(conv_1.weight.detach()[0, 0], cmap=\"gray\")\nax[0, 1].imshow(conv_1.weight.detach()[0, 1], cmap=\"gray\")\nax[1, 0].imshow(conv_1.weight.detach()[0, 2], cmap=\"gray\")\nax[1, 1].set_axis_off()\nplt.show()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-visualize-the-effect-of-the-convolution-operation-3",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-visualize-the-effect-of-the-convolution-operation-3",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Visualize the effect of the convolution operation",
    "text": "Exercise: Visualize the effect of the convolution operation\n\nModify the weights of the convolution layer.\n\n\nconv_1 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, padding=0, bias=False)\n\nconv_1.weight.data[:] = torch.FloatTensor([\n  [\n    [\n      [0, 0, 0],\n      [0, 0, 0],\n      [0, 0, 0],\n    ],\n    [\n      [0, 0, 0],\n      [0, 1, 0],\n      [0, 0, 0],\n    ],\n    [\n      [0, 0, 0],\n      [0, 0, 0],\n      [0, 0, 0],\n    ],\n  ]\n])"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-visualize-the-effect-of-the-convolution-operation-4",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-visualize-the-effect-of-the-convolution-operation-4",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Visualize the effect of the convolution operation",
    "text": "Exercise: Visualize the effect of the convolution operation\n\nVisualize the effects after changing the values.\n\n\nfx = conv_1(x)\n\nfig, ax = plt.subplots(1, 2)\nax[0].imshow(x[0].permute(1, 2, 0))\nax[1].imshow(fx.detach()[0].permute(1, 2, 0))\nplt.show()\n\n\n\n\n\n\n\n\nExperiment with different values and shapes of the kernel https://en.wikipedia.org/wiki/Kernel_(image_processing)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-visualize-the-effect-of-the-convolution-operation-5",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-visualize-the-effect-of-the-convolution-operation-5",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Visualize the effect of the convolution operation",
    "text": "Exercise: Visualize the effect of the convolution operation\n\nModify the weights of the convolution layer.\n\n\nconv_1 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, padding=0, bias=False)\n\nconv_1.weight.data[:] = torch.FloatTensor([\n  [[[0, -1, 0], [-1, 5, -1], [0, -1, 0]],\n   [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n   [[0, 0, 0], [0, 0, 0], [0, 0, 0]]]\n])\n\nfx = conv_1(x)\n\nfig, ax = plt.subplots(1, 2)\nax[0].imshow(x[0].permute(1, 2, 0))\nax[1].imshow(fx.detach()[0, 0], cmap=\"gray\")\nplt.show()\n\n\n\n\n\n\n\n\nExperiment with different values and shapes of the kernel https://en.wikipedia.org/wiki/Kernel_(image_processing)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-visualize-the-effect-of-the-convolution-operation-6",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-visualize-the-effect-of-the-convolution-operation-6",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Visualize the effect of the convolution operation",
    "text": "Exercise: Visualize the effect of the convolution operation\n\nModify the weights of the convolution layer.\n\n\nconv_1 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, padding=0, bias=False)\n\nconv_1.weight.data[:] = torch.FloatTensor([\n  [[[1, 0, -1], [1, 0, -1], [1, 0, -1]],\n   [[1, 0, -1], [1, 0, -1], [1, 0, -1]],\n   [[1, 0, -1], [1, 0, -1], [1, 0, -1]]]\n])\n\nfx = conv_1(x)\n\nfig, ax = plt.subplots(1, 2)\nax[0].imshow(x[0].permute(1, 2, 0))\nax[1].imshow(fx.detach()[0, 0], cmap=\"gray\")\nplt.show()\n\n\n\n\n\n\n\n\nExperiment with different values and shapes of the kernel https://en.wikipedia.org/wiki/Kernel_(image_processing)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#examples-of-popular-deep-learning-models-in-computer-vision",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#examples-of-popular-deep-learning-models-in-computer-vision",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Examples of popular Deep Learning models in computer vision",
    "text": "Examples of popular Deep Learning models in computer vision\n\nResidual Neural Networks\n\n\nHe, Kaiming et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015): 770-778."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#examples-of-popular-deep-learning-models-in-computer-vision-1",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#examples-of-popular-deep-learning-models-in-computer-vision-1",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Examples of popular Deep Learning models in computer vision",
    "text": "Examples of popular Deep Learning models in computer vision\n\nInception (v3)\n\n\nSzegedy, Christian et al. “Going deeper with convolutions.” 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2014): 1-9."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#examples-of-popular-deep-learning-models-in-computer-vision-2",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#examples-of-popular-deep-learning-models-in-computer-vision-2",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Examples of popular Deep Learning models in computer vision",
    "text": "Examples of popular Deep Learning models in computer vision\n\nU-Net (cell segmentation)\n\n\nRonneberger, Olaf et al. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” ArXiv abs/1505.04597 (2015): n. pag."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#examples-of-popular-deep-learning-models-in-computer-vision-3",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#examples-of-popular-deep-learning-models-in-computer-vision-3",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Examples of popular Deep Learning models in computer vision",
    "text": "Examples of popular Deep Learning models in computer vision\n\nVision Transformer (classification and segmentation)\n\n\nDosovitskiy, Alexey et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv abs/2010.11929 (2020): n. pag.\nSegment Anything Model (SAM)\nMicro-SAM"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#examples-of-popular-deep-learning-models-in-computer-vision-4",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#examples-of-popular-deep-learning-models-in-computer-vision-4",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Examples of popular Deep Learning models in computer vision",
    "text": "Examples of popular Deep Learning models in computer vision\n\nLeNet-5 for handwritten digits classification (LeCun et al.)\n\n\nLeCun, Yann et al. “Gradient-based learning applied to document recognition.” Proc. IEEE 86 (1998): 2278-2324.By Daniel Voigt Godoy - https://github.com/dvgodoy/dl-visuals/, CC BY 4.0, Link"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-implement-and-train-the-letnet-5-model-with-pytorch",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-implement-and-train-the-letnet-5-model-with-pytorch",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Implement and train the LetNet-5 model with PyTorch",
    "text": "Exercise: Implement and train the LetNet-5 model with PyTorch\n\nBuild the convolutional neural network using nn.Sequential, and the nn.ReLU() activation function.\n\n\nlenet_clf = nn.Sequential(\n    nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, bias=True),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2),\n    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, bias=True),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2),\n    nn.Flatten(),\n    nn.Linear(in_features=16*5*5, out_features=120, bias=True),\n    nn.ReLU(),\n    nn.Linear(in_features=120, out_features=84, bias=True),\n    nn.ReLU(),\n    nn.Linear(in_features=84, out_features=100, bias=True),\n)\n\n\n\n\n\n\n\nNote\n\n\nPooling layers are used to downsample feature maps to summarize information from large regions."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-implement-and-train-the-letnet-5-model-with-pytorch-1",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-implement-and-train-the-letnet-5-model-with-pytorch-1",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Implement and train the LetNet-5 model with PyTorch",
    "text": "Exercise: Implement and train the LetNet-5 model with PyTorch\n\nTest our implementation.\n\n\ny_hat = lenet_clf(x)\n\ntype(y_hat), y_hat.dtype, y_hat.shape, y_hat.min(), y_hat.max()\n\n(torch.Tensor,\n torch.float32,\n torch.Size([128, 100]),\n tensor(-0.1370, grad_fn=&lt;MinBackward1&gt;),\n tensor(0.1532, grad_fn=&lt;MaxBackward1&gt;))"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-implement-and-train-the-letnet-5-model-with-pytorch-2",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-implement-and-train-the-letnet-5-model-with-pytorch-2",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Implement and train the LetNet-5 model with PyTorch",
    "text": "Exercise: Implement and train the LetNet-5 model with PyTorch\n\nTrain the model to classify images from CIFAR-100.\n\n\nnum_epochs = 10\ntrain_loss = []\nval_loss = []\n\nif torch.cuda.is_available():\n  lenet_clf.cuda()\n\noptimizer = optim.SGD(lenet_clf.parameters(), lr=0.01)\n\nfor e in range(num_epochs):\n  train_loss_avg = 0\n  total_train_samples = 0\n\n  lenet_clf.train()\n  for x, y in cifar_train_dl:\n    optimizer.zero_grad()\n\n    if torch.cuda.is_available():\n      x = x.cuda()\n    \n    y_hat = lenet_clf( x ).cpu()\n\n    loss = loss_fun(y_hat, y)\n\n    train_loss_avg += loss.item() * len(x)\n    total_train_samples += len(x)\n\n    loss.backward()\n\n    optimizer.step()\n\n  train_loss_avg /= total_train_samples\n  train_loss.append(train_loss_avg)\n\n  val_loss_avg = 0\n  total_val_samples = 0\n\n  lenet_clf.eval()\n  with torch.no_grad():\n    for x, y in cifar_val_dl:\n      if torch.cuda.is_available():\n        x = x.cuda()\n      \n      y_hat = lenet_clf( x ).cpu()\n      loss = loss_fun(y_hat, y)\n\n      val_loss_avg += loss.item() * len(x)\n      total_val_samples += len(x)\n\n  val_loss_avg /= total_val_samples\n  val_loss.append(val_loss_avg)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-implement-and-train-the-letnet-5-model-with-pytorch-3",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-implement-and-train-the-letnet-5-model-with-pytorch-3",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Implement and train the LetNet-5 model with PyTorch",
    "text": "Exercise: Implement and train the LetNet-5 model with PyTorch\n\nPlot the average train and validation losses\n\n\nplt.plot(train_loss, \"b-\", label=\"Average training loss\")\nplt.plot(val_loss, \"r-\", label=\"Average validation loss\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_1.html#exercise-implement-and-train-the-letnet-5-model-with-pytorch-4",
    "href": "notes/Adv_ML_Python_presentation_1_1.html#exercise-implement-and-train-the-letnet-5-model-with-pytorch-4",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 1)",
    "section": "Exercise: Implement and train the LetNet-5 model with PyTorch",
    "text": "Exercise: Implement and train the LetNet-5 model with PyTorch\n\nCompute the average accuracy for the Validation and Test sets.\n\n\nlenet_clf.eval()\n\nval_acc_metric = Accuracy(task=\"multiclass\", num_classes=100)\ntest_acc_metric = Accuracy(task=\"multiclass\", num_classes=100)\ntrain_acc_metric = Accuracy(task=\"multiclass\", num_classes=100)\n\nwith torch.no_grad():\n  for x, y in cifar_train_dl:\n    if torch.cuda.is_available():\n      x = x.cuda()\n    y_hat = lenet_clf( x ).cpu()\n    train_acc_metric(y_hat.softmax(dim=1), y)\n\n  train_acc = train_acc_metric.compute()\n\n  for x, y in cifar_val_dl:\n    if torch.cuda.is_available():\n      x = x.cuda()\n    y_hat = lenet_clf( x ).cpu()\n    val_acc_metric(y_hat.softmax(dim=1), y)\n\n  val_acc = val_acc_metric.compute()\n\n  for x, y in cifar_test_dl:\n    if torch.cuda.is_available():\n      x = x.cuda()\n    y_hat = lenet_clf( x ).cpu()\n    test_acc_metric(y_hat.softmax(dim=1), y)\n\n  test_acc = test_acc_metric.compute()\n\nprint(f\"Training acc={train_acc}\")\nprint(f\"Validation acc={val_acc}\")\nprint(f\"Test acc={test_acc}\")\n\ntrain_acc_metric.reset()\nval_acc_metric.reset()\ntest_acc_metric.reset()\n\nTraining acc=0.010499999858438969\nValidation acc=0.00800000037997961\nTest acc=0.009999999776482582"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_3.html#review-the-architecture-of-a-vision-transformer-vit",
    "href": "notes/Adv_ML_Python_presentation_1_3.html#review-the-architecture-of-a-vision-transformer-vit",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 3)",
    "section": "Review the architecture of a Vision Transformer (ViT)",
    "text": "Review the architecture of a Vision Transformer (ViT)\n\n\n\nDosovitskiy, Alexey et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv abs/2010.11929 (2020)\n\n\n\nhttps://docs.pytorch.org/vision/stable/models/vision_transformer.html\nReview the Attention mechanism in transformer models\n\n\n\n\n\nVaswani, Ashish et al. “Attention is All you Need.” Neural Information Processing Systems (2017)."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_3.html#exercise-use-a-pre-trained-vit-model-to-classify-images",
    "href": "notes/Adv_ML_Python_presentation_1_3.html#exercise-use-a-pre-trained-vit-model-to-classify-images",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 3)",
    "section": "Exercise: Use a pre-trained ViT model to classify images",
    "text": "Exercise: Use a pre-trained ViT model to classify images\n\nImport the pre-trained weights of the Inception V3 model from models.inception_v3\n\n\nimport torch\nfrom torchvision import models\n\ntransformer_weights = models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1\n\ntransformer_weights.meta\n\n{'categories': ['tench',\n  'goldfish',\n  'great white shark',\n  'tiger shark',\n  'hammerhead',\n  'electric ray',\n  'stingray',\n  'cock',\n  'hen',\n  'ostrich',\n  'brambling',\n  'goldfinch',\n  'house finch',\n  'junco',\n  'indigo bunting',\n  'robin',\n  'bulbul',\n  'jay',\n  'magpie',\n  'chickadee',\n  'water ouzel',\n  'kite',\n  'bald eagle',\n  'vulture',\n  'great grey owl',\n  'European fire salamander',\n  'common newt',\n  'eft',\n  'spotted salamander',\n  'axolotl',\n  'bullfrog',\n  'tree frog',\n  'tailed frog',\n  'loggerhead',\n  'leatherback turtle',\n  'mud turtle',\n  'terrapin',\n  'box turtle',\n  'banded gecko',\n  'common iguana',\n  'American chameleon',\n  'whiptail',\n  'agama',\n  'frilled lizard',\n  'alligator lizard',\n  'Gila monster',\n  'green lizard',\n  'African chameleon',\n  'Komodo dragon',\n  'African crocodile',\n  'American alligator',\n  'triceratops',\n  'thunder snake',\n  'ringneck snake',\n  'hognose snake',\n  'green snake',\n  'king snake',\n  'garter snake',\n  'water snake',\n  'vine snake',\n  'night snake',\n  'boa constrictor',\n  'rock python',\n  'Indian cobra',\n  'green mamba',\n  'sea snake',\n  'horned viper',\n  'diamondback',\n  'sidewinder',\n  'trilobite',\n  'harvestman',\n  'scorpion',\n  'black and gold garden spider',\n  'barn spider',\n  'garden spider',\n  'black widow',\n  'tarantula',\n  'wolf spider',\n  'tick',\n  'centipede',\n  'black grouse',\n  'ptarmigan',\n  'ruffed grouse',\n  'prairie chicken',\n  'peacock',\n  'quail',\n  'partridge',\n  'African grey',\n  'macaw',\n  'sulphur-crested cockatoo',\n  'lorikeet',\n  'coucal',\n  'bee eater',\n  'hornbill',\n  'hummingbird',\n  'jacamar',\n  'toucan',\n  'drake',\n  'red-breasted merganser',\n  'goose',\n  'black swan',\n  'tusker',\n  'echidna',\n  'platypus',\n  'wallaby',\n  'koala',\n  'wombat',\n  'jellyfish',\n  'sea anemone',\n  'brain coral',\n  'flatworm',\n  'nematode',\n  'conch',\n  'snail',\n  'slug',\n  'sea slug',\n  'chiton',\n  'chambered nautilus',\n  'Dungeness crab',\n  'rock crab',\n  'fiddler crab',\n  'king crab',\n  'American lobster',\n  'spiny lobster',\n  'crayfish',\n  'hermit crab',\n  'isopod',\n  'white stork',\n  'black stork',\n  'spoonbill',\n  'flamingo',\n  'little blue heron',\n  'American egret',\n  'bittern',\n  'crane bird',\n  'limpkin',\n  'European gallinule',\n  'American coot',\n  'bustard',\n  'ruddy turnstone',\n  'red-backed sandpiper',\n  'redshank',\n  'dowitcher',\n  'oystercatcher',\n  'pelican',\n  'king penguin',\n  'albatross',\n  'grey whale',\n  'killer whale',\n  'dugong',\n  'sea lion',\n  'Chihuahua',\n  'Japanese spaniel',\n  'Maltese dog',\n  'Pekinese',\n  'Shih-Tzu',\n  'Blenheim spaniel',\n  'papillon',\n  'toy terrier',\n  'Rhodesian ridgeback',\n  'Afghan hound',\n  'basset',\n  'beagle',\n  'bloodhound',\n  'bluetick',\n  'black-and-tan coonhound',\n  'Walker hound',\n  'English foxhound',\n  'redbone',\n  'borzoi',\n  'Irish wolfhound',\n  'Italian greyhound',\n  'whippet',\n  'Ibizan hound',\n  'Norwegian elkhound',\n  'otterhound',\n  'Saluki',\n  'Scottish deerhound',\n  'Weimaraner',\n  'Staffordshire bullterrier',\n  'American Staffordshire terrier',\n  'Bedlington terrier',\n  'Border terrier',\n  'Kerry blue terrier',\n  'Irish terrier',\n  'Norfolk terrier',\n  'Norwich terrier',\n  'Yorkshire terrier',\n  'wire-haired fox terrier',\n  'Lakeland terrier',\n  'Sealyham terrier',\n  'Airedale',\n  'cairn',\n  'Australian terrier',\n  'Dandie Dinmont',\n  'Boston bull',\n  'miniature schnauzer',\n  'giant schnauzer',\n  'standard schnauzer',\n  'Scotch terrier',\n  'Tibetan terrier',\n  'silky terrier',\n  'soft-coated wheaten terrier',\n  'West Highland white terrier',\n  'Lhasa',\n  'flat-coated retriever',\n  'curly-coated retriever',\n  'golden retriever',\n  'Labrador retriever',\n  'Chesapeake Bay retriever',\n  'German short-haired pointer',\n  'vizsla',\n  'English setter',\n  'Irish setter',\n  'Gordon setter',\n  'Brittany spaniel',\n  'clumber',\n  'English springer',\n  'Welsh springer spaniel',\n  'cocker spaniel',\n  'Sussex spaniel',\n  'Irish water spaniel',\n  'kuvasz',\n  'schipperke',\n  'groenendael',\n  'malinois',\n  'briard',\n  'kelpie',\n  'komondor',\n  'Old English sheepdog',\n  'Shetland sheepdog',\n  'collie',\n  'Border collie',\n  'Bouvier des Flandres',\n  'Rottweiler',\n  'German shepherd',\n  'Doberman',\n  'miniature pinscher',\n  'Greater Swiss Mountain dog',\n  'Bernese mountain dog',\n  'Appenzeller',\n  'EntleBucher',\n  'boxer',\n  'bull mastiff',\n  'Tibetan mastiff',\n  'French bulldog',\n  'Great Dane',\n  'Saint Bernard',\n  'Eskimo dog',\n  'malamute',\n  'Siberian husky',\n  'dalmatian',\n  'affenpinscher',\n  'basenji',\n  'pug',\n  'Leonberg',\n  'Newfoundland',\n  'Great Pyrenees',\n  'Samoyed',\n  'Pomeranian',\n  'chow',\n  'keeshond',\n  'Brabancon griffon',\n  'Pembroke',\n  'Cardigan',\n  'toy poodle',\n  'miniature poodle',\n  'standard poodle',\n  'Mexican hairless',\n  'timber wolf',\n  'white wolf',\n  'red wolf',\n  'coyote',\n  'dingo',\n  'dhole',\n  'African hunting dog',\n  'hyena',\n  'red fox',\n  'kit fox',\n  'Arctic fox',\n  'grey fox',\n  'tabby',\n  'tiger cat',\n  'Persian cat',\n  'Siamese cat',\n  'Egyptian cat',\n  'cougar',\n  'lynx',\n  'leopard',\n  'snow leopard',\n  'jaguar',\n  'lion',\n  'tiger',\n  'cheetah',\n  'brown bear',\n  'American black bear',\n  'ice bear',\n  'sloth bear',\n  'mongoose',\n  'meerkat',\n  'tiger beetle',\n  'ladybug',\n  'ground beetle',\n  'long-horned beetle',\n  'leaf beetle',\n  'dung beetle',\n  'rhinoceros beetle',\n  'weevil',\n  'fly',\n  'bee',\n  'ant',\n  'grasshopper',\n  'cricket',\n  'walking stick',\n  'cockroach',\n  'mantis',\n  'cicada',\n  'leafhopper',\n  'lacewing',\n  'dragonfly',\n  'damselfly',\n  'admiral',\n  'ringlet',\n  'monarch',\n  'cabbage butterfly',\n  'sulphur butterfly',\n  'lycaenid',\n  'starfish',\n  'sea urchin',\n  'sea cucumber',\n  'wood rabbit',\n  'hare',\n  'Angora',\n  'hamster',\n  'porcupine',\n  'fox squirrel',\n  'marmot',\n  'beaver',\n  'guinea pig',\n  'sorrel',\n  'zebra',\n  'hog',\n  'wild boar',\n  'warthog',\n  'hippopotamus',\n  'ox',\n  'water buffalo',\n  'bison',\n  'ram',\n  'bighorn',\n  'ibex',\n  'hartebeest',\n  'impala',\n  'gazelle',\n  'Arabian camel',\n  'llama',\n  'weasel',\n  'mink',\n  'polecat',\n  'black-footed ferret',\n  'otter',\n  'skunk',\n  'badger',\n  'armadillo',\n  'three-toed sloth',\n  'orangutan',\n  'gorilla',\n  'chimpanzee',\n  'gibbon',\n  'siamang',\n  'guenon',\n  'patas',\n  'baboon',\n  'macaque',\n  'langur',\n  'colobus',\n  'proboscis monkey',\n  'marmoset',\n  'capuchin',\n  'howler monkey',\n  'titi',\n  'spider monkey',\n  'squirrel monkey',\n  'Madagascar cat',\n  'indri',\n  'Indian elephant',\n  'African elephant',\n  'lesser panda',\n  'giant panda',\n  'barracouta',\n  'eel',\n  'coho',\n  'rock beauty',\n  'anemone fish',\n  'sturgeon',\n  'gar',\n  'lionfish',\n  'puffer',\n  'abacus',\n  'abaya',\n  'academic gown',\n  'accordion',\n  'acoustic guitar',\n  'aircraft carrier',\n  'airliner',\n  'airship',\n  'altar',\n  'ambulance',\n  'amphibian',\n  'analog clock',\n  'apiary',\n  'apron',\n  'ashcan',\n  'assault rifle',\n  'backpack',\n  'bakery',\n  'balance beam',\n  'balloon',\n  'ballpoint',\n  'Band Aid',\n  'banjo',\n  'bannister',\n  'barbell',\n  'barber chair',\n  'barbershop',\n  'barn',\n  'barometer',\n  'barrel',\n  'barrow',\n  'baseball',\n  'basketball',\n  'bassinet',\n  'bassoon',\n  'bathing cap',\n  'bath towel',\n  'bathtub',\n  'beach wagon',\n  'beacon',\n  'beaker',\n  'bearskin',\n  'beer bottle',\n  'beer glass',\n  'bell cote',\n  'bib',\n  'bicycle-built-for-two',\n  'bikini',\n  'binder',\n  'binoculars',\n  'birdhouse',\n  'boathouse',\n  'bobsled',\n  'bolo tie',\n  'bonnet',\n  'bookcase',\n  'bookshop',\n  'bottlecap',\n  'bow',\n  'bow tie',\n  'brass',\n  'brassiere',\n  'breakwater',\n  'breastplate',\n  'broom',\n  'bucket',\n  'buckle',\n  'bulletproof vest',\n  'bullet train',\n  'butcher shop',\n  'cab',\n  'caldron',\n  'candle',\n  'cannon',\n  'canoe',\n  'can opener',\n  'cardigan',\n  'car mirror',\n  'carousel',\n  \"carpenter's kit\",\n  'carton',\n  'car wheel',\n  'cash machine',\n  'cassette',\n  'cassette player',\n  'castle',\n  'catamaran',\n  'CD player',\n  'cello',\n  'cellular telephone',\n  'chain',\n  'chainlink fence',\n  'chain mail',\n  'chain saw',\n  'chest',\n  'chiffonier',\n  'chime',\n  'china cabinet',\n  'Christmas stocking',\n  'church',\n  'cinema',\n  'cleaver',\n  'cliff dwelling',\n  'cloak',\n  'clog',\n  'cocktail shaker',\n  'coffee mug',\n  'coffeepot',\n  'coil',\n  'combination lock',\n  'computer keyboard',\n  'confectionery',\n  'container ship',\n  'convertible',\n  'corkscrew',\n  'cornet',\n  'cowboy boot',\n  'cowboy hat',\n  'cradle',\n  'crane',\n  'crash helmet',\n  'crate',\n  'crib',\n  'Crock Pot',\n  'croquet ball',\n  'crutch',\n  'cuirass',\n  'dam',\n  'desk',\n  'desktop computer',\n  'dial telephone',\n  'diaper',\n  'digital clock',\n  'digital watch',\n  'dining table',\n  'dishrag',\n  'dishwasher',\n  'disk brake',\n  'dock',\n  'dogsled',\n  'dome',\n  'doormat',\n  'drilling platform',\n  'drum',\n  'drumstick',\n  'dumbbell',\n  'Dutch oven',\n  'electric fan',\n  'electric guitar',\n  'electric locomotive',\n  'entertainment center',\n  'envelope',\n  'espresso maker',\n  'face powder',\n  'feather boa',\n  'file',\n  'fireboat',\n  'fire engine',\n  'fire screen',\n  'flagpole',\n  'flute',\n  'folding chair',\n  'football helmet',\n  'forklift',\n  'fountain',\n  'fountain pen',\n  'four-poster',\n  'freight car',\n  'French horn',\n  'frying pan',\n  'fur coat',\n  'garbage truck',\n  'gasmask',\n  'gas pump',\n  'goblet',\n  'go-kart',\n  'golf ball',\n  'golfcart',\n  'gondola',\n  'gong',\n  'gown',\n  'grand piano',\n  'greenhouse',\n  'grille',\n  'grocery store',\n  'guillotine',\n  'hair slide',\n  'hair spray',\n  'half track',\n  'hammer',\n  'hamper',\n  'hand blower',\n  'hand-held computer',\n  'handkerchief',\n  'hard disc',\n  'harmonica',\n  'harp',\n  'harvester',\n  'hatchet',\n  'holster',\n  'home theater',\n  'honeycomb',\n  'hook',\n  'hoopskirt',\n  'horizontal bar',\n  'horse cart',\n  'hourglass',\n  'iPod',\n  'iron',\n  \"jack-o'-lantern\",\n  'jean',\n  'jeep',\n  'jersey',\n  'jigsaw puzzle',\n  'jinrikisha',\n  'joystick',\n  'kimono',\n  'knee pad',\n  'knot',\n  'lab coat',\n  'ladle',\n  'lampshade',\n  'laptop',\n  'lawn mower',\n  'lens cap',\n  'letter opener',\n  'library',\n  'lifeboat',\n  'lighter',\n  'limousine',\n  'liner',\n  'lipstick',\n  'Loafer',\n  'lotion',\n  'loudspeaker',\n  'loupe',\n  'lumbermill',\n  'magnetic compass',\n  'mailbag',\n  'mailbox',\n  'maillot',\n  'maillot tank suit',\n  'manhole cover',\n  'maraca',\n  'marimba',\n  'mask',\n  'matchstick',\n  'maypole',\n  'maze',\n  'measuring cup',\n  'medicine chest',\n  'megalith',\n  'microphone',\n  'microwave',\n  'military uniform',\n  'milk can',\n  'minibus',\n  'miniskirt',\n  'minivan',\n  'missile',\n  'mitten',\n  'mixing bowl',\n  'mobile home',\n  'Model T',\n  'modem',\n  'monastery',\n  'monitor',\n  'moped',\n  'mortar',\n  'mortarboard',\n  'mosque',\n  'mosquito net',\n  'motor scooter',\n  'mountain bike',\n  'mountain tent',\n  'mouse',\n  'mousetrap',\n  'moving van',\n  'muzzle',\n  'nail',\n  'neck brace',\n  'necklace',\n  'nipple',\n  'notebook',\n  'obelisk',\n  'oboe',\n  'ocarina',\n  'odometer',\n  'oil filter',\n  'organ',\n  'oscilloscope',\n  'overskirt',\n  'oxcart',\n  'oxygen mask',\n  'packet',\n  'paddle',\n  'paddlewheel',\n  'padlock',\n  'paintbrush',\n  'pajama',\n  'palace',\n  'panpipe',\n  'paper towel',\n  'parachute',\n  'parallel bars',\n  'park bench',\n  'parking meter',\n  'passenger car',\n  'patio',\n  'pay-phone',\n  'pedestal',\n  'pencil box',\n  'pencil sharpener',\n  'perfume',\n  'Petri dish',\n  'photocopier',\n  'pick',\n  'pickelhaube',\n  'picket fence',\n  'pickup',\n  'pier',\n  'piggy bank',\n  'pill bottle',\n  'pillow',\n  'ping-pong ball',\n  'pinwheel',\n  'pirate',\n  'pitcher',\n  'plane',\n  'planetarium',\n  'plastic bag',\n  'plate rack',\n  'plow',\n  'plunger',\n  'Polaroid camera',\n  'pole',\n  'police van',\n  'poncho',\n  'pool table',\n  'pop bottle',\n  'pot',\n  \"potter's wheel\",\n  'power drill',\n  'prayer rug',\n  'printer',\n  'prison',\n  'projectile',\n  'projector',\n  'puck',\n  'punching bag',\n  'purse',\n  'quill',\n  'quilt',\n  'racer',\n  'racket',\n  'radiator',\n  'radio',\n  'radio telescope',\n  'rain barrel',\n  'recreational vehicle',\n  'reel',\n  'reflex camera',\n  'refrigerator',\n  'remote control',\n  'restaurant',\n  'revolver',\n  'rifle',\n  'rocking chair',\n  'rotisserie',\n  'rubber eraser',\n  'rugby ball',\n  'rule',\n  'running shoe',\n  'safe',\n  'safety pin',\n  'saltshaker',\n  'sandal',\n  'sarong',\n  'sax',\n  'scabbard',\n  'scale',\n  'school bus',\n  'schooner',\n  'scoreboard',\n  'screen',\n  'screw',\n  'screwdriver',\n  'seat belt',\n  'sewing machine',\n  'shield',\n  'shoe shop',\n  'shoji',\n  'shopping basket',\n  'shopping cart',\n  'shovel',\n  'shower cap',\n  'shower curtain',\n  'ski',\n  'ski mask',\n  'sleeping bag',\n  'slide rule',\n  'sliding door',\n  'slot',\n  'snorkel',\n  'snowmobile',\n  'snowplow',\n  'soap dispenser',\n  'soccer ball',\n  'sock',\n  'solar dish',\n  'sombrero',\n  'soup bowl',\n  'space bar',\n  'space heater',\n  'space shuttle',\n  'spatula',\n  'speedboat',\n  'spider web',\n  'spindle',\n  'sports car',\n  'spotlight',\n  'stage',\n  'steam locomotive',\n  'steel arch bridge',\n  'steel drum',\n  'stethoscope',\n  'stole',\n  'stone wall',\n  'stopwatch',\n  'stove',\n  'strainer',\n  'streetcar',\n  'stretcher',\n  'studio couch',\n  'stupa',\n  'submarine',\n  'suit',\n  'sundial',\n  'sunglass',\n  'sunglasses',\n  'sunscreen',\n  'suspension bridge',\n  'swab',\n  'sweatshirt',\n  'swimming trunks',\n  'swing',\n  'switch',\n  'syringe',\n  'table lamp',\n  'tank',\n  'tape player',\n  'teapot',\n  'teddy',\n  'television',\n  'tennis ball',\n  'thatch',\n  'theater curtain',\n  'thimble',\n  'thresher',\n  'throne',\n  'tile roof',\n  'toaster',\n  'tobacco shop',\n  'toilet seat',\n  'torch',\n  'totem pole',\n  'tow truck',\n  'toyshop',\n  'tractor',\n  'trailer truck',\n  'tray',\n  'trench coat',\n  'tricycle',\n  'trimaran',\n  'tripod',\n  'triumphal arch',\n  'trolleybus',\n  'trombone',\n  'tub',\n  'turnstile',\n  'typewriter keyboard',\n  'umbrella',\n  'unicycle',\n  'upright',\n  'vacuum',\n  'vase',\n  'vault',\n  'velvet',\n  'vending machine',\n  'vestment',\n  'viaduct',\n  'violin',\n  'volleyball',\n  'waffle iron',\n  'wall clock',\n  'wallet',\n  'wardrobe',\n  'warplane',\n  'washbasin',\n  'washer',\n  'water bottle',\n  'water jug',\n  'water tower',\n  'whiskey jug',\n  'whistle',\n  'wig',\n  'window screen',\n  'window shade',\n  'Windsor tie',\n  'wine bottle',\n  'wing',\n  'wok',\n  'wooden spoon',\n  'wool',\n  'worm fence',\n  'wreck',\n  'yawl',\n  'yurt',\n  'web site',\n  'comic book',\n  'crossword puzzle',\n  'street sign',\n  'traffic light',\n  'book jacket',\n  'menu',\n  'plate',\n  'guacamole',\n  'consomme',\n  'hot pot',\n  'trifle',\n  'ice cream',\n  'ice lolly',\n  'French loaf',\n  'bagel',\n  'pretzel',\n  'cheeseburger',\n  'hotdog',\n  'mashed potato',\n  'head cabbage',\n  'broccoli',\n  'cauliflower',\n  'zucchini',\n  'spaghetti squash',\n  'acorn squash',\n  'butternut squash',\n  'cucumber',\n  'artichoke',\n  'bell pepper',\n  'cardoon',\n  'mushroom',\n  'Granny Smith',\n  'strawberry',\n  'orange',\n  'lemon',\n  'fig',\n  'pineapple',\n  'banana',\n  'jackfruit',\n  'custard apple',\n  'pomegranate',\n  'hay',\n  'carbonara',\n  'chocolate sauce',\n  'dough',\n  'meat loaf',\n  'pizza',\n  'potpie',\n  'burrito',\n  'red wine',\n  'espresso',\n  'cup',\n  'eggnog',\n  'alp',\n  'bubble',\n  'cliff',\n  'coral reef',\n  'geyser',\n  'lakeside',\n  'promontory',\n  'sandbar',\n  'seashore',\n  'valley',\n  'volcano',\n  'ballplayer',\n  'groom',\n  'scuba diver',\n  'rapeseed',\n  'daisy',\n  \"yellow lady's slipper\",\n  'corn',\n  'acorn',\n  'hip',\n  'buckeye',\n  'coral fungus',\n  'agaric',\n  'gyromitra',\n  'stinkhorn',\n  'earthstar',\n  'hen-of-the-woods',\n  'bolete',\n  'ear',\n  'toilet tissue'],\n 'recipe': 'https://github.com/facebookresearch/SWAG',\n 'license': 'https://github.com/facebookresearch/SWAG/blob/main/LICENSE',\n 'num_params': 86859496,\n 'min_size': (384, 384),\n '_metrics': {'ImageNet-1K': {'acc@1': 85.304, 'acc@5': 97.65}},\n '_ops': 55.484,\n '_file_size': 331.398,\n '_docs': '\\n                These weights are learnt via transfer learning by end-to-end fine-tuning the original\\n                `SWAG &lt;https://arxiv.org/abs/2201.08371&gt;`_ weights on ImageNet-1K data.\\n            '}\n\n\n\nStore the categories in a variable to use them later\n\n\ncategories = transformer_weights.meta[\"categories\"]\n\n\n\n\n\n\n\nTip\n\n\nMore info about Vision Transformer implementation in torchvision here"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_3.html#inspect-the-self-attention-operations-of-the-vit",
    "href": "notes/Adv_ML_Python_presentation_1_3.html#inspect-the-self-attention-operations-of-the-vit",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 3)",
    "section": "Inspect the self-attention operations of the ViT",
    "text": "Inspect the self-attention operations of the ViT\n\ndl_model.encoder.layers[-1]\n\nEncoderBlock(\n  (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  (self_attention): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (dropout): Dropout(p=0.0, inplace=False)\n  (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  (mlp): MLPBlock(\n    (0): Linear(in_features=768, out_features=3072, bias=True)\n    (1): GELU(approximate='none')\n    (2): Dropout(p=0.0, inplace=False)\n    (3): Linear(in_features=3072, out_features=768, bias=True)\n    (4): Dropout(p=0.0, inplace=False)\n  )\n)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_3.html#redefine-some-of-transformer-operations-to-enable-storing-the-attention-weights",
    "href": "notes/Adv_ML_Python_presentation_1_3.html#redefine-some-of-transformer-operations-to-enable-storing-the-attention-weights",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 3)",
    "section": "Redefine some of transformer operations to enable storing the attention weights",
    "text": "Redefine some of transformer operations to enable storing the attention weights\n\nfrom functools import partial\nfrom typing import Callable\nfrom timm.models.vision_transformer import Attention\n\nclass EncoderBlockAttnMap(models.vision_transformer.EncoderBlock):\n    \"\"\"Transformer encoder block.\"\"\"\n\n    def __init__(self,\n                 num_heads: int,\n                 hidden_dim: int,\n                 mlp_dim: int,\n                 dropout: float,\n                 attention_dropout: float,\n                 norm_layer: Callable[..., torch.nn.Module] = partial(torch.nn.LayerNorm, eps=1e-6)):\n        # The definition is the same, only the forward function changes &lt;------------------------------------\n        super(EncoderBlockAttnMap, self).__init__(num_heads, hidden_dim, mlp_dim, dropout, attention_dropout, norm_layer)\n        self.self_attention = Attention(hidden_dim, num_heads, attn_drop=attention_dropout, proj_drop=0.0, norm_layer=norm_layer, qkv_bias=True)\n\n    def forward(self, input: torch.Tensor):\n        # with torch.autograd.graph.save_on_cpu(pin_memory=True):\n        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n        x = self.ln_1(input)\n\n        # Modify this line, so we get the attention map from the self attention modules &lt;--------------------\n        x = self.self_attention(x)\n\n        x = self.dropout(x)\n        x = x + input\n\n        y = self.ln_2(x)\n        y = self.mlp(y)\n\n        # Return the attention map along with the encoder output &lt;-------------------------------------------\n        return x + y\n\n\nfrom collections import OrderedDict\n\nclass EncoderAttnMap(models.vision_transformer.Encoder):\n    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n\n    def __init__(\n        self,\n        seq_length: int,\n        num_layers: int,\n        num_heads: int,\n        hidden_dim: int,\n        mlp_dim: int,\n        dropout: float,\n        attention_dropout: float,\n        norm_layer: Callable[..., torch.nn.Module] = partial(torch.nn.LayerNorm, eps=1e-6),\n    ):\n        super().__init__(seq_length, num_layers, num_heads, hidden_dim, mlp_dim, dropout, attention_dropout, norm_layer)\n\n        layers: OrderedDict[str, nn.Module] = OrderedDict()\n        for i in range(num_layers):\n            # Use the modified encoder block &lt;---------------------------------------------------------------\n            layers[f\"encoder_layer_{i}\"] = EncoderBlockAttnMap(\n                num_heads,\n                hidden_dim,\n                mlp_dim,\n                dropout,\n                attention_dropout,\n                norm_layer,\n            )\n\n        self.layers = torch.nn.Sequential(layers)\n\n\n# Redefine the classifier head to have access to the attention maps\nclass ViTAttnEnabled(models.vision_transformer.VisionTransformer):\n    \"\"\"Implementation of the classifier head from the ViT-B-16 architecture.\n    \"\"\"\n    def __init__(self, image_size, patch_size=14, num_layers=32, num_heads=16, hidden_dim=1280, mlp_dim=5120, **kwargs):      \n        super(ViTAttnEnabled, self).__init__(\n            image_size,\n            patch_size=patch_size,\n            num_layers=num_layers,\n            num_heads=num_heads,\n            hidden_dim=hidden_dim,\n            mlp_dim=mlp_dim,\n            **kwargs)\n\n        # Change the encoder to the modified ekwargsoder that returns the attention maps &lt;-----------\n        self.encoder = EncoderAttnMap(\n            self.seq_length,\n            num_layers=num_layers,\n            num_heads=num_heads,\n            hidden_dim=hidden_dim,\n            mlp_dim=mlp_dim,\n            dropout=0,\n            attention_dropout=0,\n            norm_layer=partial(torch.nn.LayerNorm, eps=1e-6)\n        )\n\n        self.attentions = []\n        self.attentions_gradients = []\n\n    def get_attention(self, module, input, output):\n        self.attentions.append(output.detach())\n\n    def get_attention_gradients(self, module, grad_input, grad_output):\n        self.attentions_gradients.append(grad_input[0].detach())\n\n    def register_attn_grad_hooks(self):\n        for name, module in self.named_modules():\n            if \"self_attention.norm\" in name:\n                module.register_forward_hook(self.get_attention)\n                module.register_full_backward_hook(self.get_attention_gradients)\n\n    def clear_attentions(self):\n        self.attentions.clear()\n        self.attentions_gradients.clear()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_3.html#use-a-modified-vit-model-that-enables-tracking-its-attention-weights",
    "href": "notes/Adv_ML_Python_presentation_1_3.html#use-a-modified-vit-model-that-enables-tracking-its-attention-weights",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 3)",
    "section": "Use a modified ViT model that enables tracking its attention weights",
    "text": "Use a modified ViT model that enables tracking its attention weights\n\n[] Initialize a ViT with the Vit-B-16 architecture\n\n\nvit_model_self_attn = ViTAttnEnabled(\n        image_size=384,\n        patch_size=16,\n        num_heads=12,\n        num_layers=12,\n        hidden_dim=768,\n        mlp_dim=3072)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_3.html#apply-the-vit-class-prediction-on-an-image-and-compute-the-corresponding-attention-map",
    "href": "notes/Adv_ML_Python_presentation_1_3.html#apply-the-vit-class-prediction-on-an-image-and-compute-the-corresponding-attention-map",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 3)",
    "section": "Apply the ViT class prediction on an image and compute the corresponding attention map",
    "text": "Apply the ViT class prediction on an image and compute the corresponding attention map\n\nRun the forward operation of the ViT model and the respective backward opreation to compute and store the attention weights\n\n\nsample_im = skimage.io.imread(\"https://r0k.us/graphics/kodak/kodak/kodim20.png\")\n\nvit_model_self_attn.clear_attentions()\n\nsample_x = pipeline(sample_im)\n\nwith torch.autograd.graph.save_on_cpu(pin_memory=True):\n    sample_y = vit_model_self_attn(sample_x[None, ...])\n\nattn_class = torch.argmax(sample_y, dim=1).item()\nattn_class = torch.LongTensor([attn_class])\nattn_class = torch.nn.functional.one_hot(attn_class, num_classes=sample_y.shape[1])\n\nattn_class = torch.sum(attn_class * sample_y)\n\nvit_model_self_attn.zero_grad()\nattn_class.backward()\n\nattn_out = [attn_tensor.clone() for attn_tensor in vit_model_self_attn.attentions]\ngrad_attn_out = [attn_tensor.clone() for attn_tensor in vit_model_self_attn.attentions_gradients]"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_3.html#roll-out-the-attention-maps",
    "href": "notes/Adv_ML_Python_presentation_1_3.html#roll-out-the-attention-maps",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 3)",
    "section": "Roll out the attention maps",
    "text": "Roll out the attention maps\n\nThe attention map can be computed as the accumulation of the attention weigths of each Encoder layer of the Transformer\n\n\nattn_rollout = torch.eye(attn_out[0].size(1))[None, ...]\n\nfor attn_map, attn_grad in zip(attn_out, grad_attn_out):\n    if attn_grad is not None:\n        attn_map = attn_map * attn_grad\n        attn_map[attn_map &lt; 0] = 0\n\n    attn_map, _ = torch.topk(attn_map, 10, dim=-1)\n    attn_map = attn_map.mean(dim=-1)\n\n    # Normalize attention map\n    attn_map = attn_map + torch.eye(attn_map.size(1), device=attn_map.device)[None, ...]\n    attn_map = attn_map / attn_map.sum(dim=-1, keepdim=True)\n\n    attn_rollout = torch.matmul(attn_map, attn_rollout)\n\nattn_rollout.shape\n\ntorch.Size([1, 577, 577])"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_3.html#visualize-the-attention-map-computed-from-the-attention-weights",
    "href": "notes/Adv_ML_Python_presentation_1_3.html#visualize-the-attention-map-computed-from-the-attention-weights",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 3)",
    "section": "Visualize the attention map computed from the attention weights",
    "text": "Visualize the attention map computed from the attention weights\n\n[] Show an overlay of the attention map over the original image\n\n\nplt.imshow(sample_im)\nplt.imshow(attn_rollout, cmap=\"magma\", extent=(0, sample_im.shape[1], sample_im.shape[0], 0), alpha=0.75)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_2.html#workshop-outcomes",
    "href": "notes/Adv_ML_Python_presentation_1_2.html#workshop-outcomes",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 2)",
    "section": "Workshop outcomes",
    "text": "Workshop outcomes\n\nUnderstand the process of training ML models.\nLoad pre-trained ML models and fine-tune them with new data.\nEvaluate the performance of ML models.\nAdapt ML models for different tasks from pre-trained models."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_2.html#load-pre-trained-models-1",
    "href": "notes/Adv_ML_Python_presentation_1_2.html#load-pre-trained-models-1",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 2)",
    "section": "Load pre-trained models",
    "text": "Load pre-trained models\n\nLets use one from the PyTorch’s torchvision module for computer vision\nTry first with the InceptionV3 model."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_2.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images",
    "href": "notes/Adv_ML_Python_presentation_1_2.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 2)",
    "section": "Exercise: Use a pre-trained deep learning model to classify images",
    "text": "Exercise: Use a pre-trained deep learning model to classify images\n\nImport the pre-trained weights of the Inception V3 model from models.inception_v3\n\n\nimport torch\nfrom torchvision import models\n\ninception_weights = models.inception.Inception_V3_Weights.IMAGENET1K_V1\n\ninception_weights.meta\n\n{'num_params': 27161264,\n 'min_size': (75, 75),\n 'categories': ['tench',\n  'goldfish',\n  'great white shark',\n  'tiger shark',\n  'hammerhead',\n  'electric ray',\n  'stingray',\n  'cock',\n  'hen',\n  'ostrich',\n  'brambling',\n  'goldfinch',\n  'house finch',\n  'junco',\n  'indigo bunting',\n  'robin',\n  'bulbul',\n  'jay',\n  'magpie',\n  'chickadee',\n  'water ouzel',\n  'kite',\n  'bald eagle',\n  'vulture',\n  'great grey owl',\n  'European fire salamander',\n  'common newt',\n  'eft',\n  'spotted salamander',\n  'axolotl',\n  'bullfrog',\n  'tree frog',\n  'tailed frog',\n  'loggerhead',\n  'leatherback turtle',\n  'mud turtle',\n  'terrapin',\n  'box turtle',\n  'banded gecko',\n  'common iguana',\n  'American chameleon',\n  'whiptail',\n  'agama',\n  'frilled lizard',\n  'alligator lizard',\n  'Gila monster',\n  'green lizard',\n  'African chameleon',\n  'Komodo dragon',\n  'African crocodile',\n  'American alligator',\n  'triceratops',\n  'thunder snake',\n  'ringneck snake',\n  'hognose snake',\n  'green snake',\n  'king snake',\n  'garter snake',\n  'water snake',\n  'vine snake',\n  'night snake',\n  'boa constrictor',\n  'rock python',\n  'Indian cobra',\n  'green mamba',\n  'sea snake',\n  'horned viper',\n  'diamondback',\n  'sidewinder',\n  'trilobite',\n  'harvestman',\n  'scorpion',\n  'black and gold garden spider',\n  'barn spider',\n  'garden spider',\n  'black widow',\n  'tarantula',\n  'wolf spider',\n  'tick',\n  'centipede',\n  'black grouse',\n  'ptarmigan',\n  'ruffed grouse',\n  'prairie chicken',\n  'peacock',\n  'quail',\n  'partridge',\n  'African grey',\n  'macaw',\n  'sulphur-crested cockatoo',\n  'lorikeet',\n  'coucal',\n  'bee eater',\n  'hornbill',\n  'hummingbird',\n  'jacamar',\n  'toucan',\n  'drake',\n  'red-breasted merganser',\n  'goose',\n  'black swan',\n  'tusker',\n  'echidna',\n  'platypus',\n  'wallaby',\n  'koala',\n  'wombat',\n  'jellyfish',\n  'sea anemone',\n  'brain coral',\n  'flatworm',\n  'nematode',\n  'conch',\n  'snail',\n  'slug',\n  'sea slug',\n  'chiton',\n  'chambered nautilus',\n  'Dungeness crab',\n  'rock crab',\n  'fiddler crab',\n  'king crab',\n  'American lobster',\n  'spiny lobster',\n  'crayfish',\n  'hermit crab',\n  'isopod',\n  'white stork',\n  'black stork',\n  'spoonbill',\n  'flamingo',\n  'little blue heron',\n  'American egret',\n  'bittern',\n  'crane bird',\n  'limpkin',\n  'European gallinule',\n  'American coot',\n  'bustard',\n  'ruddy turnstone',\n  'red-backed sandpiper',\n  'redshank',\n  'dowitcher',\n  'oystercatcher',\n  'pelican',\n  'king penguin',\n  'albatross',\n  'grey whale',\n  'killer whale',\n  'dugong',\n  'sea lion',\n  'Chihuahua',\n  'Japanese spaniel',\n  'Maltese dog',\n  'Pekinese',\n  'Shih-Tzu',\n  'Blenheim spaniel',\n  'papillon',\n  'toy terrier',\n  'Rhodesian ridgeback',\n  'Afghan hound',\n  'basset',\n  'beagle',\n  'bloodhound',\n  'bluetick',\n  'black-and-tan coonhound',\n  'Walker hound',\n  'English foxhound',\n  'redbone',\n  'borzoi',\n  'Irish wolfhound',\n  'Italian greyhound',\n  'whippet',\n  'Ibizan hound',\n  'Norwegian elkhound',\n  'otterhound',\n  'Saluki',\n  'Scottish deerhound',\n  'Weimaraner',\n  'Staffordshire bullterrier',\n  'American Staffordshire terrier',\n  'Bedlington terrier',\n  'Border terrier',\n  'Kerry blue terrier',\n  'Irish terrier',\n  'Norfolk terrier',\n  'Norwich terrier',\n  'Yorkshire terrier',\n  'wire-haired fox terrier',\n  'Lakeland terrier',\n  'Sealyham terrier',\n  'Airedale',\n  'cairn',\n  'Australian terrier',\n  'Dandie Dinmont',\n  'Boston bull',\n  'miniature schnauzer',\n  'giant schnauzer',\n  'standard schnauzer',\n  'Scotch terrier',\n  'Tibetan terrier',\n  'silky terrier',\n  'soft-coated wheaten terrier',\n  'West Highland white terrier',\n  'Lhasa',\n  'flat-coated retriever',\n  'curly-coated retriever',\n  'golden retriever',\n  'Labrador retriever',\n  'Chesapeake Bay retriever',\n  'German short-haired pointer',\n  'vizsla',\n  'English setter',\n  'Irish setter',\n  'Gordon setter',\n  'Brittany spaniel',\n  'clumber',\n  'English springer',\n  'Welsh springer spaniel',\n  'cocker spaniel',\n  'Sussex spaniel',\n  'Irish water spaniel',\n  'kuvasz',\n  'schipperke',\n  'groenendael',\n  'malinois',\n  'briard',\n  'kelpie',\n  'komondor',\n  'Old English sheepdog',\n  'Shetland sheepdog',\n  'collie',\n  'Border collie',\n  'Bouvier des Flandres',\n  'Rottweiler',\n  'German shepherd',\n  'Doberman',\n  'miniature pinscher',\n  'Greater Swiss Mountain dog',\n  'Bernese mountain dog',\n  'Appenzeller',\n  'EntleBucher',\n  'boxer',\n  'bull mastiff',\n  'Tibetan mastiff',\n  'French bulldog',\n  'Great Dane',\n  'Saint Bernard',\n  'Eskimo dog',\n  'malamute',\n  'Siberian husky',\n  'dalmatian',\n  'affenpinscher',\n  'basenji',\n  'pug',\n  'Leonberg',\n  'Newfoundland',\n  'Great Pyrenees',\n  'Samoyed',\n  'Pomeranian',\n  'chow',\n  'keeshond',\n  'Brabancon griffon',\n  'Pembroke',\n  'Cardigan',\n  'toy poodle',\n  'miniature poodle',\n  'standard poodle',\n  'Mexican hairless',\n  'timber wolf',\n  'white wolf',\n  'red wolf',\n  'coyote',\n  'dingo',\n  'dhole',\n  'African hunting dog',\n  'hyena',\n  'red fox',\n  'kit fox',\n  'Arctic fox',\n  'grey fox',\n  'tabby',\n  'tiger cat',\n  'Persian cat',\n  'Siamese cat',\n  'Egyptian cat',\n  'cougar',\n  'lynx',\n  'leopard',\n  'snow leopard',\n  'jaguar',\n  'lion',\n  'tiger',\n  'cheetah',\n  'brown bear',\n  'American black bear',\n  'ice bear',\n  'sloth bear',\n  'mongoose',\n  'meerkat',\n  'tiger beetle',\n  'ladybug',\n  'ground beetle',\n  'long-horned beetle',\n  'leaf beetle',\n  'dung beetle',\n  'rhinoceros beetle',\n  'weevil',\n  'fly',\n  'bee',\n  'ant',\n  'grasshopper',\n  'cricket',\n  'walking stick',\n  'cockroach',\n  'mantis',\n  'cicada',\n  'leafhopper',\n  'lacewing',\n  'dragonfly',\n  'damselfly',\n  'admiral',\n  'ringlet',\n  'monarch',\n  'cabbage butterfly',\n  'sulphur butterfly',\n  'lycaenid',\n  'starfish',\n  'sea urchin',\n  'sea cucumber',\n  'wood rabbit',\n  'hare',\n  'Angora',\n  'hamster',\n  'porcupine',\n  'fox squirrel',\n  'marmot',\n  'beaver',\n  'guinea pig',\n  'sorrel',\n  'zebra',\n  'hog',\n  'wild boar',\n  'warthog',\n  'hippopotamus',\n  'ox',\n  'water buffalo',\n  'bison',\n  'ram',\n  'bighorn',\n  'ibex',\n  'hartebeest',\n  'impala',\n  'gazelle',\n  'Arabian camel',\n  'llama',\n  'weasel',\n  'mink',\n  'polecat',\n  'black-footed ferret',\n  'otter',\n  'skunk',\n  'badger',\n  'armadillo',\n  'three-toed sloth',\n  'orangutan',\n  'gorilla',\n  'chimpanzee',\n  'gibbon',\n  'siamang',\n  'guenon',\n  'patas',\n  'baboon',\n  'macaque',\n  'langur',\n  'colobus',\n  'proboscis monkey',\n  'marmoset',\n  'capuchin',\n  'howler monkey',\n  'titi',\n  'spider monkey',\n  'squirrel monkey',\n  'Madagascar cat',\n  'indri',\n  'Indian elephant',\n  'African elephant',\n  'lesser panda',\n  'giant panda',\n  'barracouta',\n  'eel',\n  'coho',\n  'rock beauty',\n  'anemone fish',\n  'sturgeon',\n  'gar',\n  'lionfish',\n  'puffer',\n  'abacus',\n  'abaya',\n  'academic gown',\n  'accordion',\n  'acoustic guitar',\n  'aircraft carrier',\n  'airliner',\n  'airship',\n  'altar',\n  'ambulance',\n  'amphibian',\n  'analog clock',\n  'apiary',\n  'apron',\n  'ashcan',\n  'assault rifle',\n  'backpack',\n  'bakery',\n  'balance beam',\n  'balloon',\n  'ballpoint',\n  'Band Aid',\n  'banjo',\n  'bannister',\n  'barbell',\n  'barber chair',\n  'barbershop',\n  'barn',\n  'barometer',\n  'barrel',\n  'barrow',\n  'baseball',\n  'basketball',\n  'bassinet',\n  'bassoon',\n  'bathing cap',\n  'bath towel',\n  'bathtub',\n  'beach wagon',\n  'beacon',\n  'beaker',\n  'bearskin',\n  'beer bottle',\n  'beer glass',\n  'bell cote',\n  'bib',\n  'bicycle-built-for-two',\n  'bikini',\n  'binder',\n  'binoculars',\n  'birdhouse',\n  'boathouse',\n  'bobsled',\n  'bolo tie',\n  'bonnet',\n  'bookcase',\n  'bookshop',\n  'bottlecap',\n  'bow',\n  'bow tie',\n  'brass',\n  'brassiere',\n  'breakwater',\n  'breastplate',\n  'broom',\n  'bucket',\n  'buckle',\n  'bulletproof vest',\n  'bullet train',\n  'butcher shop',\n  'cab',\n  'caldron',\n  'candle',\n  'cannon',\n  'canoe',\n  'can opener',\n  'cardigan',\n  'car mirror',\n  'carousel',\n  \"carpenter's kit\",\n  'carton',\n  'car wheel',\n  'cash machine',\n  'cassette',\n  'cassette player',\n  'castle',\n  'catamaran',\n  'CD player',\n  'cello',\n  'cellular telephone',\n  'chain',\n  'chainlink fence',\n  'chain mail',\n  'chain saw',\n  'chest',\n  'chiffonier',\n  'chime',\n  'china cabinet',\n  'Christmas stocking',\n  'church',\n  'cinema',\n  'cleaver',\n  'cliff dwelling',\n  'cloak',\n  'clog',\n  'cocktail shaker',\n  'coffee mug',\n  'coffeepot',\n  'coil',\n  'combination lock',\n  'computer keyboard',\n  'confectionery',\n  'container ship',\n  'convertible',\n  'corkscrew',\n  'cornet',\n  'cowboy boot',\n  'cowboy hat',\n  'cradle',\n  'crane',\n  'crash helmet',\n  'crate',\n  'crib',\n  'Crock Pot',\n  'croquet ball',\n  'crutch',\n  'cuirass',\n  'dam',\n  'desk',\n  'desktop computer',\n  'dial telephone',\n  'diaper',\n  'digital clock',\n  'digital watch',\n  'dining table',\n  'dishrag',\n  'dishwasher',\n  'disk brake',\n  'dock',\n  'dogsled',\n  'dome',\n  'doormat',\n  'drilling platform',\n  'drum',\n  'drumstick',\n  'dumbbell',\n  'Dutch oven',\n  'electric fan',\n  'electric guitar',\n  'electric locomotive',\n  'entertainment center',\n  'envelope',\n  'espresso maker',\n  'face powder',\n  'feather boa',\n  'file',\n  'fireboat',\n  'fire engine',\n  'fire screen',\n  'flagpole',\n  'flute',\n  'folding chair',\n  'football helmet',\n  'forklift',\n  'fountain',\n  'fountain pen',\n  'four-poster',\n  'freight car',\n  'French horn',\n  'frying pan',\n  'fur coat',\n  'garbage truck',\n  'gasmask',\n  'gas pump',\n  'goblet',\n  'go-kart',\n  'golf ball',\n  'golfcart',\n  'gondola',\n  'gong',\n  'gown',\n  'grand piano',\n  'greenhouse',\n  'grille',\n  'grocery store',\n  'guillotine',\n  'hair slide',\n  'hair spray',\n  'half track',\n  'hammer',\n  'hamper',\n  'hand blower',\n  'hand-held computer',\n  'handkerchief',\n  'hard disc',\n  'harmonica',\n  'harp',\n  'harvester',\n  'hatchet',\n  'holster',\n  'home theater',\n  'honeycomb',\n  'hook',\n  'hoopskirt',\n  'horizontal bar',\n  'horse cart',\n  'hourglass',\n  'iPod',\n  'iron',\n  \"jack-o'-lantern\",\n  'jean',\n  'jeep',\n  'jersey',\n  'jigsaw puzzle',\n  'jinrikisha',\n  'joystick',\n  'kimono',\n  'knee pad',\n  'knot',\n  'lab coat',\n  'ladle',\n  'lampshade',\n  'laptop',\n  'lawn mower',\n  'lens cap',\n  'letter opener',\n  'library',\n  'lifeboat',\n  'lighter',\n  'limousine',\n  'liner',\n  'lipstick',\n  'Loafer',\n  'lotion',\n  'loudspeaker',\n  'loupe',\n  'lumbermill',\n  'magnetic compass',\n  'mailbag',\n  'mailbox',\n  'maillot',\n  'maillot tank suit',\n  'manhole cover',\n  'maraca',\n  'marimba',\n  'mask',\n  'matchstick',\n  'maypole',\n  'maze',\n  'measuring cup',\n  'medicine chest',\n  'megalith',\n  'microphone',\n  'microwave',\n  'military uniform',\n  'milk can',\n  'minibus',\n  'miniskirt',\n  'minivan',\n  'missile',\n  'mitten',\n  'mixing bowl',\n  'mobile home',\n  'Model T',\n  'modem',\n  'monastery',\n  'monitor',\n  'moped',\n  'mortar',\n  'mortarboard',\n  'mosque',\n  'mosquito net',\n  'motor scooter',\n  'mountain bike',\n  'mountain tent',\n  'mouse',\n  'mousetrap',\n  'moving van',\n  'muzzle',\n  'nail',\n  'neck brace',\n  'necklace',\n  'nipple',\n  'notebook',\n  'obelisk',\n  'oboe',\n  'ocarina',\n  'odometer',\n  'oil filter',\n  'organ',\n  'oscilloscope',\n  'overskirt',\n  'oxcart',\n  'oxygen mask',\n  'packet',\n  'paddle',\n  'paddlewheel',\n  'padlock',\n  'paintbrush',\n  'pajama',\n  'palace',\n  'panpipe',\n  'paper towel',\n  'parachute',\n  'parallel bars',\n  'park bench',\n  'parking meter',\n  'passenger car',\n  'patio',\n  'pay-phone',\n  'pedestal',\n  'pencil box',\n  'pencil sharpener',\n  'perfume',\n  'Petri dish',\n  'photocopier',\n  'pick',\n  'pickelhaube',\n  'picket fence',\n  'pickup',\n  'pier',\n  'piggy bank',\n  'pill bottle',\n  'pillow',\n  'ping-pong ball',\n  'pinwheel',\n  'pirate',\n  'pitcher',\n  'plane',\n  'planetarium',\n  'plastic bag',\n  'plate rack',\n  'plow',\n  'plunger',\n  'Polaroid camera',\n  'pole',\n  'police van',\n  'poncho',\n  'pool table',\n  'pop bottle',\n  'pot',\n  \"potter's wheel\",\n  'power drill',\n  'prayer rug',\n  'printer',\n  'prison',\n  'projectile',\n  'projector',\n  'puck',\n  'punching bag',\n  'purse',\n  'quill',\n  'quilt',\n  'racer',\n  'racket',\n  'radiator',\n  'radio',\n  'radio telescope',\n  'rain barrel',\n  'recreational vehicle',\n  'reel',\n  'reflex camera',\n  'refrigerator',\n  'remote control',\n  'restaurant',\n  'revolver',\n  'rifle',\n  'rocking chair',\n  'rotisserie',\n  'rubber eraser',\n  'rugby ball',\n  'rule',\n  'running shoe',\n  'safe',\n  'safety pin',\n  'saltshaker',\n  'sandal',\n  'sarong',\n  'sax',\n  'scabbard',\n  'scale',\n  'school bus',\n  'schooner',\n  'scoreboard',\n  'screen',\n  'screw',\n  'screwdriver',\n  'seat belt',\n  'sewing machine',\n  'shield',\n  'shoe shop',\n  'shoji',\n  'shopping basket',\n  'shopping cart',\n  'shovel',\n  'shower cap',\n  'shower curtain',\n  'ski',\n  'ski mask',\n  'sleeping bag',\n  'slide rule',\n  'sliding door',\n  'slot',\n  'snorkel',\n  'snowmobile',\n  'snowplow',\n  'soap dispenser',\n  'soccer ball',\n  'sock',\n  'solar dish',\n  'sombrero',\n  'soup bowl',\n  'space bar',\n  'space heater',\n  'space shuttle',\n  'spatula',\n  'speedboat',\n  'spider web',\n  'spindle',\n  'sports car',\n  'spotlight',\n  'stage',\n  'steam locomotive',\n  'steel arch bridge',\n  'steel drum',\n  'stethoscope',\n  'stole',\n  'stone wall',\n  'stopwatch',\n  'stove',\n  'strainer',\n  'streetcar',\n  'stretcher',\n  'studio couch',\n  'stupa',\n  'submarine',\n  'suit',\n  'sundial',\n  'sunglass',\n  'sunglasses',\n  'sunscreen',\n  'suspension bridge',\n  'swab',\n  'sweatshirt',\n  'swimming trunks',\n  'swing',\n  'switch',\n  'syringe',\n  'table lamp',\n  'tank',\n  'tape player',\n  'teapot',\n  'teddy',\n  'television',\n  'tennis ball',\n  'thatch',\n  'theater curtain',\n  'thimble',\n  'thresher',\n  'throne',\n  'tile roof',\n  'toaster',\n  'tobacco shop',\n  'toilet seat',\n  'torch',\n  'totem pole',\n  'tow truck',\n  'toyshop',\n  'tractor',\n  'trailer truck',\n  'tray',\n  'trench coat',\n  'tricycle',\n  'trimaran',\n  'tripod',\n  'triumphal arch',\n  'trolleybus',\n  'trombone',\n  'tub',\n  'turnstile',\n  'typewriter keyboard',\n  'umbrella',\n  'unicycle',\n  'upright',\n  'vacuum',\n  'vase',\n  'vault',\n  'velvet',\n  'vending machine',\n  'vestment',\n  'viaduct',\n  'violin',\n  'volleyball',\n  'waffle iron',\n  'wall clock',\n  'wallet',\n  'wardrobe',\n  'warplane',\n  'washbasin',\n  'washer',\n  'water bottle',\n  'water jug',\n  'water tower',\n  'whiskey jug',\n  'whistle',\n  'wig',\n  'window screen',\n  'window shade',\n  'Windsor tie',\n  'wine bottle',\n  'wing',\n  'wok',\n  'wooden spoon',\n  'wool',\n  'worm fence',\n  'wreck',\n  'yawl',\n  'yurt',\n  'web site',\n  'comic book',\n  'crossword puzzle',\n  'street sign',\n  'traffic light',\n  'book jacket',\n  'menu',\n  'plate',\n  'guacamole',\n  'consomme',\n  'hot pot',\n  'trifle',\n  'ice cream',\n  'ice lolly',\n  'French loaf',\n  'bagel',\n  'pretzel',\n  'cheeseburger',\n  'hotdog',\n  'mashed potato',\n  'head cabbage',\n  'broccoli',\n  'cauliflower',\n  'zucchini',\n  'spaghetti squash',\n  'acorn squash',\n  'butternut squash',\n  'cucumber',\n  'artichoke',\n  'bell pepper',\n  'cardoon',\n  'mushroom',\n  'Granny Smith',\n  'strawberry',\n  'orange',\n  'lemon',\n  'fig',\n  'pineapple',\n  'banana',\n  'jackfruit',\n  'custard apple',\n  'pomegranate',\n  'hay',\n  'carbonara',\n  'chocolate sauce',\n  'dough',\n  'meat loaf',\n  'pizza',\n  'potpie',\n  'burrito',\n  'red wine',\n  'espresso',\n  'cup',\n  'eggnog',\n  'alp',\n  'bubble',\n  'cliff',\n  'coral reef',\n  'geyser',\n  'lakeside',\n  'promontory',\n  'sandbar',\n  'seashore',\n  'valley',\n  'volcano',\n  'ballplayer',\n  'groom',\n  'scuba diver',\n  'rapeseed',\n  'daisy',\n  \"yellow lady's slipper\",\n  'corn',\n  'acorn',\n  'hip',\n  'buckeye',\n  'coral fungus',\n  'agaric',\n  'gyromitra',\n  'stinkhorn',\n  'earthstar',\n  'hen-of-the-woods',\n  'bolete',\n  'ear',\n  'toilet tissue'],\n 'recipe': 'https://github.com/pytorch/vision/tree/main/references/classification#inception-v3',\n '_metrics': {'ImageNet-1K': {'acc@1': 77.294, 'acc@5': 93.45}},\n '_ops': 5.713,\n '_file_size': 103.903,\n '_docs': 'These weights are ported from the original paper.'}\n\n\n\nStore the categories in a variable to use them later\n\n\ncategories = inception_weights.meta[\"categories\"]\n\n\n\n\n\n\n\nTip\n\n\nMore info about Inception V3 implementation in torchvision here"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_2.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-1",
    "href": "notes/Adv_ML_Python_presentation_1_2.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-1",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 2)",
    "section": "Exercise: Use a pre-trained deep learning model to classify images",
    "text": "Exercise: Use a pre-trained deep learning model to classify images\n\nLoad the Inception V3 model using the pre-trained weights inception_weights\n\n\ndl_model = models.inception_v3(inception_weights, progress=True)\n\ndl_model.eval()\n\nInception3(\n  (Conv2d_1a_3x3): BasicConv2d(\n    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (Conv2d_2a_3x3): BasicConv2d(\n    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (Conv2d_2b_3x3): BasicConv2d(\n    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (Conv2d_3b_1x1): BasicConv2d(\n    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (Conv2d_4a_3x3): BasicConv2d(\n    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (Mixed_5b): InceptionA(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_1): BasicConv2d(\n      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_2): BasicConv2d(\n      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_5c): InceptionA(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_1): BasicConv2d(\n      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_2): BasicConv2d(\n      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_5d): InceptionA(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_1): BasicConv2d(\n      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_2): BasicConv2d(\n      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6a): InceptionB(\n    (branch3x3): BasicConv2d(\n      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6b): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6c): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6d): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6e): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (AuxLogits): InceptionAux(\n    (conv0): BasicConv2d(\n      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (conv1): BasicConv2d(\n      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (fc): Linear(in_features=768, out_features=1000, bias=True)\n  )\n  (Mixed_7a): InceptionD(\n    (branch3x3_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2): BasicConv2d(\n      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_2): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_3): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_4): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_7b): InceptionE(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_1): BasicConv2d(\n      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_7c): InceptionE(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_1): BasicConv2d(\n      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_2.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-2",
    "href": "notes/Adv_ML_Python_presentation_1_2.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-2",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 2)",
    "section": "Exercise: Use a pre-trained deep learning model to classify images",
    "text": "Exercise: Use a pre-trained deep learning model to classify images\n\nLoad a sample image to predict its category\n\n\nimport skimage\nimport matplotlib.pyplot as plt\n\nsample_im = skimage.data.rocket()\nsample_im.shape\n\n(427, 640, 3)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_2.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-3",
    "href": "notes/Adv_ML_Python_presentation_1_2.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-3",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 2)",
    "section": "Exercise: Use a pre-trained deep learning model to classify images",
    "text": "Exercise: Use a pre-trained deep learning model to classify images\n\nVisualize the sample image\n\n\nplt.imshow(sample_im)\nplt.show()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_2.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-4",
    "href": "notes/Adv_ML_Python_presentation_1_2.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-4",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 2)",
    "section": "Exercise: Use a pre-trained deep learning model to classify images",
    "text": "Exercise: Use a pre-trained deep learning model to classify images\n\nInspect what transforms are required by the pre-trained Inception model to work properly\n\n\ninception_weights.transforms\n\nfunctools.partial(&lt;class 'torchvision.transforms._presets.ImageClassification'&gt;, crop_size=299, resize_size=342)\n\n\n\n\n\n\n\n\nImportant\n\n\nfunctools.partial is a function to define functions with static arguments. So 👆 returns a function when it is called!\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe transforms used by the Inception V3 are\n\nresize the image to 342x342 pixels,\ncrop the center 299x299 pixels window, and\nnormalize the values of the RGB channels."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_2.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-5",
    "href": "notes/Adv_ML_Python_presentation_1_2.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-5",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 2)",
    "section": "Exercise: Use a pre-trained deep learning model to classify images",
    "text": "Exercise: Use a pre-trained deep learning model to classify images\n\nDefine a preprocessing pipeline using the inception_weights.transforms() method. Add also a transformation from numpy arrays into torch tensors.\n\n\nfrom torchvision.transforms.v2 import Compose, ToTensor\n\npipeline = Compose([\n  ToTensor(),\n  inception_weights.transforms()\n])\n\npipeline\n\nCompose(\n      ToTensor()\n      ImageClassification(\n      crop_size=[299]\n      resize_size=[342]\n      mean=[0.485, 0.456, 0.406]\n      std=[0.229, 0.224, 0.225]\n      interpolation=InterpolationMode.BILINEAR\n  )\n)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_2.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-6",
    "href": "notes/Adv_ML_Python_presentation_1_2.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-6",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 2)",
    "section": "Exercise: Use a pre-trained deep learning model to classify images",
    "text": "Exercise: Use a pre-trained deep learning model to classify images\n\nPre-process the sample image using our pipeline\n\n\nsample_x = pipeline(sample_im)\ntype(sample_x), sample_x.shape, sample_x.min(), sample_x.max()\n\n(torch.Tensor, torch.Size([3, 299, 299]), tensor(-1.8792), tensor(2.6400))"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_2.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-7",
    "href": "notes/Adv_ML_Python_presentation_1_2.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-7",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 2)",
    "section": "Exercise: Use a pre-trained deep learning model to classify images",
    "text": "Exercise: Use a pre-trained deep learning model to classify images\n\nUse the pre-trained model to predict the class of our sample image\n\n\n\n\n\n\n\nCaution\n\n\nApply the model on sample_x[None, …], so it is treated as a one-sample batch\n\n\n\n\nsample_y = dl_model(sample_x[None, ...])\n\nsample_y.shape\n\ntorch.Size([1, 1000])"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_2.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-8",
    "href": "notes/Adv_ML_Python_presentation_1_2.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-8",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 2)",
    "section": "Exercise: Use a pre-trained deep learning model to classify images",
    "text": "Exercise: Use a pre-trained deep learning model to classify images\n\n\n\n\n\n\nNote\n\n\nThe model’s output are the log-probabilities of sample_x belonging to each of the 1000 classes.\n\n\n\n\nShow the categories with the highest log-probabilities.\n\n\nsample_y.argsort(dim=1)\n\ntensor([[578, 253, 339, 982, 301,  40, 584, 897, 382,  12, 170, 181,  25, 161,\n         255, 162, 159,  14, 500,  72,  44, 368, 714, 211, 201, 240,  90, 277,\n          99, 519,  46,  13, 790, 160, 883, 307,  31, 467, 271,  32, 452, 997,\n          92, 414, 286,  64, 241, 661, 360, 381, 473, 999, 316,  24, 184, 213,\n         804,  26, 889, 383, 689,  11, 231,  28, 207, 520, 134,  77,  37, 177,\n         513, 539, 191, 264, 306,  19, 103, 529, 823, 171, 838, 247, 174, 400,\n         228, 665, 669, 278,  16, 870, 354, 412, 272, 377, 175, 411, 551,  75,\n         265, 390, 601, 929,  27, 543, 434, 268,  94, 200,  33, 504, 760, 614,\n         387, 875, 167, 809, 588, 189,  30, 291, 703, 944, 323, 610, 631,  57,\n          41, 881, 349, 938, 493, 825, 732, 249, 593, 299, 516, 237, 246, 195,\n         137, 850, 756,  91, 695, 336, 799, 431, 766, 761, 310, 337, 894, 622,\n         305, 129, 351, 989, 581, 991, 612, 110, 672, 436, 239, 226,  93,  76,\n         785,  21, 260,  63, 294, 176, 960, 528, 606, 826, 168, 166,  87, 531,\n         355, 154,  15, 774, 717, 322,  18,   0, 193, 435, 152, 678, 722, 269,\n          86,  47, 292, 759, 248, 317, 105, 302, 621, 376,  60, 793, 295, 283,\n         692, 720, 495, 859, 284, 393, 267, 596, 537, 656, 235, 831, 707, 444,\n          85, 130, 232, 534, 560, 204, 188, 430, 379, 853,  43,  61, 261, 684,\n         932, 172, 746, 178, 986, 876, 615, 996, 770, 700, 298, 459, 384, 273,\n         328,  52, 605, 736, 968,  95, 410, 922, 296, 259, 347, 401, 518, 256,\n         912, 937, 319,  97, 155, 463, 636, 457,  53, 671, 119, 893, 300, 281,\n         921, 752, 394, 163, 244, 340, 697, 985, 763, 553, 243, 133, 568, 210,\n         454, 933, 597, 443,  20, 308, 451, 579, 549, 164, 716, 917,  59, 592,\n         136, 114, 185, 690, 472, 878, 642, 135, 771, 639, 276, 456, 485, 238,\n         943, 582, 486, 544, 979,  42, 868, 511, 321, 156,   9, 721, 365, 113,\n         263, 910,  88, 499, 670,  17, 635, 734, 266, 984, 386, 140, 280, 750,\n         407,  10, 654, 197, 478,  36,  62, 775, 638,  55, 275, 861, 230, 421,\n         603, 646, 447, 112, 233, 788, 222, 655, 492, 423, 896, 618, 675, 993,\n         794, 616, 215, 915, 196, 402, 229, 852, 364,  22, 139,  81, 685, 713,\n         309, 314, 153, 116, 547, 217, 643, 566, 719, 874,  98, 330, 587, 254,\n         458, 623, 886, 901, 842, 541, 930, 817, 507, 946, 362, 849,  73, 786,\n         939,  35, 157, 396, 250, 326, 725, 345, 617, 580, 988, 778, 552, 251,\n         187, 865,  71, 887, 242, 651, 448,  23, 559, 359, 202, 145, 180, 857,\n         691, 182, 475, 813, 743, 131, 843, 304, 318, 353, 422, 440,  89, 234,\n         704, 706,  68, 728, 150, 395, 108, 391, 512, 425, 468, 947, 143, 293,\n         350, 526, 257, 550, 957, 970, 955, 575,   5, 101, 123, 945, 633, 104,\n          80, 502, 729, 508,  49, 225, 258, 426, 637, 183, 995, 789, 344, 906,\n         948, 179, 950, 398, 397, 664, 532, 903, 198, 626, 194, 236, 667, 676,\n         757, 465, 324, 335, 218, 303, 699, 496, 934, 158,  38, 441, 433, 992,\n         375, 357, 810, 815, 693, 192, 585, 107, 586, 289, 331, 735, 772, 125,\n         515, 416, 392, 482, 869, 608, 681, 607, 882, 640, 380, 126, 854, 924,\n         768, 312, 589, 837, 141, 570, 315, 653, 102, 572, 479, 369, 956, 503,\n         115,  74, 481, 797, 122, 535, 327, 352,  45, 953, 802,  48, 389,  96,\n         270, 723, 224, 378, 795, 424, 450, 461, 796, 100,  39, 252, 800, 648,\n         679, 121, 787, 453, 925, 613, 846, 726, 702, 173,   8, 221, 287, 926,\n         169, 341, 325, 186, 128, 356, 709, 455, 645, 983,   6,  67, 449, 285,\n         972,  65, 765, 564, 282, 663, 677, 118, 806,  29, 576, 151, 388, 748,\n         782, 311, 358, 963, 329, 824, 333, 533, 203, 905, 673, 830, 951, 480,\n         419, 445, 829, 127, 904, 858,  69, 371, 313, 208, 753, 509, 521, 641,\n         219, 214, 209,  56, 928, 462, 138, 149, 776, 987, 523, 206, 916, 332,\n          51,   1, 320, 262, 798, 524, 805, 514, 627, 274, 705, 334, 420, 227,\n         773, 474, 205, 696, 899,  84, 212, 111,   7, 483, 801, 109, 990, 710,\n         866, 488,  78, 791, 602, 851, 747, 971,  70,  82, 439, 848, 367, 873,\n         591, 958, 505, 464, 342, 290, 674, 659, 898, 288, 803, 967, 594, 370,\n         730, 385, 567, 779, 223, 374, 429, 649, 741, 836, 711,  50, 952, 165,\n         598, 739, 546, 487, 686, 658, 749, 715, 501, 609, 890, 954, 432,  34,\n         769, 428, 647, 973, 962, 742, 981,   4, 619, 892,  58, 841, 964,  83,\n         142, 891, 361, 885, 909, 927, 124, 777, 363, 220, 373, 144, 406, 660,\n         731, 297, 783, 630, 245, 577, 106, 583, 965, 911, 427, 556, 834, 132,\n         762, 469, 569, 738, 199, 491, 446, 348,   3, 346, 147, 902, 808, 740,\n         936, 908, 819, 372, 827, 624, 975, 687, 548, 879, 611,  79, 574, 662,\n         476, 522, 864, 745, 969, 880, 998, 680, 343, 632, 884, 877, 497, 835,\n         477, 701, 811, 573, 818, 120, 949, 565, 190, 767, 914, 650, 418, 931,\n         216, 844, 872, 666, 415, 527, 117, 604, 148, 438, 923, 279, 599, 888,\n         698, 961, 855, 941, 338, 978, 466, 538, 688, 724, 839, 792, 542, 652,\n         525, 814, 918, 708, 366, 590, 860, 862, 600, 784, 942, 966, 976, 867,\n           2, 146, 489, 413, 712, 595, 644, 828, 994,  54, 399, 629, 780, 727,\n         822, 959, 764, 935, 832, 755, 751, 417, 977, 561, 558, 545, 625, 490,\n          66, 470, 980, 974, 816, 571, 863, 506, 737, 494, 555, 907, 409, 840,\n         460, 563, 471, 634, 620, 820, 484, 530, 913, 536, 403, 718, 856, 758,\n         845, 919, 821, 562, 871, 920, 404, 408, 683, 781, 940, 510, 847, 498,\n         628, 694, 895, 554, 405, 900, 442, 754, 833, 437, 807, 682, 668, 557,\n         812, 733, 517, 540, 744, 657]])"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_2.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-9",
    "href": "notes/Adv_ML_Python_presentation_1_2.html#exercise-use-a-pre-trained-deep-learning-model-to-classify-images-9",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 2)",
    "section": "Exercise: Use a pre-trained deep learning model to classify images",
    "text": "Exercise: Use a pre-trained deep learning model to classify images\n\nUse the list of categories to translate the predicted class index into its category.\n\n\nsorted_predicted_classes = sample_y.argsort(dim=1, descending=True)[0, :10]\nsorted_probs = torch.softmax(sample_y, dim=1)[0, sorted_predicted_classes]\n\nfor idx, prob in zip(sorted_predicted_classes, sorted_probs):\n    print(categories[idx], \"%3.2f %%\" % (prob * 100))\n\nmissile 37.93 %\nprojectile 14.39 %\ndrilling platform 12.16 %\ncrane 5.72 %\npole 1.06 %\nspace shuttle 0.93 %\nflagpole 0.93 %\nmosque 0.51 %\nobelisk 0.46 %\nsolar dish 0.45 %"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_2.html#try-with-other-sample-images",
    "href": "notes/Adv_ML_Python_presentation_1_2.html#try-with-other-sample-images",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 2)",
    "section": "Try with other sample images",
    "text": "Try with other sample images\n\n\n\n\n\n\nCaution\n\n\nOnly works with RGB images\n\n\n\n\nUse the pre-trained model to classify images from the internet (try some from the Kodak Lossless True Color Image Suite).\n\n\nsample_im = skimage.io.imread(\"https://r0k.us/graphics/kodak/kodak/kodim21.png\")\nsample_x = pipeline(sample_im)\nsample_y = dl_model(sample_x[None, ...])\n\nplt.imshow(sample_im)\nplt.title(categories[sample_y.argmax(dim=1)])\nplt.show()\n\nsorted_predicted_classes = sample_y.argsort(dim=1, descending=True)[0, :10]\nsorted_probs = torch.softmax(sample_y, dim=1)[0, sorted_predicted_classes]\n\nfor idx, prob in zip(sorted_predicted_classes, sorted_probs):\n    print(categories[idx], \"%3.2f %%\" % (prob * 100))\n\n\n\n\n\n\n\n\nbeacon 97.15 %\nbreakwater 0.15 %\nAngora 0.06 %\nwallaby 0.04 %\nwood rabbit 0.04 %\npromontory 0.03 %\npicket fence 0.03 %\nschooner 0.02 %\ncastle 0.02 %\nseashore 0.02 %\n\n\n\nTry with an image from a category that is not in the labels set of the model (like one with hanging caps)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_2.html#exercise-modify-the-classifier-layer-dl_model.fc-to-return-the-features-map-from-the-input-image-instead-of-the-category",
    "href": "notes/Adv_ML_Python_presentation_1_2.html#exercise-modify-the-classifier-layer-dl_model.fc-to-return-the-features-map-from-the-input-image-instead-of-the-category",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 2)",
    "section": "Exercise: Modify the classifier layer dl_model.fc to return the features map from the input image instead of the category",
    "text": "Exercise: Modify the classifier layer dl_model.fc to return the features map from the input image instead of the category\n\n\n\n\n\n\nTip\n\n\nThe classifier layer is commonly implemented as a MultiLayer Perceptron (Fully connected) at the end of the models. The specific name of that layer can vary between implementations.\n\n\n\n\nLoad the pre-trained Inception V3 model again to use it as feature extractor.\n\n\ndl_extractor = models.inception_v3(inception_weights, progress=True)\ndl_extractor.eval()\n\ndl_extractor.fc\n\nLinear(in_features=2048, out_features=1000, bias=True)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_1_2.html#exercise-modify-the-classifier-layer-dl_model.fc-to-return-the-features-map-from-the-input-image-instead-of-the-category-1",
    "href": "notes/Adv_ML_Python_presentation_1_2.html#exercise-modify-the-classifier-layer-dl_model.fc-to-return-the-features-map-from-the-input-image-instead-of-the-category-1",
    "title": "Advanced Machine Learning with Python (Session 1 - Part 2)",
    "section": "Exercise: Modify the classifier layer dl_model.fc to return the features map from the input image instead of the category",
    "text": "Exercise: Modify the classifier layer dl_model.fc to return the features map from the input image instead of the category\n\nReplace the .fc layer with a torch.nn.Identity module.\n\n\ndl_extractor.fc = torch.nn.Identity()\n\n\nUse the model for feature extraction in the same way it is used for image classification.\n\n\nsample_fx = dl_extractor(sample_x[None, ...])\n\nsample_fx.shape\n\ntorch.Size([1, 2048])"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Machine Learning with Python",
    "section": "",
    "text": "My name is Fernando Cervantes, a Systems Analyst II at The Jackson Laboratory’s Research IT department.\nYou can ask me questions about this material at fernando.cervantes@jax.org"
  },
  {
    "objectID": "index.html#dates-november-19th-20th-2025",
    "href": "index.html#dates-november-19th-20th-2025",
    "title": "Advanced Machine Learning with Python",
    "section": "",
    "text": "My name is Fernando Cervantes, a Systems Analyst II at The Jackson Laboratory’s Research IT department.\nYou can ask me questions about this material at fernando.cervantes@jax.org"
  },
  {
    "objectID": "notes/CellPaintint_for_ML.html",
    "href": "notes/CellPaintint_for_ML.html",
    "title": "Install required packages to access images from S3 storage",
    "section": "",
    "text": "!pip install -U imagecodecs s3fs tifffile\n!git clone https://github.com/jump-cellpainting/JUMP-Target !git clone https://github.com/jump-cellpainting/datasets.git"
  },
  {
    "objectID": "notes/CellPaintint_for_ML.html#get-the-url-of-each-assay-plate-from-the-bucket",
    "href": "notes/CellPaintint_for_ML.html#get-the-url-of-each-assay-plate-from-the-bucket",
    "title": "Install required packages to access images from S3 storage",
    "section": "Get the URL of each assay plate from the bucket",
    "text": "Get the URL of each assay plate from the bucket\n\nimport s3fs\n\n\nfs = s3fs.S3FileSystem(anon=True)\n\nbatch_names = {}\nplate_paths = {}\nsource_names = {}\nplate_types = {}\n\nfor _, src_row in jump_plates_metadata.groupby([\"Metadata_Source\", \"Metadata_Batch\"]).describe().iterrows():\n    source_name, batch_name = src_row.name\n\n    # Ignore 'source_8' since the naming of the images is not standard\n    if source_name in [\"source_8\"]:\n        continue\n\n    plate_type = src_row[\"Metadata_PlateType\"].top\n\n    for plate_path in fs.ls(f\"cellpainting-gallery/cpg0016-jump/{source_name}/images/{batch_name}/images/\"):\n        plate_path = plate_path.split(\"/\")[-1]\n        if not plate_path:\n            continue\n\n        plate_name = plate_path.split(\"__\")[0]\n\n        source_names[plate_name] = source_name\n        batch_names[plate_name] = batch_name\n        plate_types[plate_name] = plate_type\n        plate_paths[plate_name] = plate_path\n\n\nplate_maps = pd.DataFrame()\nplate_maps[\"Plate_name\"] = batch_names.keys()\nplate_maps[\"Source_name\"] = plate_maps[\"Plate_name\"].map(source_names)\nplate_maps[\"Batch_name\"] = plate_maps[\"Plate_name\"].map(batch_names)\nplate_maps[\"Plate_type\"] = plate_maps[\"Plate_name\"].map(plate_types)\nplate_maps[\"Plate_path\"] = plate_maps[\"Plate_name\"].map(plate_paths)\n\n\nplate_maps\n\n\n  \n    \n\n\n\n\n\n\nPlate_name\nSource_name\nBatch_name\nPlate_type\nPlate_path\n\n\n\n\n0\nUL000109\nsource_1\nBatch1_20221004\nCOMPOUND\nUL000109__2022-10-05T06_35_06-Measurement1\n\n\n1\nUL001641\nsource_1\nBatch1_20221004\nCOMPOUND\nUL001641__2022-10-04T23_16_28-Measurement1\n\n\n2\nUL001643\nsource_1\nBatch1_20221004\nCOMPOUND\nUL001643__2022-10-04T18_52_42-Measurement2\n\n\n3\nUL001645\nsource_1\nBatch1_20221004\nCOMPOUND\nUL001645__2022-10-05T00_44_11-Measurement1\n\n\n4\nUL001651\nsource_1\nBatch1_20221004\nCOMPOUND\nUL001651__2022-10-04T20_20_52-Measurement1\n\n\n...\n...\n...\n...\n...\n...\n\n\n2333\nGR00004417\nsource_9\n20211103-Run16\nCOMPOUND\nGR00004417\n\n\n2334\nGR00004418\nsource_9\n20211103-Run16\nCOMPOUND\nGR00004418\n\n\n2335\nGR00004419\nsource_9\n20211103-Run16\nCOMPOUND\nGR00004419\n\n\n2336\nGR00004420\nsource_9\n20211103-Run16\nCOMPOUND\nGR00004420\n\n\n2337\nGR00004421\nsource_9\n20211103-Run16\nCOMPOUND\nGR00004421\n\n\n\n\n2338 rows × 5 columns\n\n    \n      \n  \n    \n      \n  \n    \n  \n    \n    \n  \n\n    \n  \n  \n    \n  \n  \n\n\n\ncomp_plate_maps = plate_maps.query(\"Plate_type=='COMPOUND'\")\ncomp_plate_maps\n\n\n  \n    \n\n\n\n\n\n\nPlate_name\nSource_name\nBatch_name\nPlate_type\nPlate_path\n\n\n\n\n0\nUL000109\nsource_1\nBatch1_20221004\nCOMPOUND\nUL000109__2022-10-05T06_35_06-Measurement1\n\n\n1\nUL001641\nsource_1\nBatch1_20221004\nCOMPOUND\nUL001641__2022-10-04T23_16_28-Measurement1\n\n\n2\nUL001643\nsource_1\nBatch1_20221004\nCOMPOUND\nUL001643__2022-10-04T18_52_42-Measurement2\n\n\n3\nUL001645\nsource_1\nBatch1_20221004\nCOMPOUND\nUL001645__2022-10-05T00_44_11-Measurement1\n\n\n4\nUL001651\nsource_1\nBatch1_20221004\nCOMPOUND\nUL001651__2022-10-04T20_20_52-Measurement1\n\n\n...\n...\n...\n...\n...\n...\n\n\n2333\nGR00004417\nsource_9\n20211103-Run16\nCOMPOUND\nGR00004417\n\n\n2334\nGR00004418\nsource_9\n20211103-Run16\nCOMPOUND\nGR00004418\n\n\n2335\nGR00004419\nsource_9\n20211103-Run16\nCOMPOUND\nGR00004419\n\n\n2336\nGR00004420\nsource_9\n20211103-Run16\nCOMPOUND\nGR00004420\n\n\n2337\nGR00004421\nsource_9\n20211103-Run16\nCOMPOUND\nGR00004421\n\n\n\n\n1905 rows × 5 columns\n\n    \n      \n  \n    \n      \n  \n    \n  \n    \n    \n  \n\n    \n  \n  \n    \n  \n  \n\n\n\npert_plate_maps = plate_maps[plate_maps[\"Plate_type\"].isin([\"CRISPR\", \"ORF\", \"DMSO\"])]\npert_plate_maps\n\n\n  \n    \n\n\n\n\n\n\nPlate_name\nSource_name\nBatch_name\nPlate_type\nPlate_path\n\n\n\n\n142\nDest210628-161651\nsource_10\n2021_06_28_U2OS_48_hr_run9\nDMSO\nDest210628-161651\n\n\n143\nDest210628-162003\nsource_10\n2021_06_28_U2OS_48_hr_run9\nDMSO\nDest210628-162003\n\n\n457\nCP-CC9-R1-01\nsource_13\n20220914_Run1\nCRISPR\nCP-CC9-R1-01\n\n\n458\nCP-CC9-R1-02\nsource_13\n20220914_Run1\nCRISPR\nCP-CC9-R1-02\n\n\n459\nCP-CC9-R1-03\nsource_13\n20220914_Run1\nCRISPR\nCP-CC9-R1-03\n\n\n...\n...\n...\n...\n...\n...\n\n\n1591\nBR00127145\nsource_4\n2021_08_30_Batch13\nORF\nBR00127145__2021-09-22T04_01_46-Measurement1\n\n\n1592\nBR00127146\nsource_4\n2021_08_30_Batch13\nORF\nBR00127146__2021-09-22T12_25_07-Measurement1\n\n\n1593\nBR00127147\nsource_4\n2021_08_30_Batch13\nORF\nBR00127147__2021-09-18T10_27_12-Measurement1\n\n\n1594\nBR00127148\nsource_4\n2021_08_30_Batch13\nORF\nBR00127148__2021-09-21T11_44_23-Measurement1\n\n\n1595\nBR00127149\nsource_4\n2021_08_30_Batch13\nORF\nBR00127149__2021-09-18T02_10_04-Measurement1\n\n\n\n\n433 rows × 5 columns\n\n    \n      \n  \n    \n      \n  \n    \n  \n    \n    \n  \n\n    \n  \n  \n    \n  \n  \n\n\n\npert_plate_maps[\"Source_name\"].unique()\n\narray(['source_10', 'source_13', 'source_4'], dtype=object)\n\n\n\ncomp_plate_maps[\"Source_name\"].unique()\n\narray(['source_1', 'source_10', 'source_11', 'source_15', 'source_2',\n       'source_3', 'source_5', 'source_6', 'source_7', 'source_9'],\n      dtype=object)"
  },
  {
    "objectID": "notes/CellPaintint_for_ML.html#create-the-datasets-from-the-list-of-urls",
    "href": "notes/CellPaintint_for_ML.html#create-the-datasets-from-the-list-of-urls",
    "title": "Install required packages to access images from S3 storage",
    "section": "Create the datasets from the list of URLs",
    "text": "Create the datasets from the list of URLs\n\ntraining_ds = TiffS3Dataset(pert_plate_maps, wells_metadata, trn_plates, 16, 24, 9, 5, shuffle=True)\nvalidation_ds = TiffS3Dataset(pert_plate_maps, wells_metadata, val_plates, 16, 24, 9, 5, shuffle=True)\ntesting_ds = TiffS3Dataset(pert_plate_maps, wells_metadata, tst_plates, 16, 24, 9, 5, shuffle=True)"
  },
  {
    "objectID": "notes/CellPaintint_for_ML.html#create-a-torch-dataloader-to-train-pytorch-models",
    "href": "notes/CellPaintint_for_ML.html#create-a-torch-dataloader-to-train-pytorch-models",
    "title": "Install required packages to access images from S3 storage",
    "section": "Create a torch DataLoader to train PyTorch models",
    "text": "Create a torch DataLoader to train PyTorch models\n\nfrom tqdm.auto import tqdm\nfrom torch.utils.data.dataloader import DataLoader\n\n\nbatch_size = 100\n\ntraining_dl = DataLoader(training_ds, batch_size=batch_size, num_workers=8, worker_init_fn=s3dataset_worker_init_fn)\n\n\nfeatures = []\ntargets = []\n\nfor i, (x, y, _) in tqdm(enumerate(training_dl)):\n    if i &gt;= 1000:\n        break\n\n    b, c, h, w = x.shape\n    x_t = model_transforms(torch.tile(x.reshape(-1, 1, h, w), (1, 3, 1, 1)))\n\n    if torch.cuda.is_available():\n        x_t = x_t.cuda()\n    \n    with torch.no_grad():\n        x_out = model(x_t)\n        x_out = x_out.detach().cpu().reshape(-1, c, 576, 7, 7).sum(dim=1)\n        x_out = org_avgpool(x_out).detach().reshape(b, -1)\n\n    features.append(x_out)\n    targets.append(y)\n\n    if (i + 1) % 100 == 0:\n        features = torch.cat(features, dim=0)\n\n        # The labels are mapped as NONE/DMSO = 0, ORF = 1, CRISPS = 2, and COMPUND = 3\n        targets = torch.cat(targets, dim=0)\n\n        torch.save(dict(features=features, targets=targets), f\"trn_features_{i // 100:03d}.pt\")\n        print(\"Saved features checkpoint\", f\"trn_features_{i // 100:03d}.pt\", features.shape, targets.shape)\n\n        features = []\n        targets = []\n\n\n\n\nSaved features checkpoint trn_features_000.pt torch.Size([10000, 576]) torch.Size([10000])\nSaved features checkpoint trn_features_001.pt torch.Size([10000, 576]) torch.Size([10000])\nSaved features checkpoint trn_features_002.pt torch.Size([10000, 576]) torch.Size([10000])\nSaved features checkpoint trn_features_003.pt torch.Size([10000, 576]) torch.Size([10000])\nSaved features checkpoint trn_features_004.pt torch.Size([10000, 576]) torch.Size([10000])\nSaved features checkpoint trn_features_005.pt torch.Size([10000, 576]) torch.Size([10000])\nSaved features checkpoint trn_features_006.pt torch.Size([10000, 576]) torch.Size([10000])\nSaved features checkpoint trn_features_007.pt torch.Size([10000, 576]) torch.Size([10000])\nSaved features checkpoint trn_features_008.pt torch.Size([10000, 576]) torch.Size([10000])\nSaved features checkpoint trn_features_009.pt torch.Size([10000, 576]) torch.Size([10000])\n\n\n\nval_features = []\nval_targets = []\n\nvalidation_dl = DataLoader(validation_ds, batch_size=100, num_workers=8, worker_init_fn=s3dataset_worker_init_fn)\n\nfor i, (x, y, _) in tqdm(enumerate(validation_dl)):\n    if i &gt;= 50:\n        break\n\n    b, c, h, w = x.shape\n    x_t = model_transforms(torch.tile(x.reshape(-1, 1, h, w), (1, 3, 1, 1)))\n\n    if torch.cuda.is_available():\n        x_t = x_t.cuda()\n\n    with torch.no_grad():\n        x_out = model(x_t)\n        x_out = x_out.detach().reshape(-1, c, 576, 7, 7).sum(dim=1)\n        x_out = org_avgpool(x_out).detach().reshape(b, -1)\n\n    val_features.append(x_out)\n    val_targets.append(y)\n\nval_features = torch.cat(val_features, dim=0)\n\n# The labels are mapped as NONE/DMSO = 0, ORF = 1, CRISPS = 2, and COMPUND = 3\nval_targets = torch.cat(val_targets, dim=0)\n\n\n\n\nException ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n    if w.is_alive():\n       ^^Exception ignored in: ^^&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;^\n^Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n^^Exception ignored in:     ^self._shutdown_workers()^&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\n^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n\n^Traceback (most recent call last):\n\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    if w.is_alive():      File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n\nself._shutdown_workers()\n       File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\nassert self._parent_pid == os.getpid(), 'can only test a child process' \n     if w.is_alive(): \n             ^  ^ ^  ^ ^^ ^ ^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^^    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n^^ \n^   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^     ^ assert self._parent_pid == os.getpid(), 'can only test a child process'^ ^\n^   ^  ^ ^  ^  ^ ^^ ^ ^^^ ^ ^^ ^^ ^^^^^\n^^AssertionError^^^: ^^can only test a child process\n^^^^^^^^Exception ignored in: ^^&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;^^^\n^^Traceback (most recent call last):\n^^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n^^    ^^self._shutdown_workers()^^\n^^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n^^^    ^^if w.is_alive():^^\n^ ^^^ ^^ ^^^ \n^ AssertionError^ : ^ can only test a child process^^\n^^\n^AssertionError^: Exception ignored in: ^can only test a child process&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\n^\n^Traceback (most recent call last):\n^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n^    ^Exception ignored in: self._shutdown_workers()^\n&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;^\n\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\nTraceback (most recent call last):\n      File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    assert self._parent_pid == os.getpid(), 'can only test a child process'if w.is_alive():    \n\nself._shutdown_workers()  \n    File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n         if w.is_alive():  \n      ^ ^  ^  ^^^ ^ ^^ ^^^^^^^^^^^^^^^^\n^^^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^^    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n^ ^^ ^^ \n ^   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^ ^     ^ assert self._parent_pid == os.getpid(), 'can only test a child process' ^\n  ^  ^^^ ^^ ^^ ^ ^ ^^ ^^^^ ^^^ ^\n^ AssertionError^^: ^^^can only test a child process^^\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^AssertionError^: ^can only test a child process^\n^^^Exception ignored in: ^&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\n\nAssertionErrorTraceback (most recent call last):\n:   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\ncan only test a child process\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\nException ignored in:     &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;if w.is_alive():\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n      self._shutdown_workers() \n   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n       if w.is_alive():^\n^  ^ ^ ^ ^ ^^ ^^^^^^^^^\n^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^^    assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n           ^^^^^^^^\n^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'\n^ ^ ^ ^ ^ ^ ^^   ^^^ ^ ^^^^^^^^^^^^^^^^^\nAssertionError: ^can only test a child process^\n^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n\n\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-162003/Dest210628-162003_G23_T0001F009L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-162003/Dest210628-162003_L14_T0001F007L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-162003/Dest210628-162003_C06_T0001F007L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-162003/Dest210628-162003_N23_T0001F008L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-162003/Dest210628-162003_P12_T0001F008L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-162003/Dest210628-162003_I11_T0001F007L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-162003/Dest210628-162003_J03_T0001F008L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-162003/Dest210628-162003_F11_T0001F009L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-162003/Dest210628-162003_M11_T0001F009L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-162003/Dest210628-162003_J16_T0001F009L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-162003/Dest210628-162003_C18_T0001F008L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-162003/Dest210628-162003_P06_T0001F008L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-162003/Dest210628-162003_D18_T0001F009L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-162003/Dest210628-162003_K21_T0001F009L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-162003/Dest210628-162003_L12_T0001F009L01A01Z01C01.tif\n\n\n\ntorch.save(dict(features=val_features, targets=val_targets), \"val_features.pt\")\n\n\ntst_features = []\ntst_targets = []\n\ntesting_dl = DataLoader(testing_ds, batch_size=batch_size, num_workers=8, worker_init_fn=s3dataset_worker_init_fn)\n\nfor i, (x, y, _) in tqdm(enumerate(testing_dl)):\n    if i &gt;= 50:\n        break\n\n    b, c, h, w = x.shape\n    x_t = model_transforms(torch.tile(x.reshape(-1, 1, h, w), (1, 3, 1, 1)))\n\n    if torch.cuda.is_available():\n        x_t = x_t.cuda()\n\n    with torch.no_grad():\n        x_out = model(x_t)\n        x_out = x_out.detach().reshape(-1, c, 576, 7, 7).sum(dim=1)\n        x_out = org_avgpool(x_out).detach().reshape(b, -1)\n\n    tst_features.append(x_out)\n    tst_targets.append(y)\n\ntst_features = torch.cat(tst_features, dim=0)\n\n# The labels are mapped as NONE/DMSO = 0, ORF = 1, CRISPS = 2, and COMPUND = 3\ntst_targets = torch.cat(tst_targets, dim=0)\n\n\n\n\nException ignored in: Exception ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\nException ignored in: Exception ignored in: Exception ignored in: \nTraceback (most recent call last):\n&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n\n&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;Exception ignored in:         Traceback (most recent call last):\n\n\n&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;self._shutdown_workers()self._shutdown_workers()Exception ignored in: Exception ignored in: Traceback (most recent call last):\n\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\nTraceback (most recent call last):\n&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;    \n    \n            self._shutdown_workers()if w.is_alive():self._shutdown_workers()Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\nif w.is_alive():self._shutdown_workers()\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n\n\n\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n\n       File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n      AssertionError  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n self._shutdown_workers()self._shutdown_workers()         : \n     if w.is_alive():\nif w.is_alive(): if w.is_alive(): can only test a child process  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n\n\n\n\n               if w.is_alive():if w.is_alive():  ^\n\n    ^  ^  ^ ^     ^^   ^ ^   ^   ^^^^ ^^ ^^  ^^^^ ^  ^^^^^^^^ ^^^^^^^^^\n^^^^^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^^^^^^    ^^^^^\nassert self._parent_pid == os.getpid(), 'can only test a child process'^^^^^^\n^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n ^^^^^^^\n^^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^^\n\n    ^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^    \nassert self._parent_pid == os.getpid(), 'can only test a child process'^    \n \n  ^assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n \n          ^  ^assert self._parent_pid == os.getpid(), 'can only test a child process' ^ \n                      assert self._parent_pid == os.getpid(), 'can only test a child process'  \n    ^^ ^   ^ ^ ^  ^   ^  ^  ^ ^^^^^^\n^^^  ^ ^ ^ ^ ^ ^^^^^^ ^ ^^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^    ^^^ ^^^^^^assert self._parent_pid == os.getpid(), 'can only test a child process'^^^ ^^^^^^^^\n^^^^^^^^ ^^^^ ^^^^^ ^^^^^^^^^^^^^^^ ^^^^^ ^^^^^ ^^^^^ ^^^^^^ ^^^^^^^^^^^^^^^ ^^^ ^ ^^^^^^^^^^^^^^^^^^^^\n^^^AssertionError^^^^^^: ^^^can only test a child process^^^\n^^AssertionError^^^^: ^^^^can only test a child process^\n^\n^^\n^^^^^AssertionError^^^: \n^^^^AssertionErrorcan only test a child process^: ^^can only test a child process\n^^\n^^^^^^^\nAssertionError: \ncan only test a child process\n^AssertionError: ^can only test a child process^\n^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n    if w.is_alive():\n    Exception ignored in:  &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\nTraceback (most recent call last):\n Exception ignored in:   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;     ^\nTraceback (most recent call last):\nself._shutdown_workers()  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n^\n    ^^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n^Exception ignored in:     ^&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;self._shutdown_workers()if w.is_alive():\n\n^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n^ ^ ^    ^ \nif w.is_alive(): Traceback (most recent call last):\n^\n   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\nException ignored in:  \n  &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;      File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n     self._shutdown_workers()assert self._parent_pid == os.getpid(), 'can only test a child process' \n^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n\n^  Traceback (most recent call last):\n      ^   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n^if w.is_alive():^\n       ^self._shutdown_workers() ^Exception ignored in:   \n^&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt; ^^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n ^\n^  Traceback (most recent call last):\n^     if w.is_alive(): ^ \n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n  ^ ^  ^     \n^ self._shutdown_workers()^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^^ ^\n^    ^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n^ ^assert self._parent_pid == os.getpid(), 'can only test a child process'    ^ ^^^\n^if w.is_alive(): ^^^ ^\n ^\n   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^^  ^      ^^^^assert self._parent_pid == os.getpid(), 'can only test a child process' ^ ^^\n ^^^^ ^^\n^  ^   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n ^     ^^   assert self._parent_pid == os.getpid(), 'can only test a child process'^ ^ ^ ^^\n^^^ ^^^ ^^^ ^ ^^ ^^ ^^ \n^^^ Exception ignored in: ^^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n  ^  ^&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;      ^^ ^^assert self._parent_pid == os.getpid(), 'can only test a child process'\n ^\n^^ ^Traceback (most recent call last):\n^^^  ^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n^^^ ^^    ^ ^^^self._shutdown_workers()\n ^^ ^AssertionError^\n^: ^^ ^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n^ can only test a child process\n^   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n\n^    ^     ^^ ^assert self._parent_pid == os.getpid(), 'can only test a child process'if w.is_alive():^^\n^\n ^^^  ^^^^ ^^^^ ^ ^^  ^^^Exception ignored in:   &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;^^^ ^\n^ ^^Traceback (most recent call last):\n^^ ^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n^^  ^ ^    ^ ^^self._shutdown_workers() ^^\n^^^ ^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n^^^^^^^^^^^    ^^^^^^^^^if w.is_alive():^^^^^\n^^ ^^^^^ ^^^^^ ^^^^^ ^^^^^^ ^^^^^\n^ ^^^^ AssertionError\n^^^AssertionError^\n^^: : ^^^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^can only test a child process^can only test a child process^\n^^^    \n^^assert self._parent_pid == os.getpid(), 'can only test a child process'\n^^Exception ignored in: ^^^^ ^&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt; ^^ Exception ignored in: ^^ &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;^ ^ ^^\n\n^AssertionErrorTraceback (most recent call last):\n : \n^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^    Traceback (most recent call last):\n^     can only test a child process^^ self._shutdown_workers()\nassert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n\n ^^\n\n     ^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\nAssertionError^self._shutdown_workers()Exception ignored in:  ^Exception ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;    \n^if w.is_alive():^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\n:  \n\n     Traceback (most recent call last):\ncan only test a child process  ^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n\n Traceback (most recent call last):\n       File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n^if w.is_alive():self._shutdown_workers()^       ^\n self._shutdown_workers()  \nException ignored in: ^\n ^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n ^^^ ^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;^    ^\n      ^ if w.is_alive():AssertionErrorif w.is_alive(): ^\n ^ ^: \nTraceback (most recent call last):\n\n^^can only test a child process  ^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n ^  ^    ^^\n ^^ ^self._shutdown_workers()^ ^^\n ^^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n^ ^^^ Exception ignored in:     ^^^  ^^^if w.is_alive(): ^ &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;^^^^^^\n^ ^^ ^^^^ \n ^^^^Traceback (most recent call last):\n ^^^^   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n^^ ^^^    ^^^^ ^self._shutdown_workers()^^ ^^^^^\n^^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n\n^^^^^    ^^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^^^^if w.is_alive():^^^^\n\n^^    ^\n^^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n ^^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\nassert self._parent_pid == os.getpid(), 'can only test a child process'^ ^^    \n^^     ^^ assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'^^^ ^ \n\n ^\n ^  ^   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n\n^  ^AssertionError^      : ^^ assert self._parent_pid == os.getpid(), 'can only test a child process'   \n^ \ncan only test a child process   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n\n   ^  ^      ^   assert self._parent_pid == os.getpid(), 'can only test a child process'   ^  ^ \nException ignored in:   \n ^^&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;   AssertionError^ \n ^ :  ^   ^can only test a child process Traceback (most recent call last):\n^ ^^\n  ^^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n^ ^^^^ ^^^^ ^^    ^^^Exception ignored in: ^ ^\n&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;self._shutdown_workers()^^^^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^\n^\n^ Traceback (most recent call last):\n^^^       File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n^assert self._parent_pid == os.getpid(), 'can only test a child process'^^      File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n^\n^ ^^    ^^^if w.is_alive():  ^self._shutdown_workers()\n^ ^\n^^^ ^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n ^^^     ^^ if w.is_alive():^ ^^ ^^^\n^ ^ ^  ^ ^^^   ^^^^ ^^^ ^^^ ^^^^ ^^ ^^^ ^^^^ ^^^ ^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^^^^^^AssertionError^^^^^\n: ^^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^^^^can only test a child process^^    ^^\n\n^^^AssertionError^^assert self._parent_pid == os.getpid(), 'can only test a child process'^^^: \n\n^^can only test a child process^^\n\n^AssertionError\n^^: Exception ignored in: AssertionError   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n: ^&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;can only test a child process^ can only test a child process\n    \nassert self._parent_pid == os.getpid(), 'can only test a child process'\n Exception ignored in:  ^\n^ &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt; ^\n^   Exception ignored in:  Traceback (most recent call last):\n^Traceback (most recent call last):\n^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n\n^ &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;Exception ignored in:   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n    AssertionError^\n  &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;    : Traceback (most recent call last):\nself._shutdown_workers()\n ^   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n^can only test a child process \nself._shutdown_workers() ^Traceback (most recent call last):\n \n\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n^   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n      ^^    if w.is_alive():    ^^if w.is_alive():^\n^self._shutdown_workers()^^ \nException ignored in: ^^    &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;\n self._shutdown_workers() ^\n^^   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\nTraceback (most recent call last):\n ^ ^ \n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n      ^^AssertionError      ^if w.is_alive():^: \n self._shutdown_workers() \n^^can only test a child process  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n^^\n  ^    \n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n^^if w.is_alive(): ^^^\n ^    ^Exception ignored in: ^ ^^^if w.is_alive():  ^&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;^ \n^^^ \nTraceback (most recent call last):\n^^ ^  ^^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n ^^^^     ^^^^^ ^^  ^ ^^^  ^^ ^^^^^ \n^^^^^^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^^self._shutdown_workers()^    ^^^^\n^assert self._parent_pid == os.getpid(), 'can only test a child process'^^^\n^^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n      ^^^if w.is_alive():^^^\n ^^\n^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^  ^^      ^^^^^^ \n^ ^assert self._parent_pid == os.getpid(), 'can only test a child process'  ^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^\n^^ ^^^^  ^^      ^assert self._parent_pid == os.getpid(), 'can only test a child process'^   ^\n^^^  ^^ ^\n^  ^\n ^^AssertionError \n^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n ^ ^\n       File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n: ^ AssertionError^^assert self._parent_pid == os.getpid(), 'can only test a child process'can only test a child process : ^  ^\ncan only test a child process ^    \n^\n ^assert self._parent_pid == os.getpid(), 'can only test a child process'^    ^^ ^^\n^^   ^ ^^ ^ Exception ignored in:  ^^^ &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;^^^ \n  ^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^\n^ ^     ^Traceback (most recent call last):\n ^assert self._parent_pid == os.getpid(), 'can only test a child process'^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n^^ ^\n^ ^^     ^^ ^  ^self._shutdown_workers()  ^^^ ^ \n^^^^ ^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n ^^    ^^^^ ^^^if w.is_alive():^^ ^\n^^^ ^ ^^^^ ^^  ^^^^ ^ ^^ ^^^ ^^ ^  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^^^^^^^^^^^^^^^^^^AssertionError\n^: ^^^AssertionError^^: ^can only test a child process^^^^^^can only test a child process\n^^^^\n^^^^^^^^^\n^^^^^^^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^^^^    ^^\n^assert self._parent_pid == os.getpid(), 'can only test a child process'^^^\nAssertionError^: ^ ^\ncan only test a child process AssertionError\n ^ ^: can only test a child process^^\n^ ^^^ ^ ^Exception ignored in: \n &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7fffa43334c0&gt;^\nAssertionError ^Traceback (most recent call last):\n: ^ ^   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\ncan only test a child process^^    ^\nself._shutdown_workers()^^^\n^^^  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n\n    if w.is_alive():^AssertionError\n: ^can only test a child process\n  ^     ^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^  ^^ ^^ ^ ^  ^ ^ ^\n  AssertionError^: can only test a child process^\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n\n\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_L23_T0001F008L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_L03_T0001F007L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_O07_T0001F008L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_N12_T0001F008L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_G18_T0001F009L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_I07_T0001F007L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_P09_T0001F008L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_L10_T0001F008L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_C17_T0001F008L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_I05_T0001F008L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_A08_T0001F007L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_K03_T0001F009L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_I02_T0001F009L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_N16_T0001F008L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_E02_T0001F009L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_K12_T0001F009L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_F24_T0001F007L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_J01_T0001F009L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_A17_T0001F009L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_L17_T0001F008L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_I02_T0001F009L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_B16_T0001F008L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_F12_T0001F009L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_O08_T0001F009L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_K02_T0001F009L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_D17_T0001F008L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_M21_T0001F007L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_D20_T0001F008L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_D22_T0001F008L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_M21_T0001F009L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_C20_T0001F009L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_K03_T0001F008L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-161651/Dest210628-161651_M24_T0001F007L01A01Z01C01.tif\n\n\n\ntorch.save(dict(features=tst_features, targets=tst_targets), \"tst_features.pt\")"
  },
  {
    "objectID": "notes/CellPaintint_for_ML.html#load-the-image-from-the-aws-bucket",
    "href": "notes/CellPaintint_for_ML.html#load-the-image-from-the-aws-bucket",
    "title": "Install required packages to access images from S3 storage",
    "section": "Load the image from the AWS bucket",
    "text": "Load the image from the AWS bucket\n\nx_comp, y_comp = load_well(comp_plate_map, wells_metadata, 1, 1, 0, 5, s3)"
  },
  {
    "objectID": "notes/CellPaintint_for_ML.html#add-a-dummy-axis-to-treat-a-single-sample-as-a-batch-of-size-one",
    "href": "notes/CellPaintint_for_ML.html#add-a-dummy-axis-to-treat-a-single-sample-as-a-batch-of-size-one",
    "title": "Install required packages to access images from S3 storage",
    "section": "Add a dummy axis to treat a single sample as a batch of size one",
    "text": "Add a dummy axis to treat a single sample as a batch of size one\n\nx_comp = torch.from_numpy(x_comp[None, ...])\n\n\nx_comp.shape, x_comp.dtype, y_comp"
  },
  {
    "objectID": "notes/CellPaintint_for_ML.html#extract-features-with-the-baseline-model",
    "href": "notes/CellPaintint_for_ML.html#extract-features-with-the-baseline-model",
    "title": "Install required packages to access images from S3 storage",
    "section": "Extract features with the baseline model",
    "text": "Extract features with the baseline model\n\nb, c, h, w = x_comp.shape\nx_comp_t = model_transforms(torch.tile(x_comp.reshape(-1, 1, h, w), (1, 3, 1, 1)))\n\nwith torch.no_grad():\n    x_out = model(x_comp_t)\n    x_out = x_out.detach().reshape(-1, c, 576, 7, 7).sum(dim=1)\n    x_out = org_avgpool(x_out).detach().reshape(b, -1)"
  },
  {
    "objectID": "notes/CellPaintint_for_ML.html#predict-the-type-of-perturbation-with-the-classifier-model",
    "href": "notes/CellPaintint_for_ML.html#predict-the-type-of-perturbation-with-the-classifier-model",
    "title": "Install required packages to access images from S3 storage",
    "section": "Predict the type of perturbation with the classifier model",
    "text": "Predict the type of perturbation with the classifier model\n\nclassifier.eval()\nwith torch.no_grad():\n    y_pred_comp = classifier(x_out)\n    fx_comp = classifier[0](x_out)\n\n\ny_pred_comp.argmax(), class_names[y_pred_comp.argmax().item()],  class_names[y_comp]\n\n\nmarkers = ['o', 's', '^', 'v']\nfor y_idx, class_name in enumerate(class_names):\n    plt.scatter(x=fx_trn[y_trn == y_idx, 0], y=fx_trn[y_trn == y_idx, 1], marker=markers[y_idx], label=class_name)\n\nplt.scatter(x=fx_comp[0, 0], y=fx_comp[0, 1], marker=\"x\", label=\"Test\")\n\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#review-the-metadata-of-the-cpg0016-jump-dataset",
    "href": "notes/Adv_ML_Python_presentation_2.html#review-the-metadata-of-the-cpg0016-jump-dataset",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Review the metadata of the cpg0016-jump dataset",
    "text": "Review the metadata of the cpg0016-jump dataset\n\nLoad the plate-level metadata from the jump-cellpainting/datasets repository\n\n\nimport pandas as pd\n\n\njump_plates_metadata = pd.read_csv(\"datasets/metadata/plate.csv.gz\")\njump_plates_metadata[\"Metadata_PlateType\"].unique()\n\narray(['COMPOUND_EMPTY', 'COMPOUND', 'DMSO', 'TARGET2', 'CRISPR', 'ORF',\n       'TARGET1', 'POSCON8'], dtype=object)\n\n\n\nInspect the structure of the database\n\n\njump_plates_metadata.groupby([\"Metadata_Source\", \"Metadata_Batch\"]).describe()\n\n\n  \n    \n\n\n\n\n\n\n\nMetadata_Plate\nMetadata_PlateType\n\n\n\n\ncount\nunique\ntop\nfreq\ncount\nunique\ntop\nfreq\n\n\nMetadata_Source\nMetadata_Batch\n\n\n\n\n\n\n\n\n\n\n\n\nsource_1\nBatch1_20221004\n9\n9\nUL000109\n1\n9\n2\nCOMPOUND\n6\n\n\nBatch2_20221006\n7\n7\nUL001647\n1\n7\n1\nCOMPOUND\n7\n\n\nBatch3_20221010\n8\n8\nUL000087\n1\n8\n1\nCOMPOUND\n8\n\n\nBatch4_20221012\n8\n8\nUL000081\n1\n8\n1\nCOMPOUND\n8\n\n\nBatch5_20221030\n11\n11\nUL000561\n1\n11\n2\nCOMPOUND\n10\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nsource_9\n20210918-Run11\n9\n9\nGR00004367\n1\n9\n2\nCOMPOUND\n8\n\n\n20210918-Run12\n8\n8\nGR00004377\n1\n8\n1\nCOMPOUND\n8\n\n\n20211013-Run14\n13\n13\nGR00003279\n1\n13\n2\nCOMPOUND\n12\n\n\n20211102-Run15\n11\n11\nGR00004391\n1\n11\n2\nCOMPOUND\n10\n\n\n20211103-Run16\n17\n17\nGR00004405\n1\n17\n2\nCOMPOUND\n16\n\n\n\n\n149 rows × 8 columns"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#subset-the-dataset-to-extract-samples-with-crispr-orf-and-nonedmso-treatments",
    "href": "notes/Adv_ML_Python_presentation_2.html#subset-the-dataset-to-extract-samples-with-crispr-orf-and-nonedmso-treatments",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Subset the dataset to extract samples with CRISPR, ORF, and NONE/DMSO treatments",
    "text": "Subset the dataset to extract samples with CRISPR, ORF, and NONE/DMSO treatments\n\nRead the CRISPR plate maps from the JUMP-Target repository\n\n\ncrispr_wells_metadata = pd.read_csv(\"JUMP-Target/JUMP-Target-1_crispr_platemap.tsv\", sep=\"\\t\")\n\n\nAdd one column to identify the type of treatment, and other to assign a numeric label (CRISPR = 1)\n\n\ncrispr_wells_metadata[\"Plate_type\"] = \"CRISPR\"\ncrispr_wells_metadata[\"Plate_label\"] = 1\ncrispr_wells_metadata\n\n\n  \n    \n\n\n\n\n\n\nwell_position\nbroad_sample\nPlate_type\nPlate_label\n\n\n\n\n0\nA01\nBRDN0001480888\nCRISPR\n1\n\n\n1\nA02\nBRDN0001483495\nCRISPR\n1\n\n\n2\nA03\nBRDN0001147364\nCRISPR\n1\n\n\n3\nA04\nBRDN0001490272\nCRISPR\n1\n\n\n4\nA05\nBRDN0001480510\nCRISPR\n1\n\n\n...\n...\n...\n...\n...\n\n\n379\nP20\nBRDN0001145303\nCRISPR\n1\n\n\n380\nP21\nBRDN0001484228\nCRISPR\n1\n\n\n381\nP22\nBRDN0001487618\nCRISPR\n1\n\n\n382\nP23\nBRDN0001487864\nCRISPR\n1\n\n\n383\nP24\nBRDN0000735603\nCRISPR\n1\n\n\n\n\n384 rows × 4 columns"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#get-the-url-of-each-assay-plate-from-the-s3-bucket",
    "href": "notes/Adv_ML_Python_presentation_2.html#get-the-url-of-each-assay-plate-from-the-s3-bucket",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Get the URL of each assay plate from the S3 bucket",
    "text": "Get the URL of each assay plate from the S3 bucket\n\nList the plates in the cellpainting-gallery/cpg0016-jump bucket\n\n\nimport s3fs\n\nfs = s3fs.S3FileSystem(anon=True)\n\nbatch_names = {}\nplate_paths = {}\nsource_names = {}\nplate_types = {}\n\nfor _, src_row in jump_plates_metadata.groupby([\"Metadata_Source\", \"Metadata_Batch\"]).describe().iterrows():\n    source_name, batch_name = src_row.name\n\n    # Ignore 'source_8' since the naming of the images is not standard\n    if source_name in [\"source_8\"]:\n        continue\n\n    plate_type = src_row[\"Metadata_PlateType\"].top\n\n    for plate_path in fs.ls(f\"cellpainting-gallery/cpg0016-jump/{source_name}/images/{batch_name}/images/\"):\n        plate_path = plate_path.split(\"/\")[-1]\n        if not plate_path:\n            continue\n\n        plate_name = plate_path.split(\"__\")[0]\n\n        source_names[plate_name] = source_name\n        batch_names[plate_name] = batch_name\n        plate_types[plate_name] = plate_type\n        plate_paths[plate_name] = plate_path"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#subset-the-data-frame-to-separate-perturbation-crisprorfnone-plates-from-compound-plates",
    "href": "notes/Adv_ML_Python_presentation_2.html#subset-the-data-frame-to-separate-perturbation-crisprorfnone-plates-from-compound-plates",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Subset the data frame to separate perturbation (CRISPR/ORF/NONE) plates from COMPOUND plates",
    "text": "Subset the data frame to separate perturbation (CRISPR/ORF/NONE) plates from COMPOUND plates\n\nSubset the COMPOUND plates\n\n\ncomp_plate_maps = plate_maps.query(\"Plate_type=='COMPOUND'\")\ncomp_plate_maps\n\n\n  \n    \n\n\n\n\n\n\nPlate_name\nSource_name\nBatch_name\nPlate_type\nPlate_path\n\n\n\n\n0\nUL000109\nsource_1\nBatch1_20221004\nCOMPOUND\nUL000109__2022-10-05T06_35_06-Measurement1\n\n\n1\nUL001641\nsource_1\nBatch1_20221004\nCOMPOUND\nUL001641__2022-10-04T23_16_28-Measurement1\n\n\n2\nUL001643\nsource_1\nBatch1_20221004\nCOMPOUND\nUL001643__2022-10-04T18_52_42-Measurement2\n\n\n3\nUL001645\nsource_1\nBatch1_20221004\nCOMPOUND\nUL001645__2022-10-05T00_44_11-Measurement1\n\n\n4\nUL001651\nsource_1\nBatch1_20221004\nCOMPOUND\nUL001651__2022-10-04T20_20_52-Measurement1\n\n\n...\n...\n...\n...\n...\n...\n\n\n2333\nGR00004417\nsource_9\n20211103-Run16\nCOMPOUND\nGR00004417\n\n\n2334\nGR00004418\nsource_9\n20211103-Run16\nCOMPOUND\nGR00004418\n\n\n2335\nGR00004419\nsource_9\n20211103-Run16\nCOMPOUND\nGR00004419\n\n\n2336\nGR00004420\nsource_9\n20211103-Run16\nCOMPOUND\nGR00004420\n\n\n2337\nGR00004421\nsource_9\n20211103-Run16\nCOMPOUND\nGR00004421\n\n\n\n\n1905 rows × 5 columns\n\n    \n      \n  \n    \n      \n  \n    \n  \n    \n    \n  \n\n    \n  \n  \n    \n  \n  \n\n\n\nSubset the CRISPR/ORF/DMSO plates\n\n\npert_plate_maps = plate_maps[plate_maps[\"Plate_type\"].isin([\"CRISPR\", \"ORF\", \"DMSO\"])]\npert_plate_maps\n\n\n  \n    \n\n\n\n\n\n\nPlate_name\nSource_name\nBatch_name\nPlate_type\nPlate_path\n\n\n\n\n142\nDest210628-161651\nsource_10\n2021_06_28_U2OS_48_hr_run9\nDMSO\nDest210628-161651\n\n\n143\nDest210628-162003\nsource_10\n2021_06_28_U2OS_48_hr_run9\nDMSO\nDest210628-162003\n\n\n457\nCP-CC9-R1-01\nsource_13\n20220914_Run1\nCRISPR\nCP-CC9-R1-01\n\n\n458\nCP-CC9-R1-02\nsource_13\n20220914_Run1\nCRISPR\nCP-CC9-R1-02\n\n\n459\nCP-CC9-R1-03\nsource_13\n20220914_Run1\nCRISPR\nCP-CC9-R1-03\n\n\n...\n...\n...\n...\n...\n...\n\n\n1591\nBR00127145\nsource_4\n2021_08_30_Batch13\nORF\nBR00127145__2021-09-22T04_01_46-Measurement1\n\n\n1592\nBR00127146\nsource_4\n2021_08_30_Batch13\nORF\nBR00127146__2021-09-22T12_25_07-Measurement1\n\n\n1593\nBR00127147\nsource_4\n2021_08_30_Batch13\nORF\nBR00127147__2021-09-18T10_27_12-Measurement1\n\n\n1594\nBR00127148\nsource_4\n2021_08_30_Batch13\nORF\nBR00127148__2021-09-21T11_44_23-Measurement1\n\n\n1595\nBR00127149\nsource_4\n2021_08_30_Batch13\nORF\nBR00127149__2021-09-18T02_10_04-Measurement1\n\n\n\n\n433 rows × 5 columns"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#split-the-perturbation-plates-into-training-validation-and-test-sets",
    "href": "notes/Adv_ML_Python_presentation_2.html#split-the-perturbation-plates-into-training-validation-and-test-sets",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Split the perturbation plates into Training, Validation, and Test sets",
    "text": "Split the perturbation plates into Training, Validation, and Test sets\nWe’ll separate the plates in each batch into the three sets to have batch-level effects in each of the sets\n\nAssign \\(70\\) % of plates for training, \\(20\\) % for validation, and \\(10\\) % for testing\n\n\nimport random\nimport math\n\n\ntrn_plates = []\nval_plates = []\ntst_plates = []\n\ntrn_proportion = 0.7\nval_proportion = 0.2\ntst_proportion = 0.1\n\nfor batch_name in pert_plate_maps[\"Batch_name\"].unique():\n    plate_names = pert_plate_maps.query(f\"Batch_name == '{batch_name}'\")[\"Plate_name\"].tolist()\n    random.shuffle(plate_names)\n\n    tst_plates_count = int(math.ceil(len(plate_names) * tst_proportion))\n    val_plates_count = int(math.ceil(len(plate_names) * val_proportion))\n\n    tst_plates += plate_names[:tst_plates_count]\n    val_plates += plate_names[tst_plates_count:tst_plates_count + val_plates_count]\n    trn_plates += plate_names[tst_plates_count + val_plates_count:]\n\n\nprint(\"Training set size:\", len(trn_plates))\nprint(\"Validation set size:\", len(val_plates))\nprint(\"Testing set size:\", len(tst_plates))\n\nTraining set size: 283\nValidation set size: 96\nTesting set size: 54"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#create-a-pytorch-dataset-to-load-images-from-s3-storage",
    "href": "notes/Adv_ML_Python_presentation_2.html#create-a-pytorch-dataset-to-load-images-from-s3-storage",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Create a PyTorch Dataset to load images from S3 storage",
    "text": "Create a PyTorch Dataset to load images from S3 storage\nDefining a custom PyTorch dataset allows us to access the image data from S3 storage, even if it is not in a standard format across the distinct sources inside the database. Moreover, it is completely iterative, so no additional storage is used as the images are analyzed on the fly.\n\n# @title Definition of a Dataset class capable to pull images from S3 buckets\nimport random\nimport numpy as np\nimport string\nimport s3fs\n\nfrom itertools import product\n\nfrom PIL import Image\nimport tifffile\n\nfrom torch.utils.data import IterableDataset, get_worker_info\n\n\nDefine a function to load an image from the cellpainting-gallery bucket given a field-well position from a specific plate\n\n\ndef load_well(plate_metadata, well_row, well_col, field_id, channels, s3):\n    # Get the label of the current well\n    curr_well_image = []\n\n    plate_path = \"cellpainting-gallery/cpg0016-jump/\" + plate_metadata[\"Source_name\"] + \"/images/\" + plate_metadata[\"Batch_name\"] + \"/images/\" + plate_metadata[\"Plate_path\"]\n\n    for channel_id in range(channels):\n        if plate_metadata[\"Source_name\"] in [\"source_1\", \"source_3\", \"source_4\", \"source_9\", \"source_11\", \"source_15\"]:\n            image_suffix = f\"Images/r{well_row + 1:02d}c{well_col + 1:02d}f{field_id + 1:02d}p01-ch{channel_id + 1}sk1fk1fl1.tiff\"\n\n        else:\n            if plate_metadata[\"Source_name\"] in [\"source_2\", \"source_5\"]:\n                a_locs = [1, 2, 3, 4, 5]\n            elif plate_metadata[\"Source_name\"] in [\"source_6\", \"source_10\"]:\n                a_locs = [1, 2, 2, 3, 1, 4]\n            elif plate_metadata[\"Source_name\"] in [\"source_7\", \"source_13\"]:\n                a_locs = [1, 1, 2, 3, 4]\n\n            image_suffix = f\"{plate_metadata[\"Plate_name\"]}_{string.ascii_uppercase[well_row]}{well_col + 1:02d}_T0001F{field_id + 1:03d}L01A{a_locs[channel_id]:02d}Z01C{channel_id + 1:02d}.tif\"\n\n        image_url = \"s3://\" + plate_path + \"/\" + image_suffix\n\n        try:\n            with s3.open(image_url, 'rb') as f:\n                curr_image = tifffile.imread(f)\n\n        except FileNotFoundError:\n            print(\"Failed retrieving:\", image_url)\n            return None\n\n        curr_image = curr_image.astype(np.float32)\n        curr_image /= 2 ** 16 - 1\n\n        curr_well_image.append(curr_image)\n\n    curr_well_image = np.array(curr_well_image)\n\n    return curr_well_image"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#create-the-different-datasets-from-the-plates-lists",
    "href": "notes/Adv_ML_Python_presentation_2.html#create-the-different-datasets-from-the-plates-lists",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Create the different datasets from the plates lists",
    "text": "Create the different datasets from the plates lists\n\nInstantiate a TiffS3Dataset from the plates assigned for training, validation, and testing\n\n\ntraining_ds = TiffS3Dataset(pert_plate_maps, wells_metadata, trn_plates, 16, 24, 9, 5, shuffle=True)\nvalidation_ds = TiffS3Dataset(pert_plate_maps, wells_metadata, val_plates, 16, 24, 9, 5, shuffle=True)\ntesting_ds = TiffS3Dataset(pert_plate_maps, wells_metadata, tst_plates, 16, 24, 9, 5, shuffle=True)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#import-a-pre-trained-model-from-torchvision",
    "href": "notes/Adv_ML_Python_presentation_2.html#import-a-pre-trained-model-from-torchvision",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Import a pre-trained model from torchvision",
    "text": "Import a pre-trained model from torchvision\nWe’ll start with a pre-trained MobileNet model for feature extraction since it is lightweight and fast. This in terms of computation resources required to use this model.\nIn the literature, more complex models are used, such as Inception V3, DenseNet, or even Vision Transformers. However, these models require GPU acceleration to be efficiently applied.\n\nLoad a pre-trained MobileNet from the torchvision package\n\n\nfrom torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n\n\nweights = MobileNet_V3_Small_Weights.DEFAULT\nmodel = mobilenet_v3_small(weights=weights)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#modify-the-models-architecture-to-convert-it-into-a-feature-extraction-function",
    "href": "notes/Adv_ML_Python_presentation_2.html#modify-the-models-architecture-to-convert-it-into-a-feature-extraction-function",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Modify the model’s architecture to convert it into a feature extraction function",
    "text": "Modify the model’s architecture to convert it into a feature extraction function\n\nSave the original aggregation layer (Average Pooling) and replace it with an Identity layer\n\n\nimport torch\n\n\norg_avgpool = model.avgpool\nmodel.avgpool = torch.nn.Identity()\n\n\nReplace the original classifier layer with an Identity layer\n\n\nmodel.classifier = torch.nn.Identity()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#load-the-pre-processing-transforms-form-the-original-model",
    "href": "notes/Adv_ML_Python_presentation_2.html#load-the-pre-processing-transforms-form-the-original-model",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Load the pre-processing transforms form the original model",
    "text": "Load the pre-processing transforms form the original model\nWe need to apply the same transforms to the images that we feed to the model to have the expected behavior.\n\nGet the original transforms pipeline used when the model was originally trained\n\n\nmodel_transforms = weights.transforms()\nmodel_transforms\n\nImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#create-a-pytorch-dataloader",
    "href": "notes/Adv_ML_Python_presentation_2.html#create-a-pytorch-dataloader",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Create a PyTorch DataLoader",
    "text": "Create a PyTorch DataLoader\nA DataLoader takes a Dataset (or IterableDataset) and serves mini-batches of samples that can be used for model training or evaluation. It manages the mini-batch collation, and if enabled, the multi-thread loading of data.\n\nfrom torch.utils.data.dataloader import DataLoader\n\n\nbatch_size = 10\n\ntraining_dl = DataLoader(training_ds, batch_size=batch_size, num_workers=2, worker_init_fn=dataset_worker_init_fn)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#load-the-pre-extracted-features-to-train-the-classifier",
    "href": "notes/Adv_ML_Python_presentation_2.html#load-the-pre-extracted-features-to-train-the-classifier",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Load the pre-extracted features to train the classifier",
    "text": "Load the pre-extracted features to train the classifier\n\nDefine an IterableDataset to load the features from the trn_feature_XXX.pt files\n\n\nclass GCPStorageDataset(IterableDataset):\n    \"\"\"This dataset loads the features from Cloud Storage\n    \"\"\"\n    def __init__(self, features_url, reducer=None, shuffle=False):\n        super(GCPStorageDataset).__init__()\n\n        self._features_url = features_url\n        self._features_dict = None\n        self._reducer = reducer\n\n        self._shuffle = shuffle\n\n        self._worker_sel = slice(0, len(self._features_url))\n        self._worker_id = 0\n        self._num_workers = 1\n\n    def __iter__(self):\n        # Select the barcodes that correspond to this worker\n        self._features_url = self._features_url[self._worker_sel]\n\n        if self._shuffle:\n            random.shuffle(self._features_url)\n\n        for url in self._features_url:\n            features_dict = torch.load(url)\n            \n            if self._reducer is not None:\n                embeddings = reducer.transform(features_dict[\"features\"])\n\n            curr_n_samples = len(features_dict[\"features\"])\n\n            for index in range(curr_n_samples):\n                if self._shuffle:\n                    index = random.randrange(curr_n_samples)\n\n                feats = features_dict[\"features\"][index]\n                target = features_dict[\"targets\"][index]\n\n                if self._reducer is not None:\n                    reduced_feats = embeddings[index]\n                else:\n                    reduced_feats = None\n\n                yield feats, target, reduced_feats"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#inspect-the-distribution-of-the-feature-space",
    "href": "notes/Adv_ML_Python_presentation_2.html#inspect-the-distribution-of-the-feature-space",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Inspect the distribution of the feature space",
    "text": "Inspect the distribution of the feature space\n\nFit a UMap to the training set of features to reduce the \\(576\\) features into only \\(2\\) dimensions for visualization\n\n\nimport umap\n\n\nreducer = umap.UMAP()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#create-a-classifier-with-a-multilayer-perceptron-mlp-architecture",
    "href": "notes/Adv_ML_Python_presentation_2.html#create-a-classifier-with-a-multilayer-perceptron-mlp-architecture",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Create a classifier with a Multilayer Perceptron (MLP) architecture",
    "text": "Create a classifier with a Multilayer Perceptron (MLP) architecture\nThe MobileNet model extracts \\(576\\) features per image, these will be the input features for the MLP classifer.\n\nDefine a Multilayer Perceptron MLP classifier that takes the pre-extracted features and predicts the perturbation applied to that sample\n\n\nclass PerturbationClassifier(torch.nn.Module):\n    def __init__(self, num_features, num_classes):\n        super(PerturbationClassifier, self).__init__()\n\n        self._reducer = torch.nn.Sequential(\n            torch.nn.BatchNorm1d(num_features=num_features),\n            torch.nn.Linear(in_features=num_features, out_features=2, bias=False),\n        )\n\n        self._classifier = torch.nn.Sequential(\n            torch.nn.Dropout(0.1),\n            torch.nn.ReLU(),\n            torch.nn.Linear(in_features=2, out_features=num_classes, bias=False)\n        )\n\n    def forward(self, input):\n        fx = self._reducer(input)\n        \n        y_pred = self._classifier(fx)\n\n        return y_pred, fx"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#train-the-mlp-model",
    "href": "notes/Adv_ML_Python_presentation_2.html#train-the-mlp-model",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Train the MLP model",
    "text": "Train the MLP model\n\nInstantiate the datasets that will load the features from Cloud Storage\n\n\ntrn_feat_ds = GCPStorageDataset([f\"trn_features_{i:03d}.pt\" for i in range(10)], reducer=reducer, shuffle=True)\nval_feat_ds = GCPStorageDataset([\"val_features.pt\"], reducer=reducer, shuffle=False)\ntst_feat_ds = GCPStorageDataset([\"tst_features.pt\"], reducer=reducer, shuffle=False)\n\n\nCreate one DataLoader for the training set and one for the validation set of pre-extracted features\n\n\ntrn_feat_dl = DataLoader(trn_feat_ds, batch_size=100, num_workers=2, worker_init_fn=dataset_worker_init_fn)\nval_feat_dl = DataLoader(val_feat_ds, batch_size=100)\ntst_feat_dl = DataLoader(tst_feat_ds, batch_size=100)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#wrap-the-training-and-validation-steps-for-multiple-epochs",
    "href": "notes/Adv_ML_Python_presentation_2.html#wrap-the-training-and-validation-steps-for-multiple-epochs",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Wrap the training and validation steps for multiple epochs",
    "text": "Wrap the training and validation steps for multiple epochs\nTrack the performance of the model throughout the epochs during training\n\nCopy the training and validation steps into an epochs loop\n\n\navg_cls_loss_trn = []\navg_red_loss_trn = []\navg_acc_trn = []\n\navg_cls_loss_val = []\navg_red_loss_val = []\navg_acc_val = []\n\nn_epochs = 20\nq = tqdm(total=n_epochs)\n\nfor e in range(n_epochs):\n    # Training loop\n    classifier.train()\n\n    loss_epoch = 0\n\n    trn_acc_metric.reset()\n    \n    total_samples = 0\n    for x, y, fx in trn_feat_dl:\n        optimizer.zero_grad()\n\n        if torch.cuda.is_available():\n            x = x.cuda()\n\n        y_pred, fx_pred = classifier(x)\n\n        cls_loss = classifier_loss_fn(y_pred.cpu(), y)\n        red_loss = reducer_loss_fn(fx_pred.cpu(), fx)\n\n        cls_loss.backward(retain_graph=True)\n        red_loss.backward()\n\n        optimizer.step()\n\n        cls_loss_epoch += cls_loss.item() * len(y)\n        red_loss_epoch += red_loss.item() * len(y)\n\n        trn_acc_metric(y_pred.cpu().softmax(dim=1), y)\n        total_samples += len(y)\n\n    avg_cls_loss_trn.append(cls_loss_epoch / total_samples)\n    avg_red_loss_trn.append(red_loss_epoch / total_samples)\n\n    avg_acc_trn.append(trn_acc_metric.compute())\n\n    # Validation loop\n    classifier.eval()\n\n    cls_loss_epoch = 0\n    red_loss_epoch = 0\n\n    val_acc_metric.reset()\n\n    total_samples = 0\n    for x_val, y_val, fx_val in val_feat_dl:\n        with torch.no_grad():\n            if torch.cuda.is_available():\n                x_val = x_val.cuda()\n\n            y_val_pred, fx_val_pred = classifier(x_val)\n\n            cls_loss = classifier_loss_fn(y_val_pred.cpu(), y_val)\n            red_loss = reducer_loss_fn(fx_val_pred.cpu(), fx_val)\n\n        cls_loss_epoch += cls_loss.item() * len(y_val)\n        red_loss_epoch += red_loss.item() * len(y_val)\n\n        val_acc_metric(y_val_pred.cpu().softmax(dim=1), y_val)\n        total_samples += len(y_val)\n\n    avg_cls_loss_val.append(cls_loss_epoch / total_samples)\n    avg_red_loss_val.append(red_loss_epoch / total_samples)\n\n    avg_acc_val.append(val_acc_metric.compute())\n\n    q.set_description(f\"Average training CE loss: {avg_cls_loss_trn[-1]:0.4f} / MSE loss: {avg_red_loss_trn[-1]:0.4f} (Accuracy: {100 * avg_acc_trn[-1]:0.2f} %). Average validation CE loss: {avg_cls_loss_val[-1]:04f} / MSE loss: {avg_red_loss_val[-1]:04f} (Accuracy: {100 * avg_acc_val[-1]:0.2f} %)\")\n    q.update()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#review-the-performance-of-the-model-throughout-training",
    "href": "notes/Adv_ML_Python_presentation_2.html#review-the-performance-of-the-model-throughout-training",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Review the performance of the model throughout training",
    "text": "Review the performance of the model throughout training\n\nPlot the loss function evaluation for the training and validation sets\n\n\nplt.plot(avg_cls_loss_trn, \"k-\", label=\"Training loss\")\nplt.plot(avg_cls_loss_val, \"b:\", label=\"Validation loss\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(avg_red_loss_trn, \"k-\", label=\"Training loss\")\nplt.plot(avg_red_loss_val, \"b:\", label=\"Validation loss\")\nplt.legend()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#save-the-classifier-model-to-be-used-later-or-shared-with-collaborators",
    "href": "notes/Adv_ML_Python_presentation_2.html#save-the-classifier-model-to-be-used-later-or-shared-with-collaborators",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Save the classifier model to be used later or shared with collaborators",
    "text": "Save the classifier model to be used later or shared with collaborators\n\nEvaluate the classifier on the testing set to measure its generalization capacity\n\n\nfrom torchmetrics.classification import ConfusionMatrix\n\nclassifier.eval()\n\nn_dmso = 0\nn_crispr = 0\nn_orf = 0\n\ncls_loss_epoch = 0\nred_loss_epoch = 0\n\ntst_acc_metric = Accuracy(\"multiclass\", num_classes=3)\ntst_confmat = ConfusionMatrix(\"multiclass\", num_classes=3)\n\nfor x_tst, y_tst, fx_tst in tst_feat_dl:\n    with torch.no_grad():\n        if torch.cuda.is_available():\n            x_tst = x_tst.cuda()\n\n        y_tst_pred, fx_tst_pred = classifier(x_tst)\n        cls_loss = classifier_loss_fn(y_tst_pred.cpu(), y_tst)\n        red_loss = reducer_loss_fn(fx_tst_pred.cpu(), fx_tst)\n\n    cls_loss_epoch += cls_loss.item() * len(y_tst)\n    red_loss_epoch += red_loss.item() * len(y_tst)\n    \n    y_tst_prob = y_tst_pred.cpu().softmax(dim=1)\n    tst_acc_metric.update(y_tst_prob, y_tst)\n    tst_confmat.update(y_tst_prob, y_tst)\n\n    n_dmso += sum(y_tst == 0)\n    n_crispr += sum(y_tst == 1)\n    n_orf += sum(y_tst == 2)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#evaluate-the-capacity-to-mimic-the-umap-dimensionality-reduction",
    "href": "notes/Adv_ML_Python_presentation_2.html#evaluate-the-capacity-to-mimic-the-umap-dimensionality-reduction",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Evaluate the capacity to mimic the UMap dimensionality reduction",
    "text": "Evaluate the capacity to mimic the UMap dimensionality reduction\n\nGet the embeddings from the trained model for the training set\n\n\ntrn_fx = []\ntrn_fx_pred = []\ntrn_y = []\n\nfor i, (x, y, fx) in enumerate(trn_feat_dl):\n    if i &gt;= 10:\n        break\n\n    with torch.no_grad():\n        if torch.cuda.is_available():\n            x = x.cuda()\n\n        _, fx_pred = classifier(x)\n        trn_fx_pred.append(fx_pred.detach().cpu())\n        trn_fx.append(fx)\n        trn_y.append(y)\n\ntrn_fx = torch.cat(trn_fx, dim=0)\ntrn_fx_pred = torch.cat(trn_fx_pred, dim=0)\ntrn_y = torch.cat(trn_y, dim=0)"
  },
  {
    "objectID": "notes/datasets/LICENCE.html",
    "href": "notes/datasets/LICENCE.html",
    "title": "Advanced Machine Learning with Python workshop",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2025, JUMP-CP All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#get-the-url-of-each-assay-plate-from-the-s3-bucket-1",
    "href": "notes/Adv_ML_Python_presentation_2.html#get-the-url-of-each-assay-plate-from-the-s3-bucket-1",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Get the URL of each assay plate from the S3 bucket",
    "text": "Get the URL of each assay plate from the S3 bucket\n\nCreate a pandas data frame to associate each plate to its location within the S3 bucket\n\n\nplate_maps = pd.DataFrame()\n\nplate_maps[\"Plate_name\"] = batch_names.keys()\nplate_maps[\"Source_name\"] = plate_maps[\"Plate_name\"].map(source_names)\nplate_maps[\"Batch_name\"] = plate_maps[\"Plate_name\"].map(batch_names)\nplate_maps[\"Plate_type\"] = plate_maps[\"Plate_name\"].map(plate_types)\nplate_maps[\"Plate_path\"] = plate_maps[\"Plate_name\"].map(plate_paths)\n\nplate_maps\n\n\n  \n    \n\n\n\n\n\n\nPlate_name\nSource_name\nBatch_name\nPlate_type\nPlate_path\n\n\n\n\n0\nUL000109\nsource_1\nBatch1_20221004\nCOMPOUND\nUL000109__2022-10-05T06_35_06-Measurement1\n\n\n1\nUL001641\nsource_1\nBatch1_20221004\nCOMPOUND\nUL001641__2022-10-04T23_16_28-Measurement1\n\n\n2\nUL001643\nsource_1\nBatch1_20221004\nCOMPOUND\nUL001643__2022-10-04T18_52_42-Measurement2\n\n\n3\nUL001645\nsource_1\nBatch1_20221004\nCOMPOUND\nUL001645__2022-10-05T00_44_11-Measurement1\n\n\n4\nUL001651\nsource_1\nBatch1_20221004\nCOMPOUND\nUL001651__2022-10-04T20_20_52-Measurement1\n\n\n...\n...\n...\n...\n...\n...\n\n\n2333\nGR00004417\nsource_9\n20211103-Run16\nCOMPOUND\nGR00004417\n\n\n2334\nGR00004418\nsource_9\n20211103-Run16\nCOMPOUND\nGR00004418\n\n\n2335\nGR00004419\nsource_9\n20211103-Run16\nCOMPOUND\nGR00004419\n\n\n2336\nGR00004420\nsource_9\n20211103-Run16\nCOMPOUND\nGR00004420\n\n\n2337\nGR00004421\nsource_9\n20211103-Run16\nCOMPOUND\nGR00004421\n\n\n\n\n2338 rows × 5 columns"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#create-a-pytorch-dataset-to-load-images-from-s3-storage-1",
    "href": "notes/Adv_ML_Python_presentation_2.html#create-a-pytorch-dataset-to-load-images-from-s3-storage-1",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Create a PyTorch Dataset to load images from S3 storage",
    "text": "Create a PyTorch Dataset to load images from S3 storage\n\nDefine an IterableDataset derived class to load images from S3 storage\n\n\nclass TiffS3Dataset(IterableDataset):\n    \"\"\"This dataset could have virtually infinite samples.\n    \"\"\"\n    def __init__(self, plate_maps, wells_metadata, plate_names, well_rows=24, well_cols=16, fields=4, channels=5, shuffle=False):\n        super(TiffS3Dataset).__init__()\n\n        self._plate_maps = plate_maps\n        self._wells_metadata = wells_metadata\n\n        self._plate_names = plate_names\n        self._well_rows = well_rows\n        self._well_cols = well_cols\n        self._fields = fields\n        self._channels = channels\n\n        self._shuffle = shuffle\n\n        self._worker_sel = slice(0, len(plate_names) * self._well_rows * self._well_cols)\n        self._worker_id = 0\n        self._num_workers = 1\n\n        self._s3 = None\n\n    def __iter__(self):\n        # Select the barcodes that correspond to this worker\n        self._s3 = s3fs.S3FileSystem(anon=True)\n\n        self._plate_names = self._plate_names[self._worker_sel]\n\n        well_row_range = range(self._well_rows)\n        well_col_range = range(self._well_cols)\n        fields_range = range(self._fields)\n\n        for plate_name, well_row, well_col, field_id in product(self._plate_names, well_row_range, well_col_range, fields_range):\n            if self._shuffle:\n                plate_name = random.choice(self._plate_names)\n                well_row = random.randrange(self._well_rows)\n                well_col = random.randrange(self._well_cols)\n                field_id = random.randrange(self._fields)\n\n            curr_plate_map = self._plate_maps.query(f\"Plate_name == '{plate_name}'\")\n\n            curr_plate_metadata = curr_plate_map.to_dict(orient='records')[0]\n\n            if not len(curr_plate_metadata):\n                continue\n\n            curr_image = load_well(curr_plate_metadata, well_row, well_col, field_id, self._channels, self._s3)\n\n            if curr_image is None:\n                continue\n\n            curr_plate_metadata[\"Well_position\"] = f\"{string.ascii_uppercase[well_row]}{well_col + 1:02d}\"\n\n            curr_image = curr_image[:, :1080, :1080]\n            _, h, w = curr_image.shape\n            pad_h = 1080 - h\n            pad_w = 1080 - w\n\n            if pad_h or pad_w:\n                curr_image = np.pad(curr_image, ((0, 0), (0, pad_h), (0, pad_w)))\n            \n            if curr_plate_metadata[\"Plate_type\"] == \"DMSO\":\n                curr_label = 0\n\n            else:\n                curr_label = self._wells_metadata.query(f\"Plate_type=='{curr_plate_metadata[\"Plate_type\"]}' & well_position=='{string.ascii_uppercase[well_row]}{well_col + 1:02d}'\")[\"Plate_label\"]\n\n                if not len(curr_label):\n                    continue\n\n                curr_label = curr_label.item()\n\n            yield curr_image, curr_label, curr_plate_metadata\n\n        self._s3 = None"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#create-a-pytorch-dataset-to-load-images-from-s3-storage-2",
    "href": "notes/Adv_ML_Python_presentation_2.html#create-a-pytorch-dataset-to-load-images-from-s3-storage-2",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Create a PyTorch Dataset to load images from S3 storage",
    "text": "Create a PyTorch Dataset to load images from S3 storage\n\nDefine an initialization function to separate the load when using multi-thread data loading\n\n\ndef dataset_worker_init_fn(worker_id):\n    \"\"\"ZarrDataset multithread workers initialization function.\n    \"\"\"\n    worker_info = torch.utils.data.get_worker_info()\n    w_sel = slice(worker_id, None, worker_info.num_workers)\n\n    dataset_obj = worker_info.dataset\n\n    # Reset the random number generators in each worker.\n    torch_seed = torch.initial_seed()\n\n    dataset_obj._worker_sel = w_sel\n    dataset_obj._worker_id = worker_id\n    dataset_obj._num_workers = worker_info.num_workers"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#modify-the-models-architecture-to-convert-it-into-a-feature-extraction-function-1",
    "href": "notes/Adv_ML_Python_presentation_2.html#modify-the-models-architecture-to-convert-it-into-a-feature-extraction-function-1",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Modify the model’s architecture to convert it into a feature extraction function",
    "text": "Modify the model’s architecture to convert it into a feature extraction function\n\nSet the model into evaluation mode. (Optional) Move the model to the GPU if available\n\n\nif torch.cuda.is_available():\n    model.cuda()\n\nmodel.eval()\n\nMobileNetV3(\n  (features): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n      (2): Hardswish()\n    )\n    (1): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (1): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (2): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (3): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (4): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (5): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (6): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (7): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (8): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (9): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (10): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (11): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (12): Conv2dNormActivation(\n      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n      (2): Hardswish()\n    )\n  )\n  (avgpool): Identity()\n  (classifier): Identity()\n)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#inspect-the-distribution-of-the-feature-space-1",
    "href": "notes/Adv_ML_Python_presentation_2.html#inspect-the-distribution-of-the-feature-space-1",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Inspect the distribution of the feature space",
    "text": "Inspect the distribution of the feature space\n\nLoad the training features and fit the UMap reducer\n\n\ntrn_features_ds = GCPStorageDataset([\"trn_features_002.pt\"], shuffle=False)\n\ntrn_features, trn_targets, _ = list(zip(*trn_features_ds))\n\ntrn_targets = torch.tensor(trn_targets)\n\nreducer.fit(trn_features)\n\ntrn_embeddings = reducer.transform(trn_features)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#inspect-the-distribution-of-the-feature-space-2",
    "href": "notes/Adv_ML_Python_presentation_2.html#inspect-the-distribution-of-the-feature-space-2",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Inspect the distribution of the feature space",
    "text": "Inspect the distribution of the feature space\n\nVisualize the reduced feature space in two dimensions\n\n\nimport matplotlib.pyplot as plt\n\nclass_names = [\"NONE/DMSO\", \"CRISPR\", \"ORF\", \"COMPOUND\"]\nclass_markers = [\"*\", \"s\", \"o\", \"^\"]\nclass_colors = [\"black\", \"red\", \"blue\", \"green\"]\nclass_facecolors = [\"black\", \"none\", \"none\", \"none\"]\n\nfor class_idx, class_name in enumerate(class_names):\n    plt.scatter(trn_embeddings[trn_targets == class_idx, 0][::10], trn_embeddings[trn_targets == class_idx, 1][::10], label=class_names[class_idx], marker=class_markers[class_idx], facecolors=class_facecolors[class_idx], edgecolors=class_colors[class_idx])\n\nplt.legend()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#inspect-the-distribution-of-the-feature-space-3",
    "href": "notes/Adv_ML_Python_presentation_2.html#inspect-the-distribution-of-the-feature-space-3",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Inspect the distribution of the feature space",
    "text": "Inspect the distribution of the feature space\n\nConfirm that the validation set is mapped to a similar space as the training set\n\n\nval_features_ds = GCPStorageDataset([\"val_features.pt\"], shuffle=False)\n\nval_features, val_targets, _ = list(zip(*val_features_ds))\n\nval_targets = torch.tensor(val_targets)\n\nval_embedding = reducer.transform(val_features)\n\nval_embedding.shape\n\nfor class_idx, class_name in enumerate(class_names):\n    plt.scatter(val_embedding[val_targets == class_idx, 0][::10], val_embedding[val_targets == class_idx, 1][::10], label=class_names[class_idx], marker=class_markers[class_idx], facecolors=class_facecolors[class_idx], edgecolors=class_colors[class_idx])\n\nplt.legend()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#create-a-classifier-with-a-multilayer-perceptron-mlp-architecture-1",
    "href": "notes/Adv_ML_Python_presentation_2.html#create-a-classifier-with-a-multilayer-perceptron-mlp-architecture-1",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Create a classifier with a Multilayer Perceptron (MLP) architecture",
    "text": "Create a classifier with a Multilayer Perceptron (MLP) architecture\n\nInstantiate the classifier for the set of \\(576\\) features and \\(3\\) classes\n\n\nclassifier = PerturbationClassifier(576, 3)\n\n\n(Optional) If a GPU is available, move the classifier model to it\n\n\nif torch.cuda.is_available():\n    classifier.cuda()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#create-a-classifier-with-a-multilayer-perceptron-mlp-architecture-2",
    "href": "notes/Adv_ML_Python_presentation_2.html#create-a-classifier-with-a-multilayer-perceptron-mlp-architecture-2",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Create a classifier with a Multilayer Perceptron (MLP) architecture",
    "text": "Create a classifier with a Multilayer Perceptron (MLP) architecture\n\nDefine the optimizer algorithm\n\n\noptimizer = torch.optim.Adam([\n    {'params': classifier._reducer.parameters(), 'lr': 1e-5, 'weight_decay': 0.001},\n    {'params': classifier._classifier.parameters(), 'lr': 1e-4}\n])\n\n\nDefine the loss function used to assess the classification performance of the model\n\n\nclassifier_loss_fn = torch.nn.CrossEntropyLoss()\nreducer_loss_fn = torch.nn.MSELoss()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#train-the-mlp-model-1",
    "href": "notes/Adv_ML_Python_presentation_2.html#train-the-mlp-model-1",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Train the MLP model",
    "text": "Train the MLP model\n\nImplement the training loop\n\n\nfrom torchmetrics.classification import Accuracy\n\nn_dmso = 0\nn_crispr = 0\nn_orf = 0\n\n# Training loop\nclassifier.train()\n\ncls_loss_epoch = 0\nred_loss_epoch = 0\n\ntrn_acc_metric = Accuracy(task=\"multiclass\", num_classes=3)\n\nfor x, y, fx in tqdm(trn_feat_dl, total=1000):\n    optimizer.zero_grad()\n\n    if torch.cuda.is_available():\n        x = x.cuda()\n\n    y_pred, fx_pred = classifier(x)\n\n    cls_loss = classifier_loss_fn(y_pred.cpu(), y)\n    red_loss = reducer_loss_fn(fx_pred.cpu(), fx)\n\n    cls_loss.backward(retain_graph=True)\n    red_loss.backward()\n\n    optimizer.step()\n\n    cls_loss_epoch += cls_loss.item()\n    red_loss_epoch += red_loss.item()\n\n    trn_acc_metric(y_pred.cpu().softmax(dim=1), y)\n\n    n_dmso += sum(y == 0)\n    n_crispr += sum(y == 1)\n    n_orf += sum(y == 2)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#train-the-mlp-model-2",
    "href": "notes/Adv_ML_Python_presentation_2.html#train-the-mlp-model-2",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Train the MLP model",
    "text": "Train the MLP model\n\nCompute the proportion of samples per category in the training set\n\n\nn_total = n_dmso + n_crispr + n_orf\nn_dmso / n_total, n_crispr / n_total, n_orf / n_total\n\n(tensor(0.0107), tensor(0.3617), tensor(0.6276))\n\n\n\nCheck the average loss metric of the model on the training set\n\n\ncls_loss_epoch / n_total\n\ntensor(0.0090)\n\n\n\nred_loss_epoch / n_total\n\ntensor(0.8626)\n\n\n\nCompute the average accuracy of the model on the training set\n\n\ntrn_acc_metric.compute()\n\ntensor(0.5685)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#train-the-mlp-model-3",
    "href": "notes/Adv_ML_Python_presentation_2.html#train-the-mlp-model-3",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Train the MLP model",
    "text": "Train the MLP model\n\nImplement the validation loop\n\n\nn_dmso = 0\nn_crispr = 0\nn_orf = 0\n\ncls_loss_epoch = 0\nred_loss_epoch = 0\n\nval_acc_metric = Accuracy(task=\"multiclass\", num_classes=3)\n\nfor x_val, y_val, fx_val in tqdm(val_feat_dl):\n    with torch.no_grad():\n        if torch.cuda.is_available():\n            x_val = x_val.cuda()\n\n        y_val_pred, fx_val_pred = classifier(x_val)\n\n        cls_loss = classifier_loss_fn(y_val_pred.cpu(), y_val)\n        red_loss = reducer_loss_fn(fx_val_pred.cpu(), fx_val)\n\n    cls_loss_epoch += cls_loss.item()\n    red_loss_epoch += red_loss.item()\n    \n    val_acc_metric(y_val_pred.cpu().softmax(dim=1), y_val)\n\n    n_dmso += sum(y_val == 0)\n    n_crispr += sum(y_val == 1)\n    n_orf += sum(y_val == 2)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#train-the-mlp-model-4",
    "href": "notes/Adv_ML_Python_presentation_2.html#train-the-mlp-model-4",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Train the MLP model",
    "text": "Train the MLP model\n\nCompute the proportion of samples per category in the validation set\n\n\nn_total = n_dmso + n_crispr + n_orf\nn_dmso / n_total, n_crispr / n_total, n_orf / n_total\n\n(tensor(0.0182), tensor(0.3486), tensor(0.6332))\n\n\n\nCheck the average loss metric of the model on the validation set\n\n\ncls_loss_epoch / n_total\n\ntensor(0.0074)\n\n\n\nred_loss_epoch / n_total\n\ntensor(0.7850)\n\n\n\nCompute the average accuracy of the model on the validation set\n\n\nval_acc_metric.compute()\n\ntensor(0.7378)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#review-the-performance-of-the-model-throughout-training-1",
    "href": "notes/Adv_ML_Python_presentation_2.html#review-the-performance-of-the-model-throughout-training-1",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Review the performance of the model throughout training",
    "text": "Review the performance of the model throughout training\n\nPlot the accuracy of the model on the training and validation sets\n\n\nplt.plot(avg_acc_trn, \"k-\", label=\"Training accuracy\")\nplt.plot(avg_acc_val, \"b:\", label=\"Validation accuracy\")\nplt.legend()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#save-the-classifier-model-to-be-used-later-or-shared-with-collaborators-1",
    "href": "notes/Adv_ML_Python_presentation_2.html#save-the-classifier-model-to-be-used-later-or-shared-with-collaborators-1",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Save the classifier model to be used later or shared with collaborators",
    "text": "Save the classifier model to be used later or shared with collaborators\n\nCheck the proportion of types of perturbation in the testing set\n\n\nn_total = n_dmso + n_crispr + n_orf\nn_dmso / n_total, n_crispr / n_total, n_orf / n_total\n\n(tensor(0.0240), tensor(0.2986), tensor(0.6774))\n\n\n\nCheck the loss metrics on the testing set\n\n\ncls_loss_epoch / n_total\n\ntensor(0.1779)\n\n\n\nred_loss_epoch / n_total\n\ntensor(2.5287)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#save-the-classifier-model-to-be-used-later-or-shared-with-collaborators-2",
    "href": "notes/Adv_ML_Python_presentation_2.html#save-the-classifier-model-to-be-used-later-or-shared-with-collaborators-2",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Save the classifier model to be used later or shared with collaborators",
    "text": "Save the classifier model to be used later or shared with collaborators\n\nCheck the accuracy metrics on the testing set\n\n\ntst_acc_metric.compute()\ntst_confmat.compute()\ntst_confmat.plot()\n\n(&lt;Figure size 640x480 with 1 Axes&gt;,\n &lt;Axes: xlabel='Predicted class', ylabel='True class'&gt;)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#evaluate-the-capacity-to-mimic-the-umap-dimensionality-reduction-1",
    "href": "notes/Adv_ML_Python_presentation_2.html#evaluate-the-capacity-to-mimic-the-umap-dimensionality-reduction-1",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Evaluate the capacity to mimic the UMap dimensionality reduction",
    "text": "Evaluate the capacity to mimic the UMap dimensionality reduction\n\nCompate the embedding spaces (UMap vs learned) with the training set\n\n\nfor class_idx, class_name in enumerate(class_names):\n    plt.scatter(trn_fx[trn_y == class_idx, 0], trn_fx[trn_y == class_idx, 1], label=class_names[class_idx], marker=class_markers[class_idx], facecolors=class_facecolors[class_idx], edgecolors=class_colors[class_idx])\n\nplt.legend()\n\n\n\n\n\n\n\n\n\nfor class_idx, class_name in enumerate(class_names):\n    plt.scatter(trn_fx_pred[trn_y == class_idx, 0], trn_fx_pred[trn_y == class_idx, 1], label=class_names[class_idx], marker=class_markers[class_idx], facecolors=class_facecolors[class_idx], edgecolors=class_colors[class_idx])\n\nplt.legend()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#evaluate-the-capacity-to-mimic-the-umap-dimensionality-reduction-2",
    "href": "notes/Adv_ML_Python_presentation_2.html#evaluate-the-capacity-to-mimic-the-umap-dimensionality-reduction-2",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Evaluate the capacity to mimic the UMap dimensionality reduction",
    "text": "Evaluate the capacity to mimic the UMap dimensionality reduction\n\nGet the embeddings from the trained model for the validation set\n\n\nval_fx = []\nval_fx_pred = []\nval_y = []\n\nfor i, (x_val, y_val, fx_val) in enumerate(val_feat_dl):\n    if i &gt;= 10:\n        break\n\n    with torch.no_grad():\n        if torch.cuda.is_available():\n            x_val = x_val.cuda()\n\n        _, fx_pred_val = classifier(x_val)\n        val_fx_pred.append(fx_pred_val.detach().cpu())\n        val_fx.append(fx_val)\n        val_y.append(y_val)\n\nval_fx = torch.cat(val_fx, dim=0)\nval_fx_pred = torch.cat(val_fx_pred, dim=0)\nval_y = torch.cat(val_y, dim=0)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#evaluate-the-capacity-to-mimic-the-umap-dimensionality-reduction-3",
    "href": "notes/Adv_ML_Python_presentation_2.html#evaluate-the-capacity-to-mimic-the-umap-dimensionality-reduction-3",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Evaluate the capacity to mimic the UMap dimensionality reduction",
    "text": "Evaluate the capacity to mimic the UMap dimensionality reduction\n\nCompate the embedding spaces (UMap vs learned) with the validation set\n\n\nfor class_idx, class_name in enumerate(class_names):\n    plt.scatter(val_fx[val_y == class_idx, 0], val_fx[val_y == class_idx, 1], label=class_names[class_idx], marker=class_markers[class_idx], facecolors=class_facecolors[class_idx], edgecolors=class_colors[class_idx])\n\nplt.legend()\n\n\n\n\n\n\n\n\n\nfor class_idx, class_name in enumerate(class_names):\n    plt.scatter(val_fx_pred[val_y == class_idx, 0], val_fx_pred[val_y == class_idx, 1], label=class_names[class_idx], marker=class_markers[class_idx], facecolors=class_facecolors[class_idx], edgecolors=class_colors[class_idx])\n\nplt.legend()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#execute-the-feature-extraction-and-classification-pipeline-for-compound-data",
    "href": "notes/Adv_ML_Python_presentation_2.html#execute-the-feature-extraction-and-classification-pipeline-for-compound-data",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Execute the feature extraction and classification pipeline for compound data",
    "text": "Execute the feature extraction and classification pipeline for compound data\n\ncompounds_ds = TiffS3Dataset(comp_plate_maps, wells_metadata, comp_plate_maps[\"Plate_name\"].tolist(), 16, 24, 9, 5, shuffle=True)\n\nbatch_size = 5\n\ncompounds_dl = DataLoader(compounds_ds, batch_size=batch_size, num_workers=2, worker_init_fn=dataset_worker_init_fn)\n\nmetadata_list = []\nfor i, (x, y, metadata) in tqdm(enumerate(compounds_dl)):\n    metadata_list.append(metadata)\n\n    b, c, h, w = x.shape\n    x_t = model_transforms(torch.tile(x.reshape(-1, 1, h, w), (1, 3, 1, 1)))\n\n    if torch.cuda.is_available():\n        x_t = x_t.cuda()\n\n    with torch.no_grad():\n        x_out = model(x_t)\n        x_out = x_out.detach().reshape(-1, c, 576, 7, 7).sum(dim=1)\n        x_out = org_avgpool(x_out).detach().reshape(b, -1)\n\n        y_pred, fx_pred = classifier(x_out)\n\n    break"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#execute-the-feature-extraction-and-classification-pipeline-for-compound-data-1",
    "href": "notes/Adv_ML_Python_presentation_2.html#execute-the-feature-extraction-and-classification-pipeline-for-compound-data-1",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Execute the feature extraction and classification pipeline for compound data",
    "text": "Execute the feature extraction and classification pipeline for compound data\n\nPlot the embeddings of the compound samples\n\n\nfor class_idx, class_name in enumerate(class_names[:3]):\n    plt.scatter(trn_fx_pred[trn_y == class_idx, 0][::10], trn_fx_pred[trn_y == class_idx, 1][::10], label=class_names[class_idx], marker=class_markers[class_idx], facecolors=class_facecolors[class_idx], edgecolors=class_colors[class_idx])\n\nplt.scatter(fx_pred.cpu()[:, 0], fx_pred.cpu()[:, 1], label=class_names[3], marker=class_markers[3], facecolors=class_facecolors[3], edgecolors=class_colors[3])\n\nplt.legend()"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#execute-the-feature-extraction-and-classification-pipeline-for-compound-data-2",
    "href": "notes/Adv_ML_Python_presentation_2.html#execute-the-feature-extraction-and-classification-pipeline-for-compound-data-2",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Execute the feature extraction and classification pipeline for compound data",
    "text": "Execute the feature extraction and classification pipeline for compound data\n\nLook for any interesting compounds in PubChem\n\n\nmetadata\n\n{'Plate_name': ['Dest210727-153623',\n  'PEP00004070',\n  '110000294885',\n  'P09_APTJUM419',\n  '1086290323'],\n 'Source_name': ['source_10', 'source_15', 'source_6', 'source_5', 'source_2'],\n 'Batch_name': ['2021_08_09_U2OS_48_hr_run13',\n  '2021_12_17_Batch1',\n  'p210906CPU2OS48hw384exp025JUMP',\n  'JUMPCPE-20210902-Run26_20210903_010341',\n  '20210920_Batch_12'],\n 'Plate_type': ['COMPOUND', 'COMPOUND', 'COMPOUND', 'COMPOUND', 'COMPOUND'],\n 'Plate_path': ['Dest210727-153623',\n  'PEP00004070__2021-12-14T07_25_04-Measurement2',\n  '110000294885',\n  'P09_APTJUM419',\n  '1086290323'],\n 'Well_position': ['M17', 'I02', 'P11', 'B16', 'A04']}\n\n\n\ny_pred.argmax(dim=1)\n\ntensor([2, 1, 1, 1, 2], device='cuda:0')\n\n\n\nwells_metadata.query(\"well_position == 'H13' & Plate_type=='COMPOUND'\")\n\n\n  \n    \n\n\n\n\n\n\nwell_position\nbroad_sample\nsolvent\nPlate_type\nPlate_label\n\n\n\n\n180\nH13\nNaN\nDMSO\nCOMPOUND\n0"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#execute-the-feature-extraction-with-the-deep-learning-model",
    "href": "notes/Adv_ML_Python_presentation_2.html#execute-the-feature-extraction-with-the-deep-learning-model",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Execute the feature extraction with the deep learning model",
    "text": "Execute the feature extraction with the deep learning model\n\nUse the frozen deep learning model to extract morphological features from the field-level images and create a features database\n\n\nfrom tqdm.auto import tqdm\n\nfeatures = []\ntargets = []\n\nfor i, (x, y, _) in tqdm(enumerate(training_dl)):\n    b, c, h, w = x.shape\n    x_t = model_transforms(torch.tile(x.reshape(-1, 1, h, w), (1, 3, 1, 1)))\n\n    if torch.cuda.is_available():\n        x_t = x_t.cuda()\n    \n    with torch.no_grad():\n        x_out = model(x_t)\n        x_out = x_out.detach().cpu().reshape(-1, c, 576, 7, 7).sum(dim=1)\n        x_out = org_avgpool(x_out).detach().reshape(b, -1)\n\n    features.append(x_out)\n    targets.append(y)\n\n    # This is for illustration purposes.\n    # We'll load the pre-extracted features from Cloud Storage, so no need to generate it here.\n    break\n\nfeatures = torch.cat(features, dim=0)\ntargets = torch.cat(targets, dim=0)\n\nfeatures.shape, targets.shape\n\n\n\n\n(torch.Size([10, 576]), torch.Size([10]))"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#execute-the-feature-extraction-with-the-deep-learning-model-1",
    "href": "notes/Adv_ML_Python_presentation_2.html#execute-the-feature-extraction-with-the-deep-learning-model-1",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Execute the feature extraction with the deep learning model",
    "text": "Execute the feature extraction with the deep learning model\n\nPerform the same operation for the validation set\n\n\nval_features = []\nval_targets = []\n\nvalidation_dl = DataLoader(validation_ds, batch_size=batch_size, num_workers=2, worker_init_fn=dataset_worker_init_fn)\n\nfor i, (x, y, _) in tqdm(enumerate(validation_dl)):\n    b, c, h, w = x.shape\n    x_t = model_transforms(torch.tile(x.reshape(-1, 1, h, w), (1, 3, 1, 1)))\n\n    if torch.cuda.is_available():\n        x_t = x_t.cuda()\n    \n    with torch.no_grad():\n        x_out = model(x_t)\n        x_out = x_out.detach().reshape(-1, c, 576, 7, 7).sum(dim=1)\n        x_out = org_avgpool(x_out).detach().reshape(b, -1)\n\n    val_features.append(x_out)\n    val_targets.append(y)\n\n    break\n\nval_features = torch.cat(val_features, dim=0)\nval_targets = torch.cat(val_targets, dim=0)"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#execute-the-feature-extraction-with-the-deep-learning-model-2",
    "href": "notes/Adv_ML_Python_presentation_2.html#execute-the-feature-extraction-with-the-deep-learning-model-2",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Execute the feature extraction with the deep learning model",
    "text": "Execute the feature extraction with the deep learning model\n\nAnd again for the testing set\n\n\ntst_features = []\ntst_targets = []\n\ntesting_dl = DataLoader(testing_ds, batch_size=batch_size, num_workers=2, worker_init_fn=dataset_worker_init_fn)\n\nfor i, (x, y, _) in tqdm(enumerate(testing_dl)):\n    b, c, h, w = x.shape\n    x_t = model_transforms(torch.tile(x.reshape(-1, 1, h, w), (1, 3, 1, 1)))\n\n    if torch.cuda.is_available():\n        x_t = x_t.cuda()\n    \n    with torch.no_grad():\n        x_out = model(x_t)\n        x_out = x_out.detach().reshape(-1, c, 576, 7, 7).sum(dim=1)\n        x_out = org_avgpool(x_out).detach().reshape(b, -1)\n\n    tst_features.append(x_out)\n    tst_targets.append(y)\n\n    break\n\ntst_features = torch.cat(tst_features, dim=0)\ntst_targets = torch.cat(tst_targets, dim=0)\n\n\n\n\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-162003/Dest210628-162003_C17_T0001F008L01A01Z01C01.tif\nFailed retrieving: s3://cellpainting-gallery/cpg0016-jump/source_10/images/2021_06_28_U2OS_48_hr_run9/images/Dest210628-162003/Dest210628-162003_M16_T0001F007L01A01Z01C01.tif"
  },
  {
    "objectID": "notes/Adv_ML_Python_presentation_2.html#subset-the-dataset-to-extract-samples-with-crispr-orf-and-nonedmso-treatments-1",
    "href": "notes/Adv_ML_Python_presentation_2.html#subset-the-dataset-to-extract-samples-with-crispr-orf-and-nonedmso-treatments-1",
    "title": "Advanced Machine Learning with Python (Session 2)",
    "section": "Subset the dataset to extract samples with CRISPR, ORF, and NONE/DMSO treatments",
    "text": "Subset the dataset to extract samples with CRISPR, ORF, and NONE/DMSO treatments\n\nRead the ORF plate maps from the JUMP-Target repository\n\n\norf_wells_metadata = pd.read_csv(\"JUMP-Target/JUMP-Target-1_orf_platemap.tsv\", sep=\"\\t\")\n\n\nAdd one column to identify the type of treatment, and other to assign a numeric label (ORF = 2)\n\n\norf_wells_metadata[\"Plate_type\"] = \"ORF\"\norf_wells_metadata[\"Plate_label\"] = 2\norf_wells_metadata\n\n\n  \n    \n\n\n\n\n\n\nwell_position\nbroad_sample\nPlate_type\nPlate_label\n\n\n\n\n0\nA01\nccsbBroad304_00900\nORF\n2\n\n\n1\nA02\nccsbBroad304_07795\nORF\n2\n\n\n2\nA03\nccsbBroad304_02826\nORF\n2\n\n\n3\nA04\nccsbBroad304_01492\nORF\n2\n\n\n4\nA05\nccsbBroad304_00691\nORF\n2\n\n\n...\n...\n...\n...\n...\n\n\n379\nP20\nccsbBroad304_00277\nORF\n2\n\n\n380\nP21\nccsbBroad304_06464\nORF\n2\n\n\n381\nP22\nccsbBroad304_00476\nORF\n2\n\n\n382\nP23\nccsbBroad304_01649\nORF\n2\n\n\n383\nP24\nccsbBroad304_03934\nORF\n2\n\n\n\n\n384 rows × 4 columns"
  }
]